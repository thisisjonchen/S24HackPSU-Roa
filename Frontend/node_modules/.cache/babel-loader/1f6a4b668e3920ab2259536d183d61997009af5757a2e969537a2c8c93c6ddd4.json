{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source: engine/training.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { io, Optimizer, scalar, serialization, Tensor, tensor1d, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { nameScope } from '../common';\nimport { NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { deserialize } from '../layers/serialization';\nimport { disposeTensorsInLogs } from '../logs';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport { checkUserDefinedMetadata } from '../user_defined_metadata';\nimport { count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique } from '../utils/generic_utils';\nimport { printSummary } from '../utils/layer_utils';\nimport { range } from '../utils/math_utils';\nimport { convertPythonicToTs } from '../utils/serialization_utils';\nimport { version } from '../version';\nimport { Container } from './container';\nimport { execute, FeedDict } from './executor';\nimport { evaluateDataset, fitDataset } from './training_dataset';\nimport { checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, makeBatches, sliceArrays, sliceArraysByIndices } from './training_tensors';\nimport { computeWeightedLoss, standardizeClassWeights, standardizeWeights } from './training_utils';\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x) {\n  return x instanceof Tensor;\n}\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x) {\n  return Array.isArray(x);\n}\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x) {\n  return !isDataTensor(x) && !isDataArray(x);\n}\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = '') {\n  if (names == null || names.length === 0) {\n    // Check for the case where the model expected no data, but some data got\n    // sent.\n    if (data != null) {\n      let gotUnexpectedData = false;\n      if (isDataArray(data) && data.length > 0) {\n        gotUnexpectedData = true;\n      } else if (isDataDict(data)) {\n        for (const key in data) {\n          if (data.hasOwnProperty(key)) {\n            gotUnexpectedData = true;\n            break;\n          }\n        }\n      } else {\n        // `data` is a singleton Tensor in this case.\n        gotUnexpectedData = true;\n      }\n      if (gotUnexpectedData) {\n        throw new ValueError(`Error when checking model ${exceptionPrefix} expected no data, ` + `but got ${data}`);\n      }\n    }\n    return [];\n  }\n  if (data == null) {\n    return names.map(name => null);\n  }\n  let arrays;\n  if (isDataDict(data)) {\n    data = data;\n    arrays = [];\n    for (const name of names) {\n      if (data[name] == null) {\n        throw new ValueError(`No data provided for \"${name}\". Need data for each key in: ` + `${names}`);\n      }\n      arrays.push(data[name]);\n    }\n  } else if (isDataArray(data)) {\n    data = data;\n    if (data.length !== names.length) {\n      throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of ` + `Tensors that you are passing to your model is not the size the ` + `model expected. Expected to see ${names.length} Tensor(s), but ` + `instead got the following list of Tensor(s): ${data}`);\n    }\n    arrays = data;\n  } else {\n    data = data;\n    if (names.length > 1) {\n      throw new ValueError(`The model ${exceptionPrefix} expects ${names.length} Tensor(s), ` + `but only received one Tensor. Found: Tensor with shape ${data.shape}`);\n    }\n    arrays = [data];\n  }\n  arrays = ensureTensorsRank2OrHigher(arrays);\n  // Check shape compatibility.\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} ` + `to have ${shapes[i].length} dimension(s). but got array with ` + `shape ${array.shape}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          // Skip the first (batch) axis.\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null && refDim >= 0 && dim !== refDim) {\n          throw new ValueError(`${exceptionPrefix} expected a batch of elements where each ` + `example has shape [${shapes[i].slice(1, shapes[i].length)}] ` + `(i.e.,tensor shape [*,${shapes[i].slice(1, shapes[i].length)}])` + ` but the ${exceptionPrefix} received an input with ${array.shape[0]}` + ` examples, each with shape [${array.shape.slice(1, array.shape.length)}]` + ` (tensor shape [${array.shape}])`);\n        }\n      }\n    }\n  }\n  return arrays;\n}\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(inputs, targets, weights) {\n  const setX = unique(inputs.map(input => input.shape[0]));\n  setX.sort();\n  const setY = unique(targets.map(target => target.shape[0]));\n  setY.sort();\n  // TODO(cais): Check `weights` as well.\n  if (setX.length > 1) {\n    throw new ValueError(`All input Tensors (x) should have the same number of samples. ` + `Got array shapes: ` + `${JSON.stringify(inputs.map(input => input.shape))}`);\n  }\n  if (setY.length > 1) {\n    throw new ValueError(`All target Tensors (y) should have the same number of samples. ` + `Got array shapes: ` + `${JSON.stringify(targets.map(target => target.shape))}`);\n  }\n  if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n    throw new ValueError(`Input Tensors should have the same number of samples as target ` + `Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target ` + `sample(s).`);\n  }\n}\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(targets, lossFns, outputShapes) {\n  // TODO(cais): Dedicated test coverage?\n  const keyLosses = [losses.meanSquaredError, losses.binaryCrossentropy, losses.categoricalCrossentropy];\n  for (let i = 0; i < targets.length; ++i) {\n    const y = targets[i];\n    const loss = lossFns[i];\n    const shape = outputShapes[i];\n    if (loss == null) {\n      continue;\n    }\n    if (loss === losses.categoricalCrossentropy) {\n      if (y.shape[y.shape.length - 1] === 1) {\n        throw new ValueError(`You are passing a target array of shape ${y.shape} while using ` + `a loss 'categorical_crossentropy'. 'categorical_crossentropy'` + `expects targets to be binary matrices (1s and 0s) of shape ` + `[samples, classes].`);\n        // TODO(cais): Example code in error message.\n      }\n    }\n    if (keyLosses.indexOf(loss) !== -1) {\n      const slicedYShape = y.shape.slice(1);\n      const slicedShape = shape.slice(1);\n      for (let j = 0; j < slicedYShape.length; ++j) {\n        const targetDim = slicedYShape[j];\n        const outDim = slicedShape[j];\n        if (outDim != null && targetDim !== outDim) {\n          throw new ValueError(`A target Tensor with shape ${y.shape} was passed for an ` + `output of shape ${shape}, while using a loss function that ` + `expects targets to have the same shape as the output.`);\n        }\n      }\n    }\n  }\n}\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(data, names, shapes, checkBatchAxis = true, exceptionPrefix = '') {\n  let arrays;\n  if (Array.isArray(data)) {\n    if (data.length !== names.length) {\n      throw new ValueError(`Error when checking model ${exceptionPrefix}: the Array of ` + `Tensors that you are passing to your model is not the size the ` + `the model expected. Expected to see ${names.length} Tensor(s),` + ` but instead got ${data.length} Tensors(s).`);\n    }\n    arrays = data;\n  } else {\n    if (names.length > 1) {\n      throw new ValueError(`The model expects ${names.length} ${exceptionPrefix} Tensors, ` + `but only received one Tensor. Found: array with shape ` + `${JSON.stringify(data.shape)}.`);\n    }\n    arrays = [data];\n  }\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(`Error when checking ${exceptionPrefix}: expected ${names[i]} ` + `to have ${shapes[i].length} dimension(s), but got array with ` + `shape ${JSON.stringify(array.shape)}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null) {\n          if (refDim !== dim) {\n            throw new ValueError(`Error when checking ${exceptionPrefix}: expected ` + `${names[i]} to have shape ${JSON.stringify(shapes[i])} but ` + `got array with shape ${JSON.stringify(array.shape)}.`);\n          }\n        }\n      }\n    }\n  }\n}\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(metrics, outputNames) {\n  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n    return outputNames.map(name => []);\n  }\n  let wrappedMetrics;\n  if (typeof metrics === 'string' || typeof metrics === 'function') {\n    wrappedMetrics = [metrics];\n  } else if (Array.isArray(metrics) || typeof metrics === 'object') {\n    wrappedMetrics = metrics;\n  } else {\n    throw new TypeError('Type of metrics argument not understood. Expected an string,' + `function, Array, or Object, found: ${metrics}`);\n  }\n  if (Array.isArray(wrappedMetrics)) {\n    // We then apply all metrics to all outputs.\n    return outputNames.map(name => wrappedMetrics);\n  } else {\n    // In this case, metrics is a dict.\n    const nestedMetrics = [];\n    for (const name of outputNames) {\n      let outputMetrics = wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n      if (!Array.isArray(outputMetrics)) {\n        outputMetrics = [outputMetrics];\n      }\n      nestedMetrics.push(outputMetrics);\n    }\n    return nestedMetrics;\n  }\n}\nconst LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nclass LayersModel extends Container {\n  constructor(args) {\n    super(args);\n    this.isTraining = false;\n  }\n  /**\n   * Print a text summary of the model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - If the model has non-sequential-like topology, the inputs each layer\n   *   receives\n   * - The total number of trainable and non-trainable parameters of the model.\n   *\n   * ```js\n   * const input1 = tf.input({shape: [10]});\n   * const input2 = tf.input({shape: [20]});\n   * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n   * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n   * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n   * const output =\n   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n   *\n   * const model = tf.model({inputs: [input1, input2], outputs: output});\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  summary(lineLength, positions, printFn = console.log) {\n    if (!this.built) {\n      throw new ValueError(`This model has never been called, thus its weights have not been ` + `created yet. So no summary can be displayed. Build the model ` + `first (e.g., by calling it on some test data).`);\n    }\n    printSummary(this, lineLength, positions, printFn);\n  }\n  /**\n   * Configures and prepares the model for training and evaluation.  Compiling\n   * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n   * or `evaluate` on an un-compiled model will throw an error.\n   *\n   * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n   * metrics to be used for fitting and evaluating this model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  compile(args) {\n    if (args.loss == null) {\n      args.loss = [];\n    }\n    this.loss = args.loss;\n    if (typeof args.optimizer === 'string') {\n      this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n      this.isOptimizerOwned = true;\n    } else {\n      if (!(args.optimizer instanceof Optimizer)) {\n        throw new ValueError(`User-defined optimizer must be an instance of tf.Optimizer.`);\n      }\n      this.optimizer_ = args.optimizer;\n      this.isOptimizerOwned = false;\n    }\n    // TODO(cais): Add lossWeights.\n    // TODO(cais): Add sampleWeightMode.\n    // Prepare loss functions.\n    let lossFunctions = [];\n    if (!Array.isArray(args.loss) && typeof args.loss !== 'string' && typeof args.loss !== 'function') {\n      args.loss = args.loss;\n      for (const name in args.loss) {\n        if (this.outputNames.indexOf(name) === -1) {\n          throw new ValueError(`Unknown entry in loss dictionary: \"${name}\". ` + `Only expected the following keys: ${this.outputNames}`);\n        }\n      }\n      for (const name of this.outputNames) {\n        if (args.loss[name] == null) {\n          console.warn(`Output \"${name}\" is missing from loss dictionary. We assume ` + `this was done on purpose, and we will not be expecting data ` + `to be passed to ${name} during training`);\n        }\n        lossFunctions.push(losses.get(args.loss[name]));\n      }\n    } else if (Array.isArray(args.loss)) {\n      if (args.loss.length !== this.outputs.length) {\n        throw new ValueError(`When passing an Array as loss, it should have one entry per ` + `model output. The model has ${this.outputs.length} output(s), ` + `but you passed loss=${args.loss}.`);\n      }\n      const theLosses = args.loss;\n      lossFunctions = theLosses.map(l => losses.get(l));\n    } else {\n      const lossFunction = losses.get(args.loss);\n      this.outputs.forEach(_ => {\n        lossFunctions.push(lossFunction);\n      });\n    }\n    this.lossFunctions = lossFunctions;\n    this.feedOutputNames = [];\n    this.feedOutputShapes = [];\n    this.feedLossFns = [];\n    for (let i = 0; i < this.outputs.length; ++i) {\n      // TODO(cais): Logic for skipping target(s).\n      const shape = this.internalOutputShapes[i];\n      const name = this.outputNames[i];\n      this.feedOutputNames.push(name);\n      this.feedOutputShapes.push(shape);\n      this.feedLossFns.push(this.lossFunctions[i]);\n    }\n    // TODO(cais): Add logic for output masks.\n    // TODO(cais): Add logic for sample weights.\n    const skipTargetIndices = [];\n    // Prepare metrics.\n    this.metrics = args.metrics;\n    // TODO(cais): Add weightedMetrics.\n    this.metricsNames = ['loss'];\n    this.metricsTensors = [];\n    // Compute total loss.\n    // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n    //   Here, metricsTensors are TypeScript functions. This difference is due\n    //   to the difference in symbolic/imperative property of the backends.\n    nameScope('loss', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        // TODO(cais): Add weightedLoss, sampleWeight and mask.\n        //   The following line should be weightedLoss\n        const weightedLoss = this.lossFunctions[i];\n        if (this.outputs.length > 1) {\n          this.metricsTensors.push([weightedLoss, i]);\n          this.metricsNames.push(this.outputNames[i] + '_loss');\n        }\n      }\n      // Porting Note: Due to the imperative nature of the backend, we calculate\n      //   the regularizer penalties in the totalLossFunction, instead of here.\n    });\n    const nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n    // TODO(cais): Add nestedWeightedMetrics.\n    /**\n     * Helper function used in loop below.\n     */\n    const appendMetric = (outputIndex, metricName, metricTensor) => {\n      if (this.outputNames.length > 1) {\n        metricName = this.outputNames[outputIndex] + '_' + metricName;\n      }\n      this.metricsNames.push(metricName);\n      this.metricsTensors.push([metricTensor, outputIndex]);\n    };\n    nameScope('metric', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        const outputMetrics = nestedMetrics[i];\n        // TODO(cais): Add weights and outputWeightedMetrics.\n        // TODO(cais): Add optional arg `weights` to the following function.\n        const handleMetrics = metrics => {\n          const metricNamePrefix = '';\n          let metricName;\n          let accFn;\n          let weightedMetricFn;\n          //  TODO(cais): Use 'weights_' for weighted metrics.\n          for (const metric of metrics) {\n            if (typeof metric === 'string' && ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !== -1) {\n              const outputShape = this.internalOutputShapes[i];\n              if (outputShape[outputShape.length - 1] === 1 || this.lossFunctions[i] === losses.binaryCrossentropy) {\n                // case: binary accuracy/crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryCrossentropy;\n                }\n              } else if (this.lossFunctions[i] === losses.sparseCategoricalCrossentropy) {\n                // case: categorical accuracy / crossentropy with sparse\n                // targets.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalCrossentropy;\n                }\n              } else {\n                // case: categorical accuracy / crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalCrossentropy;\n                }\n              }\n              let suffix;\n              if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                suffix = 'acc';\n              } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                suffix = 'ce';\n              }\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = accFn;\n              metricName = metricNamePrefix + suffix;\n            } else {\n              const metricFn = Metrics.get(metric);\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = metricFn;\n              metricName = metricNamePrefix + Metrics.getLossOrMetricName(metric);\n            }\n            // TODO(cais): Add weighting and masking to metricResult.\n            let metricResult;\n            nameScope(metricName, () => {\n              metricResult = weightedMetricFn;\n            });\n            appendMetric(i, metricName, metricResult);\n          }\n        };\n        handleMetrics(outputMetrics);\n        // TODO(cais): Call handleMetrics with weights.\n      }\n    });\n    // Porting Notes: Given the imperative backend of tfjs-core,\n    //   there is no need for constructing the symbolic graph and placeholders.\n    this.collectedTrainableWeights = this.trainableWeights;\n  }\n  /**\n   * Check trainable weights count consistency.\n   *\n   * This will raise a warning if `this.trainableWeights` and\n   * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n   * numbers of parameters).\n   * Inconsistency will typically arise when one modifies `model.trainable`\n   * without calling `model.compile()` again.\n   */\n  checkTrainableWeightsConsistency() {\n    if (this.collectedTrainableWeights == null) {\n      return;\n    }\n    if (this.trainableWeights.length !== this.collectedTrainableWeights.length) {\n      console.warn('Discrepancy between trainableweights and collected trainable ' + 'weights. Did you set `model.trainable` without calling ' + '`model.compile()` afterwards?');\n    }\n  }\n  /**\n   * Returns the loss value & metrics values for the model in test mode.\n   *\n   * Loss and metrics are specified during `compile()`, which needs to happen\n   * before calls to `evaluate()`.\n   *\n   * Computation is done in batches.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const result = model.evaluate(\n   *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n   * result.print();\n   * ```\n   *\n   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple inputs.\n   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple outputs.\n   * @param args A `ModelEvaluateArgs`, containing optional fields.\n   *\n   * @return `Scalar` test loss (if the model has a single output and no\n   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n   *   and/or metrics). The attribute `model.metricsNames`\n   *   will give you the display labels for the scalar outputs.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  evaluate(x, y, args = {}) {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize);\n    // TODO(cais): Standardize `config.sampleWeights` as well.\n    // Validate user data.\n    const checkBatchAxis = true;\n    const standardizedOuts = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    try {\n      // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n      // of the input to 0.\n      const ins = standardizedOuts[0].concat(standardizedOuts[1]);\n      this.makeTestFunction();\n      const f = this.testFunction;\n      const testOuts = this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n      return singletonOrArray(testOuts);\n    } finally {\n      disposeNewTensors(standardizedOuts[0], x);\n      disposeNewTensors(standardizedOuts[1], y);\n    }\n  }\n  // TODO(cais): Add code snippet below once real dataset objects are\n  //   available.\n  /**\n   * Evaluate model using a dataset object.\n   *\n   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for evaluation. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs. Of the two items in the array, the\n   *   first is the input feature(s) and the second is the output target(s).\n   * @param args A configuration object for the dataset-based evaluation.\n   * @returns Loss and metric values as an Array of `Scalar` objects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async evaluateDataset(dataset, args) {\n    this.makeTestFunction();\n    return evaluateDataset(this, dataset, args);\n  }\n  /**\n   * Get number of samples provided for training, evaluation or prediction.\n   *\n   * @param ins Input `tf.Tensor`.\n   * @param batchSize Integer batch size, optional.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring loop finished. Optional.\n   * @param stepsName The public API's parameter name for `steps`.\n   * @returns Number of samples provided.\n   */\n  checkNumSamples(ins, batchSize, steps, stepsName = 'steps') {\n    let numSamples;\n    if (steps != null) {\n      numSamples = null;\n      if (batchSize != null) {\n        throw new ValueError(`If ${stepsName} is set, batchSize must be null or undefined.` + `Got batchSize = ${batchSize}`);\n      }\n    } else if (ins != null) {\n      if (Array.isArray(ins)) {\n        numSamples = ins[0].shape[0];\n      } else {\n        numSamples = ins.shape[0];\n      }\n    } else {\n      throw new ValueError(`Either the input data should have a defined shape, or ` + `${stepsName} shoud be specified.`);\n    }\n    return numSamples;\n  }\n  /**\n   * Execute internal tensors of the model with input data feed.\n   * @param inputs Input data feed. Must match the inputs of the model.\n   * @param outputs Names of the output tensors to be fetched. Must match\n   *   names of the SymbolicTensors that belong to the graph.\n   * @returns Fetched values for `outputs`.\n   */\n  execute(inputs, outputs) {\n    if (Array.isArray(outputs) && outputs.length === 0) {\n      throw new ValueError('`outputs` is an empty Array, which is not allowed.');\n    }\n    const outputsIsArray = Array.isArray(outputs);\n    const outputNames = outputsIsArray ? outputs : [outputs];\n    const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n    // Format the input into a FeedDict.\n    const feedDict = new FeedDict();\n    if (inputs instanceof Tensor) {\n      inputs = [inputs];\n    }\n    if (Array.isArray(inputs)) {\n      if (inputs.length !== this.inputs.length) {\n        throw new ValueError(`The number of inputs provided (${inputs.length}) ` + `does not match the number of inputs of this model ` + `(${this.inputs.length}).`);\n      }\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n    } else {\n      for (const input of this.inputs) {\n        const tensorValue = inputs[input.name];\n        if (tensorValue == null) {\n          throw new ValueError(`No value is provided for the model's input ${input.name}`);\n        }\n        feedDict.add(input, tensorValue);\n      }\n    }\n    // Run execution.\n    const executeOutputs = execute(outputSymbolicTensors, feedDict);\n    return outputsIsArray ? executeOutputs : executeOutputs[0];\n  }\n  /**\n   * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n   */\n  retrieveSymbolicTensors(symbolicTensorNames) {\n    const outputSymbolicTensors = pyListRepeat(null, symbolicTensorNames.length);\n    let outputsRemaining = symbolicTensorNames.length;\n    for (const layer of this.layers) {\n      const layerOutputs = Array.isArray(layer.output) ? layer.output : [layer.output];\n      const layerOutputNames = layerOutputs.map(output => output.name);\n      for (let i = 0; i < symbolicTensorNames.length; ++i) {\n        const index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n        if (index !== -1) {\n          outputSymbolicTensors[i] = layerOutputs[index];\n          outputsRemaining--;\n        }\n        if (outputsRemaining === 0) {\n          break;\n        }\n      }\n      if (outputsRemaining === 0) {\n        break;\n      }\n    }\n    if (outputsRemaining > 0) {\n      const remainingNames = [];\n      outputSymbolicTensors.forEach((tensor, i) => {\n        if (tensor == null) {\n          remainingNames.push(symbolicTensorNames[i]);\n        }\n      });\n      throw new ValueError(`Cannot find SymbolicTensors for output name(s): ` + `${JSON.stringify(remainingNames)}`);\n    }\n    return outputSymbolicTensors;\n  }\n  /**\n   * Helper method to loop over some data in batches.\n   *\n   * Porting Note: Not using the functional approach in the Python equivalent\n   *   due to the imperative backend.\n   * Porting Note: Does not support step mode currently.\n   *\n   * @param ins: input data\n   * @param batchSize: integer batch size.\n   * @param verbose: verbosity model\n   * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n   *   `tf.Tensor` (if multipe outputs).\n   */\n  predictLoop(ins, batchSize = 32, verbose = false) {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins);\n      if (verbose) {\n        throw new NotImplementedError('Verbose predictLoop() is not implemented yet.');\n      }\n      // Sample-based predictions.\n      // Porting Note: Tensor currently does not support sliced assignments as\n      //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n      //   iterating over the batches.\n      const batches = makeBatches(numSamples, batchSize);\n      const outsBatches = this.outputs.map(output => []);\n      // TODO(cais): Can the scope() be pushed down inside the for loop?\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchOuts = tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          // TODO(cais): Take care of the case of the last element is a flag for\n          //   training/test.\n          const insBatch = sliceArrays(ins, batchStart, batchEnd);\n          // Construct the feeds for execute();\n          const feeds = [];\n          if (Array.isArray(insBatch)) {\n            for (let i = 0; i < insBatch.length; ++i) {\n              feeds.push({\n                key: this.inputs[i],\n                value: insBatch[i]\n              });\n            }\n          } else {\n            feeds.push({\n              key: this.inputs[0],\n              value: insBatch\n            });\n          }\n          const feedDict = new FeedDict(feeds);\n          return execute(this.outputs, feedDict);\n        });\n        batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));\n      }\n      return singletonOrArray(outsBatches.map(batches => tfc.concat(batches, 0)));\n    });\n  }\n  /**\n   * Generates output predictions for the input samples.\n   *\n   * Computation is done in batches.\n   *\n   * Note: the \"step\" mode of predict() is currently not supported.\n   *   This is because the TensorFlow.js core backend is imperative only.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n   * ```\n   *\n   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n   *   the model has multiple inputs.\n   * @param args A `ModelPredictArgs` object containing optional fields.\n   *\n   * @return Prediction results as a `tf.Tensor`(s).\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and the model's expectations, or in case a stateful model receives a\n   *   number of samples that is not a multiple of the batch size.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(x, args = {}) {\n    const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n    checkInputData(xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n    try {\n      // TODO(cais): Take care of stateful models.\n      //   if (this.stateful) ...\n      // TODO(cais): Take care of the learning_phase boolean flag.\n      //   if (this.useLearningPhase) ...\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      return this.predictLoop(xsRank2OrHigher, batchSize);\n    } finally {\n      disposeNewTensors(xsRank2OrHigher, x);\n    }\n  }\n  /**\n   * Returns predictions for a single batch of samples.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predictOnBatch(tf.ones([8, 10])).print();\n   * ```\n   * @param x: Input samples, as a Tensor (for models with exactly one\n   *   input) or an array of Tensors (for models with more than one input).\n   * @return Tensor(s) of predictions\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predictOnBatch(x) {\n    checkInputData(x, this.inputNames, this.feedInputShapes, true);\n    // TODO(cais): Take care of the learning_phase boolean flag.\n    //   if (this.useLearningPhase) ...\n    const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n    return this.predictLoop(x, batchSize);\n  }\n  standardizeUserDataXY(x, y, checkBatchAxis = true, batchSize) {\n    // TODO(cais): Add sampleWeight, classWeight\n    if (this.optimizer_ == null) {\n      throw new RuntimeError('You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileArgs).');\n    }\n    const outputShapes = [];\n    for (let i = 0; i < this.feedOutputShapes.length; ++i) {\n      const outputShape = this.feedOutputShapes[i];\n      const lossFn = this.feedLossFns[i];\n      if (lossFn === losses.sparseCategoricalCrossentropy) {\n        outputShapes.push(outputShape.slice(0, outputShape.length - 1).concat([1]));\n      } else {\n        // Porting Note: Because of strong typing `lossFn` must be a function.\n        outputShapes.push(outputShape);\n      }\n    }\n    x = standardizeInputData(x, this.feedInputNames, this.feedInputShapes, false, 'input');\n    y = standardizeInputData(y, this.feedOutputNames, outputShapes, false, 'target');\n    // TODO(cais): Standardize sampleWeights & classWeights.\n    checkArrayLengths(x, y, null);\n    // TODO(cais): Check sampleWeights as well.\n    checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n    if (this.stateful && batchSize != null && batchSize > 0) {\n      if (x[0].shape[0] % batchSize !== 0) {\n        throw new ValueError(`In a stateful network, you should only pass inputs with a ` + `number of samples that is divisible by the batch size ` + `${batchSize}. Found: ${x[0].shape[0]} sample(s).`);\n      }\n    }\n    return [x, y];\n  }\n  async standardizeUserData(x, y, sampleWeight, classWeight, checkBatchAxis = true, batchSize) {\n    const [standardXs, standardYs] = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    // TODO(cais): Handle sampleWeights.\n    if (sampleWeight != null) {\n      throw new Error('sample weight is not supported yet.');\n    }\n    let standardSampleWeights = null;\n    if (classWeight != null) {\n      const classWeights = standardizeClassWeights(classWeight, this.outputNames);\n      standardSampleWeights = [];\n      for (let i = 0; i < classWeights.length; ++i) {\n        standardSampleWeights.push(await standardizeWeights(standardYs[i], null, classWeights[i]));\n      }\n    }\n    // TODO(cais): Deal with the case of model.stateful == true.\n    return [standardXs, standardYs, standardSampleWeights];\n  }\n  /**\n   * Loop over some test data in batches.\n   * @param f A Function returning a list of tensors.\n   * @param ins Array of tensors to be fed to `f`.\n   * @param batchSize Integer batch size or `null` / `undefined`.\n   * @param verbose verbosity mode.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring test finished. Ignored with the default value of `null` /\n   * `undefined`.\n   * @returns Array of Scalars.\n   */\n  testLoop(f, ins, batchSize, verbose = 0, steps) {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins, batchSize, steps, 'steps');\n      const outs = [];\n      if (verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n      }\n      // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n      if (steps != null) {\n        throw new NotImplementedError('steps mode in testLoop() is not implemented yet');\n      } else {\n        const batches = makeBatches(numSamples, batchSize);\n        const indexArray = tensor1d(range(0, numSamples));\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds = K.sliceAlongFirstAxis(indexArray, batchStart, batchEnd - batchStart);\n          // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n          const insBatch = sliceArraysByIndices(ins, batchIds);\n          const batchOuts = f(insBatch);\n          if (batchIndex === 0) {\n            for (let i = 0; i < batchOuts.length; ++i) {\n              outs.push(scalar(0));\n            }\n          }\n          for (let i = 0; i < batchOuts.length; ++i) {\n            const batchOut = batchOuts[i];\n            outs[i] = tfc.add(outs[i], tfc.mul(batchEnd - batchStart, batchOut));\n          }\n        }\n        for (let i = 0; i < outs.length; ++i) {\n          outs[i] = tfc.div(outs[i], numSamples);\n        }\n      }\n      return outs;\n    });\n  }\n  getDedupedMetricsNames() {\n    const outLabels = this.metricsNames;\n    // Rename duplicated metrics names (can happen with an output layer\n    // shared among multiple dataflows).\n    const dedupedOutLabels = [];\n    for (let i = 0; i < outLabels.length; ++i) {\n      const label = outLabels[i];\n      let newLabel = label;\n      if (count(outLabels, label) > 1) {\n        const dupIndex = count(outLabels.slice(0, i), label);\n        newLabel += `_${dupIndex}`;\n      }\n      dedupedOutLabels.push(newLabel);\n    }\n    return dedupedOutLabels;\n  }\n  /**\n   * Creates a function that performs the following actions:\n   *\n   * 1. computes the losses\n   * 2. sums them to get the total loss\n   * 3. call the optimizer computes the gradients of the LayersModel's\n   *    trainable weights w.r.t. the total loss and update the variables\n   * 4. calculates the metrics\n   * 5. returns the values of the losses and metrics.\n   */\n  makeTrainFunction() {\n    return data => {\n      const lossValues = [];\n      const inputs = data.slice(0, this.inputs.length);\n      const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);\n      const sampleWeights = data.slice(this.inputs.length + this.outputs.length, this.inputs.length + this.outputs.length * 2);\n      const metricsValues = [];\n      // Create a function that computes the total loss based on the\n      // inputs. This function is used for obtaining gradients through\n      // backprop.\n      const totalLossFunction = () => {\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({\n            key: this.inputs[i],\n            value: inputs[i]\n          });\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs = execute(this.outputs, feedDict, {\n          'training': true\n        });\n        // TODO(cais): Take care of the case of multiple outputs from a\n        //   single layer?\n        let totalLoss;\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          let loss = lossFunction(targets[i], outputs[i]);\n          if (sampleWeights[i] != null) {\n            loss = computeWeightedLoss(loss, sampleWeights[i]);\n          }\n          // TODO(cais): push Scalar instead.\n          const meanLoss = tfc.mean(loss);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          lossValues.push(meanLoss);\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n        }\n        // Compute the metrics.\n        // TODO(cais): These should probably be calculated outside\n        //   totalLossFunction to benefit speed?\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          let weightedMetric;\n          if (this.outputs.length > 1 && i < this.outputs.length) {\n            weightedMetric = lossValues[i];\n          } else {\n            const metric = this.metricsTensors[i][0];\n            const outputIndex = this.metricsTensors[i][1];\n            weightedMetric = tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          }\n          tfc.keep(weightedMetric);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          metricsValues.push(weightedMetric);\n        }\n        totalLoss = tfc.mean(totalLoss);\n        // Add regularizer penalties.\n        this.calculateLosses().forEach(regularizerLoss => {\n          totalLoss = tfc.add(totalLoss, regularizerLoss);\n        });\n        return totalLoss;\n      };\n      const variables = this.collectedTrainableWeights.map(param => param.read());\n      const returnCost = true;\n      const totalLossValue = this.optimizer_.minimize(totalLossFunction, returnCost, variables);\n      return [totalLossValue].concat(metricsValues);\n    };\n  }\n  /**\n   * Create a function which, when invoked with an array of `tf.Tensor`s as a\n   * batch of inputs, returns the prespecified loss and metrics of the model\n   * under the batch of input data.\n   */\n  makeTestFunction() {\n    this.testFunction = data => {\n      return tfc.tidy(() => {\n        const valOutputs = [];\n        let totalLoss;\n        const inputs = data.slice(0, this.inputs.length);\n        const targets = data.slice(this.inputs.length, this.inputs.length + this.outputs.length);\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({\n            key: this.inputs[i],\n            value: inputs[i]\n          });\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs = execute(this.outputs, feedDict);\n        // Compute total loss.\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          // TODO(cais): Add sample weighting and replace the simple\n          // averaging.\n          const loss = tfc.mean(lossFunction(targets[i], outputs[i]));\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n          valOutputs.push(totalLoss);\n        }\n        // Compute the metrics.\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          const metric = this.metricsTensors[i][0];\n          const outputIndex = this.metricsTensors[i][1];\n          // TODO(cais): Replace K.mean() with a proper weighting function.\n          const meanMetric = tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          valOutputs.push(meanMetric);\n        }\n        return valOutputs;\n      });\n    };\n  }\n  /**\n   * Trains the model for a fixed number of epochs (iterations on a\n   * dataset).\n   *\n   * ```js\n   * const model = tf.sequential({\n   *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * for (let i = 1; i < 5 ; ++i) {\n   *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *       batchSize: 4,\n   *       epochs: 3\n   *   });\n   *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n   * }\n   * ```\n   *\n   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n   * model has multiple inputs. If all inputs in the model are named, you\n   * can also pass a dictionary mapping input names to `tf.Tensor`s.\n   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n   * the model has multiple outputs. If all outputs in the model are named,\n   * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n   * @param args A `ModelFitArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @exception ValueError In case of mismatch between the provided input\n   * data and what the model expects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fit(x, y, args = {}) {\n    if (this.isTraining) {\n      throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    this.isTraining = true;\n    let inputs;\n    let targets;\n    let originalInputs;\n    let originalTargets;\n    let inputValX;\n    let inputValY;\n    let valX;\n    let valY;\n    let sampleWeights;\n    try {\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      // Validate user data.\n      // TODO(cais): Support sampleWeight.\n      const checkBatchAxis = false;\n      const standardizedOuts = await this.standardizeUserData(x, y, args.sampleWeight, args.classWeight, checkBatchAxis, batchSize);\n      inputs = standardizedOuts[0];\n      targets = standardizedOuts[1];\n      sampleWeights = standardizedOuts[2];\n      // Prepare validation data.\n      let doValidation = false;\n      let valIns;\n      if (args.validationData != null && args.validationData.length > 0) {\n        doValidation = true;\n        if (args.validationData.length === 2) {\n          // config.validationData consists of valX and valY.\n          inputValX = args.validationData[0];\n          inputValY = args.validationData[1];\n        } else if (args.validationData.length === 3) {\n          throw new NotImplementedError('validationData including sample weights is not supported yet.');\n        } else {\n          throw new ValueError(`When passing validation data, it must contain 2 (valX, valY) ` + `or 3 (valX, valY, valSampleWeight) items; ` + `${args.validationData} is invalid.`);\n        }\n        const checkBatchAxis = true;\n        const valStandardized = await this.standardizeUserData(inputValX, inputValY, null, /** Unused sample weights. */null, /** Unused class weights. */checkBatchAxis, batchSize);\n        valX = valStandardized[0];\n        valY = valStandardized[1];\n        valIns = valX.concat(valY);\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (args.validationSplit != null && args.validationSplit > 0 && args.validationSplit < 1) {\n        doValidation = true;\n        // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n        const splitAt = Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n        const originalBatchSize = inputs[0].shape[0];\n        valX = sliceArrays(inputs, splitAt, originalBatchSize);\n        originalInputs = inputs;\n        inputs = sliceArrays(inputs, 0, splitAt);\n        valY = sliceArrays(targets, splitAt, originalBatchSize);\n        originalTargets = targets;\n        targets = sliceArrays(targets, 0, splitAt);\n        // TODO(cais): Once sampleWeights becomes available, slice it to get\n        //   valSampleWeights.\n        valIns = valX.concat(valY);\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (args.validationSteps != null) {\n        doValidation = true;\n        // TODO(cais): Add useLearningPhase.\n      }\n      const ins = inputs.concat(targets).concat(sampleWeights);\n      this.checkTrainableWeightsConsistency();\n      // TODO(cais): Handle use_learning_phase and learning_phase?\n      // Porting Note: Here we see a key deviation of tfjs-layers from\n      // Keras.\n      //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n      //  we do not construct symbolic computation graphs to embody the\n      //  training process. Instead, we define a function that performs the\n      //  training action. In PyKeras, the data (inputs and targets) are fed\n      //  through graph placeholders. In tfjs-layers, the data are fed as\n      //  function arguments. Since the function are defined below in the\n      //  scope, we don't have equivalents of PyKeras's\n      //  `_make_train_funciton`.\n      const trainFunction = this.makeTrainFunction();\n      const outLabels = this.getDedupedMetricsNames();\n      let valFunction;\n      let callbackMetrics;\n      if (doValidation) {\n        this.makeTestFunction();\n        valFunction = this.testFunction;\n        callbackMetrics = outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n      } else {\n        valFunction = null;\n        valIns = [];\n        callbackMetrics = outLabels.slice();\n      }\n      const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n      const out = await this.fitLoop(trainFunction, ins, outLabels, batchSize, args.epochs, args.verbose, callbacks, valFunction, valIns, args.shuffle, callbackMetrics, args.initialEpoch, null, null);\n      return out;\n    } finally {\n      this.isTraining = false;\n      // Memory clean up.\n      disposeNewTensors(inputs, x);\n      disposeNewTensors(targets, y);\n      disposeNewTensors(originalInputs, x);\n      disposeNewTensors(originalTargets, y);\n      disposeNewTensors(valX, inputValX);\n      disposeNewTensors(valY, inputValY);\n      if (sampleWeights != null) {\n        tfc.dispose(sampleWeights);\n      }\n    }\n    // TODO(cais): Add value to outLabels.\n  }\n  /**\n   * Abstract fit function for `f(ins)`.\n   * @param f A Function returning a list of tensors. For training, this\n   *   function is expected to perform the updates to the variables.\n   * @param ins List of tensors to be fed to `f`.\n   * @param outLabels List of strings, display names of the outputs of `f`.\n   * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n   * @param epochs Number of times to iterate over the data. Default : 1.\n   * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n   * @param callbacks List of callbacks to be called during training.\n   * @param valF Function to call for validation.\n   * @param valIns List of tensors to be fed to `valF`.\n   * @param shuffle Whether to shuffle the data at the beginning of every\n   * epoch. Default : true.\n   * @param callbackMetrics List of strings, the display names of the metrics\n   *   passed to the callbacks. They should be the concatenation of the\n   *   display names of the outputs of `f` and the list of display names\n   *   of the outputs of `valF`.\n   * @param initialEpoch Epoch at which to start training (useful for\n   *   resuming a previous training run). Default : 0.\n   * @param stepsPerEpoch Total number of steps (batches on samples) before\n   *   declaring one epoch finished and starting the next epoch. Ignored with\n   *   the default value of `undefined` or `null`.\n   * @param validationSteps Number of steps to run validation for (only if\n   *   doing validation from data tensors). Not applicable for tfjs-layers.\n   * @returns A `History` object.\n   */\n  async fitLoop(f, ins, outLabels, batchSize, epochs, verbose, callbacks, valF, valIns, shuffle, callbackMetrics, initialEpoch, stepsPerEpoch, validationSteps) {\n    if (batchSize == null) {\n      batchSize = 32;\n    }\n    if (epochs == null) {\n      epochs = 1;\n    }\n    if (shuffle == null) {\n      shuffle = true;\n    }\n    if (initialEpoch == null) {\n      initialEpoch = 0;\n    }\n    // TODO(cais): Change const to let below when implementing validation.\n    let doValidation = false;\n    if (valF != null && valIns != null) {\n      doValidation = true;\n      // TODO(cais): verbose message.\n    }\n    if (validationSteps != null) {\n      doValidation = true;\n      if (stepsPerEpoch == null) {\n        throw new ValueError('Can only use `validationSteps` when doing step-wise training, ' + 'i.e., `stepsPerEpoch` must be set.');\n      }\n    }\n    const numTrainSamples = this.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n    let indexArray;\n    if (numTrainSamples != null) {\n      indexArray = range(0, numTrainSamples);\n    }\n    if (verbose == null) {\n      verbose = 1;\n    }\n    const {\n      callbackList,\n      history\n    } = configureCallbacks(callbacks, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n    callbackList.setModel(this);\n    this.history = history;\n    await callbackList.onTrainBegin();\n    this.stopTraining_ = false;\n    // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n    // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n    for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n      await callbackList.onEpochBegin(epoch);\n      const epochLogs = {};\n      if (stepsPerEpoch != null) {\n        throw new NotImplementedError('stepsPerEpoch mode is not implemented yet.');\n      } else {\n        if (shuffle === 'batch') {\n          throw new NotImplementedError('batch shuffling is not implemneted' + ' yet');\n        } else if (shuffle) {\n          util.shuffle(indexArray);\n        }\n        // Convert the potentially shuffled indices to Tensor1D, to avoid the\n        // cost of repeated creation of Array1Ds later on.\n        const epochIndexArray1D = tensor1d(indexArray);\n        const batches = makeBatches(numTrainSamples, batchSize);\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchLogs = {};\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n          tfc.tidy(() => {\n            const batchStart = batches[batchIndex][0];\n            const batchEnd = batches[batchIndex][1];\n            const batchIds = K.sliceAlongFirstAxis(epochIndexArray1D, batchStart, batchEnd - batchStart);\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = batchEnd - batchStart;\n            // TODO(cais): In ins, train flag can be a number, instead of an\n            //   Tensor? Do we need to handle this in tfjs-layers?\n            const insBatch = sliceArraysByIndices(ins, batchIds);\n            const outs = f(insBatch);\n            for (let i = 0; i < outLabels.length; ++i) {\n              const label = outLabels[i];\n              const out = outs[i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n              // TODO(cais): Use scope() to avoid ownership.\n            }\n            if (batchIndex === batches.length - 1) {\n              // Last batch.\n              if (doValidation) {\n                const valOuts = this.testLoop(valF, valIns, batchSize);\n                // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                for (let i = 0; i < outLabels.length; ++i) {\n                  const label = outLabels[i];\n                  const out = valOuts[i];\n                  tfc.keep(out);\n                  // TODO(cais): Use scope() to avoid ownership.\n                  epochLogs['val_' + label] = out;\n                }\n              }\n            }\n          });\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n          if (this.stopTraining_) {\n            break;\n          }\n          // TODO(cais): return outs as list of Tensor.\n        }\n        epochIndexArray1D.dispose();\n      }\n      // TODO(cais): Run validation at the end of the epoch.\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      if (this.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n    await this.history.syncData();\n    return this.history;\n  }\n  // TODO(cais): Add code snippet below when it's possible to instantiate\n  //   actual dataset objects.\n  /**\n   * Trains the model using a dataset object.\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for training. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs.\n   *   Of the two items in the array, the first is the input feature(s) and\n   *   the second is the output target(s).\n   * @param args A `ModelFitDatasetArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fitDataset(dataset, args) {\n    return fitDataset(this, dataset, args);\n  }\n  /**\n   * Runs a single gradient update on a single batch of data.\n   *\n   * This method differs from `fit()` and `fitDataset()` in the following\n   * regards:\n   *   - It operates on exactly one batch of data.\n   *   - It returns only the loss and metric values, instead of\n   *     returning the batch-by-batch loss and metric values.\n   *   - It doesn't support fine-grained options such as verbosity and\n   *     callbacks.\n   *\n   * @param x Input data. It could be one of the following:\n   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n   *     multiple inputs).\n   *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n   *     model has named inputs).\n   * @param y Target data. It could be either a `tf.Tensor` or multiple\n   *   `tf.Tensor`s. It should be consistent with `x`.\n   * @returns Training loss or losses (in case the model has\n   *   multiple outputs), along with metrics (if any), as numbers.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async trainOnBatch(x, y) {\n    // TODO(cais): Support sampleWeight and classWeight.\n    // TODO(cais): Support Dataset objects.\n    const standardizeOut = await this.standardizeUserData(x, y);\n    const inputs = standardizeOut[0];\n    const targets = standardizeOut[1];\n    const trainFunction = this.makeTrainFunction();\n    const losses = trainFunction(inputs.concat(targets));\n    const lossValues = [];\n    for (const loss of losses) {\n      const v = await loss.data();\n      lossValues.push(v[0]);\n    }\n    tfc.dispose(losses);\n    disposeNewTensors(standardizeOut[0], x);\n    disposeNewTensors(standardizeOut[1], y);\n    return singletonOrArray(lossValues);\n  }\n  /**\n   * Extract weight values of the model.\n   *\n   * @param config: An instance of `io.SaveConfig`, which specifies\n   * model-saving options such as whether only trainable weights are to be\n   * saved.\n   * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n   *   non-uniqueified weight names) to their values.\n   */\n  getNamedWeights(config) {\n    const namedWeights = [];\n    const trainableOnly = config != null && config.trainableOnly;\n    const weights = trainableOnly ? this.trainableWeights : this.weights;\n    const weightValues = this.getWeights(trainableOnly);\n    for (let i = 0; i < weights.length; ++i) {\n      if (trainableOnly && !weights[i].trainable) {\n        // Optionally skip non-trainable weights.\n        continue;\n      }\n      namedWeights.push({\n        name: weights[i].originalName,\n        tensor: weightValues[i]\n      });\n    }\n    return namedWeights;\n  }\n  /**\n   * Setter used for force stopping of LayersModel.fit() (i.e., training).\n   *\n   * Example:\n   *\n   * ```js\n   * const input = tf.input({shape: [10]});\n   * const output = tf.layers.dense({units: 1}).apply(input);\n   * const model = tf.model({inputs: [input], outputs: [output]});\n   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n   * const xs = tf.ones([8, 10]);\n   * const ys = tf.zeros([8, 1]);\n   *\n   * const history = await model.fit(xs, ys, {\n   *   epochs: 10,\n   *   callbacks: {\n   *     onEpochEnd: async (epoch, logs) => {\n   *       if (epoch === 2) {\n   *         model.stopTraining = true;\n   *       }\n   *     }\n   *   }\n   * });\n   *\n   * // There should be only 3 values in the loss array, instead of 10\n   * values,\n   * // due to the stopping after 3 epochs.\n   * console.log(history.history.loss);\n   * ```\n   */\n  set stopTraining(stop) {\n    this.stopTraining_ = stop;\n  }\n  get stopTraining() {\n    return this.stopTraining_;\n  }\n  get optimizer() {\n    return this.optimizer_;\n  }\n  set optimizer(optimizer) {\n    if (this.optimizer_ !== optimizer) {\n      this.optimizer_ = optimizer;\n      this.isOptimizerOwned = false;\n    }\n  }\n  dispose() {\n    const result = super.dispose();\n    if (result.refCountAfterDispose === 0 && this.optimizer != null && this.isOptimizerOwned) {\n      const numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n      this.optimizer_.dispose();\n      result.numDisposedVariables += numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n    }\n    return result;\n  }\n  getLossIdentifiers() {\n    let lossNames;\n    if (typeof this.loss === 'string') {\n      lossNames = toSnakeCase(this.loss);\n    } else if (Array.isArray(this.loss)) {\n      for (const loss of this.loss) {\n        if (typeof loss !== 'string') {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n      lossNames = this.loss.map(name => toSnakeCase(name));\n    } else {\n      const outputNames = Object.keys(this.loss);\n      lossNames = {};\n      const losses = this.loss;\n      for (const outputName of outputNames) {\n        if (typeof losses[outputName] === 'string') {\n          lossNames[outputName] = toSnakeCase(losses[outputName]);\n        } else {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n    }\n    return lossNames;\n  }\n  getMetricIdentifiers() {\n    if (typeof this.metrics === 'string' || typeof this.metrics === 'function') {\n      return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n    } else if (Array.isArray(this.metrics)) {\n      return this.metrics.map(metric => toSnakeCase(Metrics.getLossOrMetricName(metric)));\n    } else {\n      const metricsIdentifiers = {};\n      for (const key in this.metrics) {\n        metricsIdentifiers[key] = toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n      }\n      return metricsIdentifiers;\n    }\n  }\n  getTrainingConfig() {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      }\n    };\n    // TODO(cais): Add weight_metrics when they are supported.\n    // TODO(cais): Add sample_weight_mode when it's supported.\n    // TODO(cais): Add loss_weights when it's supported.\n  }\n  loadTrainingConfig(trainingConfig) {\n    if (trainingConfig.weighted_metrics != null) {\n      throw new Error('Loading weight_metrics is not supported yet.');\n    }\n    if (trainingConfig.loss_weights != null) {\n      throw new Error('Loading loss_weights is not supported yet.');\n    }\n    if (trainingConfig.sample_weight_mode != null) {\n      throw new Error('Loading sample_weight_mode is not supported yet.');\n    }\n    const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config);\n    const optimizer = deserialize(tsConfig);\n    let loss;\n    if (typeof trainingConfig.loss === 'string') {\n      loss = toCamelCase(trainingConfig.loss);\n    } else if (Array.isArray(trainingConfig.loss)) {\n      loss = trainingConfig.loss.map(lossEntry => toCamelCase(lossEntry));\n    } else if (trainingConfig.loss != null) {\n      loss = {};\n      for (const key in trainingConfig.loss) {\n        loss[key] = toCamelCase(trainingConfig.loss[key]);\n      }\n    }\n    let metrics;\n    if (Array.isArray(trainingConfig.metrics)) {\n      metrics = trainingConfig.metrics.map(metric => toCamelCase(metric));\n    } else if (trainingConfig.metrics != null) {\n      metrics = {};\n      for (const key in trainingConfig.metrics) {\n        metrics[key] = toCamelCase(trainingConfig.metrics[key]);\n      }\n    }\n    this.compile({\n      loss,\n      metrics,\n      optimizer\n    });\n  }\n  /**\n   * Save the configuration and/or weights of the LayersModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 2. Saving `model`'s topology and weights to browser\n   * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('indexeddb://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 3. Saving `model`'s topology and weights as two files\n   * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n   * browser.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('downloads://my-model-1');\n   * ```\n   *\n   * Example 4. Send  `model`'s topology and weights to an HTTP server.\n   * See the documentation of `tf.io.http` for more details\n   * including specifying request parameters and implementation of the\n   * server.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('http://my-server/model/upload');\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL, config) {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new ValueError(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new ValueError(`Found more than one (${handlers.length}) save handlers for ` + `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new ValueError('LayersModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n    }\n    const weightDataAndSpecs = await io.encodeWeights(this.getNamedWeights(config));\n    const returnString = false;\n    const unusedArg = null;\n    const modelConfig = this.toJSON(unusedArg, returnString);\n    const modelArtifacts = {\n      modelTopology: modelConfig,\n      format: LAYERS_MODEL_FORMAT_NAME,\n      generatedBy: `TensorFlow.js tfjs-layers v${version}`,\n      convertedBy: null\n    };\n    const includeOptimizer = config == null ? false : config.includeOptimizer;\n    if (includeOptimizer && this.optimizer != null) {\n      modelArtifacts.trainingConfig = this.getTrainingConfig();\n      const weightType = 'optimizer';\n      const {\n        data: optimizerWeightData,\n        specs: optimizerWeightSpecs\n      } = await io.encodeWeights(await this.optimizer.getWeights(), weightType);\n      weightDataAndSpecs.specs.push(...optimizerWeightSpecs);\n      weightDataAndSpecs.data = io.concatenateArrayBuffers([weightDataAndSpecs.data, optimizerWeightData]);\n    }\n    if (this.userDefinedMetadata != null) {\n      // Check serialized size of user-defined metadata.\n      const checkSize = true;\n      checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n      modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n    }\n    modelArtifacts.weightData = weightDataAndSpecs.data;\n    modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n    return handlerOrURL.save(modelArtifacts);\n  }\n  /**\n   * Set user-defined metadata.\n   *\n   * The set metadata will be serialized together with the topology\n   * and weights of the model during `save()` calls.\n   *\n   * @param setUserDefinedMetadata\n   */\n  setUserDefinedMetadata(userDefinedMetadata) {\n    checkUserDefinedMetadata(userDefinedMetadata, this.name);\n    this.userDefinedMetadata = userDefinedMetadata;\n  }\n  /**\n   * Get user-defined metadata.\n   *\n   * The metadata is supplied via one of the two routes:\n   *   1. By calling `setUserDefinedMetadata()`.\n   *   2. Loaded during model loading (if the model is constructed\n   *      via `tf.loadLayersModel()`.)\n   *\n   * If no user-defined metadata is available from either of the\n   * two routes, this function will return `undefined`.\n   */\n  getUserDefinedMetadata() {\n    return this.userDefinedMetadata;\n  }\n}\n// The class name is 'Model' rather than 'LayersModel' for backwards\n// compatibility since this class name shows up in the serialization format.\n/** @nocollapse */\nLayersModel.className = 'Model';\nexport { LayersModel };\nserialization.registerClass(LayersModel);\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nclass Functional extends LayersModel {}\nFunctional.className = 'Functional';\nexport { Functional };\nserialization.registerClass(Functional);","map":{"version":3,"names":["tfc","io","Optimizer","scalar","serialization","Tensor","tensor1d","util","K","configureCallbacks","standardizeCallbacks","nameScope","NotImplementedError","RuntimeError","ValueError","deserialize","disposeTensorsInLogs","losses","Metrics","optimizers","checkUserDefinedMetadata","count","pyListRepeat","singletonOrArray","toCamelCase","toSnakeCase","unique","printSummary","range","convertPythonicToTs","version","Container","execute","FeedDict","evaluateDataset","fitDataset","checkBatchSize","disposeNewTensors","ensureTensorsRank2OrHigher","makeBatches","sliceArrays","sliceArraysByIndices","computeWeightedLoss","standardizeClassWeights","standardizeWeights","isDataTensor","x","isDataArray","Array","isArray","isDataDict","standardizeInputData","data","names","shapes","checkBatchAxis","exceptionPrefix","length","gotUnexpectedData","key","hasOwnProperty","map","name","arrays","push","shape","i","array","j","dim","refDim","slice","checkArrayLengths","inputs","targets","weights","setX","input","sort","setY","target","JSON","stringify","arraysEqual","checkLossAndTargetCompatibility","lossFns","outputShapes","keyLosses","meanSquaredError","binaryCrossentropy","categoricalCrossentropy","y","loss","indexOf","slicedYShape","slicedShape","targetDim","outDim","checkInputData","collectMetrics","metrics","outputNames","wrappedMetrics","TypeError","nestedMetrics","outputMetrics","LAYERS_MODEL_FORMAT_NAME","LayersModel","constructor","args","isTraining","summary","lineLength","positions","printFn","console","log","built","compile","optimizer","optimizer_","getOptimizer","isOptimizerOwned","lossFunctions","warn","get","outputs","theLosses","l","lossFunction","forEach","_","feedOutputNames","feedOutputShapes","feedLossFns","internalOutputShapes","skipTargetIndices","metricsNames","metricsTensors","weightedLoss","appendMetric","outputIndex","metricName","metricTensor","handleMetrics","metricNamePrefix","accFn","weightedMetricFn","metric","outputShape","binaryAccuracy","sparseCategoricalCrossentropy","sparseCategoricalAccuracy","categoricalAccuracy","suffix","metricFn","getLossOrMetricName","metricResult","collectedTrainableWeights","trainableWeights","checkTrainableWeightsConsistency","evaluate","batchSize","standardizedOuts","standardizeUserDataXY","ins","concat","makeTestFunction","f","testFunction","testOuts","testLoop","verbose","steps","dataset","checkNumSamples","stepsName","numSamples","outputsIsArray","outputSymbolicTensors","retrieveSymbolicTensors","feedDict","add","tensorValue","executeOutputs","symbolicTensorNames","outputsRemaining","layer","layers","layerOutputs","output","layerOutputNames","index","remainingNames","tensor","predictLoop","tidy","batches","outsBatches","batchIndex","batchOuts","batchStart","batchEnd","insBatch","feeds","value","batchOut","predict","xsRank2OrHigher","inputNames","feedInputShapes","predictOnBatch","lossFn","feedInputNames","stateful","standardizeUserData","sampleWeight","classWeight","standardXs","standardYs","Error","standardSampleWeights","classWeights","outs","indexArray","batchIds","sliceAlongFirstAxis","mul","div","getDedupedMetricsNames","outLabels","dedupedOutLabels","label","newLabel","dupIndex","makeTrainFunction","lossValues","sampleWeights","metricsValues","totalLossFunction","totalLoss","meanLoss","mean","weightedMetric","keep","calculateLosses","regularizerLoss","variables","param","read","returnCost","totalLossValue","minimize","valOutputs","meanMetric","fit","originalInputs","originalTargets","inputValX","inputValY","valX","valY","doValidation","valIns","validationData","valStandardized","validationSplit","splitAt","Math","floor","originalBatchSize","validationSteps","trainFunction","valFunction","callbackMetrics","n","callbacks","yieldEvery","out","fitLoop","epochs","shuffle","initialEpoch","dispose","valF","stepsPerEpoch","numTrainSamples","callbackList","history","setModel","onTrainBegin","stopTraining_","epoch","onEpochBegin","epochLogs","epochIndexArray1D","batchLogs","onBatchBegin","valOuts","onBatchEnd","onEpochEnd","onTrainEnd","syncData","trainOnBatch","standardizeOut","v","getNamedWeights","config","namedWeights","trainableOnly","weightValues","getWeights","trainable","originalName","stopTraining","stop","result","refCountAfterDispose","numTensorsBeforeOptmizerDisposal","memory","numTensors","numDisposedVariables","getLossIdentifiers","lossNames","Object","keys","outputName","getMetricIdentifiers","metricsIdentifiers","getTrainingConfig","optimizer_config","class_name","getClassName","getConfig","loadTrainingConfig","trainingConfig","weighted_metrics","loss_weights","sample_weight_mode","tsConfig","lossEntry","save","handlerOrURL","handlers","getSaveHandlers","weightDataAndSpecs","encodeWeights","returnString","unusedArg","modelConfig","toJSON","modelArtifacts","modelTopology","format","generatedBy","convertedBy","includeOptimizer","weightType","optimizerWeightData","specs","optimizerWeightSpecs","concatenateArrayBuffers","userDefinedMetadata","checkSize","weightData","weightSpecs","setUserDefinedMetadata","getUserDefinedMetadata","className","registerClass","Functional"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-layers/src/engine/training.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: engine/training.py */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {io, ModelPredictConfig as ModelPredictArgs, NamedTensorMap, Optimizer, Scalar, scalar, serialization, Tensor, Tensor1D, tensor1d, util} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {BaseCallback, configureCallbacks, History, ModelLoggingVerbosity, standardizeCallbacks} from '../base_callbacks';\nimport {nameScope} from '../common';\nimport {NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {LossIdentifier} from '../keras_format/loss_config';\nimport {OptimizerSerialization} from '../keras_format/optimizer_config';\nimport {MetricsIdentifier, TrainingConfig} from '../keras_format/training_config';\nimport {deserialize} from '../layers/serialization';\nimport { disposeTensorsInLogs, UnresolvedLogs } from '../logs';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport {LossOrMetricFn, NamedTensor} from '../types';\nimport {checkUserDefinedMetadata} from '../user_defined_metadata';\nimport {count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique} from '../utils/generic_utils';\nimport {printSummary} from '../utils/layer_utils';\nimport {range} from '../utils/math_utils';\nimport {convertPythonicToTs} from '../utils/serialization_utils';\nimport {LayerVariable} from '../variables';\nimport {version} from '../version';\n\nimport {Container, ContainerArgs} from './container';\nimport {Dataset} from './dataset_stub';\nimport {execute, FeedDict} from './executor';\nimport {DisposeResult, SymbolicTensor} from './topology';\nimport {evaluateDataset, fitDataset, ModelEvaluateDatasetArgs, ModelFitDatasetArgs} from './training_dataset';\nimport {checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, makeBatches, ModelFitArgs, sliceArrays, sliceArraysByIndices} from './training_tensors';\nimport {ClassWeight, ClassWeightMap, computeWeightedLoss, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x: Tensor|Tensor[]|{[inputName: string]: Tensor}|\n                             {[inputName: string]: Tensor[]}): boolean {\n  return x instanceof Tensor;\n}\n\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x: Tensor|Tensor[]|\n                            {[inputName: string]: Tensor}): boolean {\n  return Array.isArray(x);\n}\n\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x: Tensor|Tensor[]|\n                           {[inputName: string]: Tensor}): boolean {\n  return !isDataTensor(x) && !isDataArray(x);\n}\n\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(\n    data: Tensor|Tensor[]|{[inputName: string]: Tensor}, names: string[],\n    shapes?: Shape[], checkBatchAxis = true, exceptionPrefix = ''): Tensor[] {\n  if (names == null || names.length === 0) {\n    // Check for the case where the model expected no data, but some data got\n    // sent.\n    if (data != null) {\n      let gotUnexpectedData = false;\n      if (isDataArray(data) && (data as Tensor[]).length > 0) {\n        gotUnexpectedData = true;\n      } else if (isDataDict(data)) {\n        for (const key in data) {\n          if (data.hasOwnProperty(key)) {\n            gotUnexpectedData = true;\n            break;\n          }\n        }\n      } else {\n        // `data` is a singleton Tensor in this case.\n        gotUnexpectedData = true;\n      }\n      if (gotUnexpectedData) {\n        throw new ValueError(\n            `Error when checking model ${exceptionPrefix} expected no data, ` +\n            `but got ${data}`);\n      }\n    }\n    return [];\n  }\n  if (data == null) {\n    return names.map(name => null);\n  }\n\n  let arrays: Tensor[];\n  if (isDataDict(data)) {\n    data = data as {[inputName: string]: Tensor};\n    arrays = [];\n    for (const name of names) {\n      if (data[name] == null) {\n        throw new ValueError(\n            `No data provided for \"${name}\". Need data for each key in: ` +\n            `${names}`);\n      }\n      arrays.push(data[name]);\n    }\n  } else if (isDataArray(data)) {\n    data = data as Tensor[];\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `model expected. Expected to see ${names.length} Tensor(s), but ` +\n          `instead got the following list of Tensor(s): ${data}`);\n    }\n    arrays = data;\n  } else {\n    data = data as Tensor;\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model ${exceptionPrefix} expects ${names.length} Tensor(s), ` +\n          `but only received one Tensor. Found: Tensor with shape ${\n              data.shape}`);\n    }\n    arrays = [data];\n  }\n\n  arrays = ensureTensorsRank2OrHigher(arrays);\n\n  // Check shape compatibility.\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s). but got array with ` +\n            `shape ${array.shape}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          // Skip the first (batch) axis.\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null && refDim >= 0 && dim !== refDim) {\n          throw new ValueError(\n              `${exceptionPrefix} expected a batch of elements where each ` +\n              `example has shape [${shapes[i].slice(1, shapes[i].length)}] ` +\n              `(i.e.,tensor shape [*,${\n                  shapes[i].slice(1, shapes[i].length)}])` +\n              ` but the ${exceptionPrefix} received an input with ${\n                  array.shape[0]}` +\n              ` examples, each with shape [${\n                  array.shape.slice(1, array.shape.length)}]` +\n              ` (tensor shape [${array.shape}])`);\n        }\n      }\n    }\n  }\n  return arrays;\n}\n\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(\n    inputs: Tensor[], targets: Tensor[], weights?: Tensor[]) {\n  const setX = unique(inputs.map(input => input.shape[0]));\n  setX.sort();\n  const setY = unique(targets.map(target => target.shape[0]));\n  setY.sort();\n  // TODO(cais): Check `weights` as well.\n  if (setX.length > 1) {\n    throw new ValueError(\n        `All input Tensors (x) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(inputs.map(input => input.shape))}`);\n  }\n  if (setY.length > 1) {\n    throw new ValueError(\n        `All target Tensors (y) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(targets.map(target => target.shape))}`);\n  }\n  if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n    throw new ValueError(\n        `Input Tensors should have the same number of samples as target ` +\n        `Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target ` +\n        `sample(s).`);\n  }\n}\n\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(\n    targets: Tensor[], lossFns: LossOrMetricFn[], outputShapes: Shape[]) {\n  // TODO(cais): Dedicated test coverage?\n  const keyLosses = [\n    losses.meanSquaredError, losses.binaryCrossentropy,\n    losses.categoricalCrossentropy\n  ];\n  for (let i = 0; i < targets.length; ++i) {\n    const y = targets[i];\n    const loss = lossFns[i];\n    const shape = outputShapes[i];\n    if (loss == null) {\n      continue;\n    }\n    if (loss === losses.categoricalCrossentropy) {\n      if (y.shape[y.shape.length - 1] === 1) {\n        throw new ValueError(\n            `You are passing a target array of shape ${y.shape} while using ` +\n            `a loss 'categorical_crossentropy'. 'categorical_crossentropy'` +\n            `expects targets to be binary matrices (1s and 0s) of shape ` +\n            `[samples, classes].`);\n        // TODO(cais): Example code in error message.\n      }\n    }\n    if (keyLosses.indexOf(loss) !== -1) {\n      const slicedYShape = y.shape.slice(1);\n      const slicedShape = shape.slice(1);\n      for (let j = 0; j < slicedYShape.length; ++j) {\n        const targetDim = slicedYShape[j];\n        const outDim = slicedShape[j];\n        if (outDim != null && targetDim !== outDim) {\n          throw new ValueError(\n              `A target Tensor with shape ${y.shape} was passed for an ` +\n              `output of shape ${shape}, while using a loss function that ` +\n              `expects targets to have the same shape as the output.`);\n        }\n      }\n    }\n  }\n}\n\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(\n    data: Tensor|Tensor[], names: string[], shapes?: Shape[],\n    checkBatchAxis = true, exceptionPrefix = '') {\n  let arrays: Tensor[];\n  if (Array.isArray(data)) {\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `the model expected. Expected to see ${names.length} Tensor(s),` +\n          ` but instead got ${data.length} Tensors(s).`);\n    }\n    arrays = data;\n  } else {\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model expects ${names.length} ${exceptionPrefix} Tensors, ` +\n          `but only received one Tensor. Found: array with shape ` +\n          `${JSON.stringify(data.shape)}.`);\n    }\n    arrays = [data];\n  }\n\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s), but got array with ` +\n            `shape ${JSON.stringify(array.shape)}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null) {\n          if (refDim !== dim) {\n            throw new ValueError(\n                `Error when checking ${exceptionPrefix}: expected ` +\n                `${names[i]} to have shape ${JSON.stringify(shapes[i])} but ` +\n                `got array with shape ${JSON.stringify(array.shape)}.`);\n          }\n        }\n      }\n    }\n  }\n}\n\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(\n    metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n    {[outputName: string]: string | LossOrMetricFn},\n    outputNames: string[]): Array<Array<string|LossOrMetricFn>> {\n  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n    return outputNames.map(name => []);\n  }\n\n  let wrappedMetrics: Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  if (typeof metrics === 'string' || typeof metrics === 'function') {\n    wrappedMetrics = [metrics];\n  } else if (Array.isArray(metrics) || typeof metrics === 'object') {\n    wrappedMetrics = metrics as Array<string|LossOrMetricFn>|\n        {[outputName: string]: string} | {[outputName: string]: LossOrMetricFn};\n  } else {\n    throw new TypeError(\n        'Type of metrics argument not understood. Expected an string,' +\n        `function, Array, or Object, found: ${metrics}`);\n  }\n\n  if (Array.isArray(wrappedMetrics)) {\n    // We then apply all metrics to all outputs.\n    return outputNames.map(\n        name => wrappedMetrics as Array<string|LossOrMetricFn>);\n  } else {\n    // In this case, metrics is a dict.\n    const nestedMetrics: Array<Array<string|LossOrMetricFn>> = [];\n    for (const name of outputNames) {\n      let outputMetrics: string|LossOrMetricFn|Array<string|LossOrMetricFn> =\n          wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n      if (!Array.isArray(outputMetrics)) {\n        outputMetrics = [outputMetrics];\n      }\n      nestedMetrics.push(outputMetrics);\n    }\n    return nestedMetrics;\n  }\n}\n\nexport interface ModelEvaluateArgs {\n  /**\n   * Batch size (Integer). If unspecified, it will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * Tensor of weights to weight the contribution of different samples to the\n   * loss and metrics.\n   */\n  sampleWeight?: Tensor;\n\n  /**\n   * integer: total number of steps (batches of samples)\n   * before declaring the evaluation round finished. Ignored with the default\n   * value of `undefined`.\n   */\n  steps?: number;\n}\n\n/**\n * Configuration for calls to `LayersModel.compile()`.\n */\nexport interface ModelCompileArgs {\n  /**\n   * An instance of `tf.train.Optimizer` or a string name for an Optimizer.\n   */\n  optimizer: string|Optimizer;\n\n  /**\n   * Object function(s) or name(s) of object function(s).\n   * If the model has multiple outputs, you can use a different loss\n   * on each output by passing a dictionary or an Array of losses.\n   * The loss value that will be minimized by the model will then be the sum\n   * of all individual losses.\n   */\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n\n  /**\n   * List of metrics to be evaluated by the model during training and testing.\n   * Typically you will use `metrics=['accuracy']`.\n   * To specify different metrics for different outputs of a multi-output\n   * model, you could also pass a dictionary.\n   */\n  metrics?: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n\n  // TODO(cais): Add lossWeights, sampleWeightMode, weightedMetrics, and\n  //   targetTensors.\n}\n\nconst LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class LayersModel extends Container implements tfc.InferenceModel {\n  // The class name is 'Model' rather than 'LayersModel' for backwards\n  // compatibility since this class name shows up in the serialization format.\n  /** @nocollapse */\n  static className = 'Model';\n  protected optimizer_: Optimizer;\n  // Whether the model instance owns the optimizer: `true` if and only if\n  // `optimizer` is created from a string parameter during `compile()` call.\n  protected isOptimizerOwned: boolean;\n\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n  lossFunctions: LossOrMetricFn[];\n\n  // TODO(cais): These private variables should probably not have the string\n  //   'feed' in their names, because we are not dealing with a symbolic\n  //   backend.\n  private feedOutputShapes: Shape[];\n  private feedLossFns: LossOrMetricFn[];\n  private collectedTrainableWeights: LayerVariable[];\n  private testFunction: (data: Tensor[]) => Scalar[];\n  history: History;\n\n  // A public property that can be set by Callbacks to order early stopping\n  // during `fit()` calls.\n  protected stopTraining_: boolean;\n  protected isTraining: boolean;\n\n  metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  metricsNames: string[];\n  // Porting Note: `metrics_tensors` in PyKeras is a symbolic tensor. But given\n  //   the imperative nature of tfjs-core, `metricsTensors` is a\n  //   TypeScript function here.\n  //   Also note that due to the imperative nature of tfjs-core, `metricsTensor`\n  //   here needs an output index to keep track of which output of the\n  //   LayersModel a metric belongs to. This is unlike `metrics_tensors` in\n  //   PyKeras, which is a `list` of symbolic tensors, each of which has\n  //   implicit \"knowledge\" of the outputs it depends on.\n  metricsTensors: Array<[LossOrMetricFn, number]>;\n\n  // User defind metadata (if any).\n  private userDefinedMetadata: {};\n\n  constructor(args: ContainerArgs) {\n    super(args);\n    this.isTraining = false;\n  }\n\n  /**\n   * Print a text summary of the model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - If the model has non-sequential-like topology, the inputs each layer\n   *   receives\n   * - The total number of trainable and non-trainable parameters of the model.\n   *\n   * ```js\n   * const input1 = tf.input({shape: [10]});\n   * const input2 = tf.input({shape: [20]});\n   * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n   * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n   * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n   * const output =\n   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n   *\n   * const model = tf.model({inputs: [input1, input2], outputs: output});\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  summary(\n      lineLength?: number, positions?: number[],\n      printFn:\n          // tslint:disable-next-line:no-any\n      (message?: any, ...optionalParams: any[]) => void = console.log) {\n    if (!this.built) {\n      throw new ValueError(\n          `This model has never been called, thus its weights have not been ` +\n          `created yet. So no summary can be displayed. Build the model ` +\n          `first (e.g., by calling it on some test data).`);\n    }\n    printSummary(this, lineLength, positions, printFn);\n  }\n\n  /**\n   * Configures and prepares the model for training and evaluation.  Compiling\n   * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n   * or `evaluate` on an un-compiled model will throw an error.\n   *\n   * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n   * metrics to be used for fitting and evaluating this model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  compile(args: ModelCompileArgs): void {\n    if (args.loss == null) {\n      args.loss = [];\n    }\n    this.loss = args.loss;\n\n    if (typeof args.optimizer === 'string') {\n      this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n      this.isOptimizerOwned = true;\n    } else {\n      if (!(args.optimizer instanceof Optimizer)) {\n        throw new ValueError(\n            `User-defined optimizer must be an instance of tf.Optimizer.`);\n      }\n      this.optimizer_ = args.optimizer;\n      this.isOptimizerOwned = false;\n    }\n\n    // TODO(cais): Add lossWeights.\n    // TODO(cais): Add sampleWeightMode.\n\n    // Prepare loss functions.\n    let lossFunctions: LossOrMetricFn[] = [];\n    if (!Array.isArray(args.loss) && typeof args.loss !== 'string' &&\n        typeof args.loss !== 'function') {\n      args.loss = args.loss as {[outputName: string]: string};\n      for (const name in args.loss) {\n        if (this.outputNames.indexOf(name) === -1) {\n          throw new ValueError(\n              `Unknown entry in loss dictionary: \"${name}\". ` +\n              `Only expected the following keys: ${this.outputNames}`);\n        }\n      }\n      for (const name of this.outputNames) {\n        if (args.loss[name] == null) {\n          console.warn(\n              `Output \"${name}\" is missing from loss dictionary. We assume ` +\n              `this was done on purpose, and we will not be expecting data ` +\n              `to be passed to ${name} during training`);\n        }\n        lossFunctions.push(losses.get(args.loss[name]));\n      }\n    } else if (Array.isArray(args.loss)) {\n      if (args.loss.length !== this.outputs.length) {\n        throw new ValueError(\n            `When passing an Array as loss, it should have one entry per ` +\n            `model output. The model has ${this.outputs.length} output(s), ` +\n            `but you passed loss=${args.loss}.`);\n      }\n      const theLosses = args.loss as Array<string|LossOrMetricFn>;\n      lossFunctions = theLosses.map(l => losses.get(l));\n    } else {\n      const lossFunction = losses.get(args.loss);\n      this.outputs.forEach(_ => {\n        lossFunctions.push(lossFunction);\n      });\n    }\n\n    this.lossFunctions = lossFunctions;\n\n    this.feedOutputNames = [];\n    this.feedOutputShapes = [];\n    this.feedLossFns = [];\n    for (let i = 0; i < this.outputs.length; ++i) {\n      // TODO(cais): Logic for skipping target(s).\n      const shape = this.internalOutputShapes[i];\n      const name = this.outputNames[i];\n      this.feedOutputNames.push(name);\n      this.feedOutputShapes.push(shape);\n      this.feedLossFns.push(this.lossFunctions[i]);\n    }\n\n    // TODO(cais): Add logic for output masks.\n    // TODO(cais): Add logic for sample weights.\n    const skipTargetIndices: number[] = [];\n\n    // Prepare metrics.\n    this.metrics = args.metrics;\n    // TODO(cais): Add weightedMetrics.\n    this.metricsNames = ['loss'];\n    this.metricsTensors = [];\n\n    // Compute total loss.\n    // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n    //   Here, metricsTensors are TypeScript functions. This difference is due\n    //   to the difference in symbolic/imperative property of the backends.\n    nameScope('loss', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        // TODO(cais): Add weightedLoss, sampleWeight and mask.\n        //   The following line should be weightedLoss\n        const weightedLoss = this.lossFunctions[i];\n        if (this.outputs.length > 1) {\n          this.metricsTensors.push([weightedLoss, i]);\n          this.metricsNames.push(this.outputNames[i] + '_loss');\n        }\n      }\n\n      // Porting Note: Due to the imperative nature of the backend, we calculate\n      //   the regularizer penalties in the totalLossFunction, instead of here.\n    });\n\n    const nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n    // TODO(cais): Add nestedWeightedMetrics.\n\n    /**\n     * Helper function used in loop below.\n     */\n    const appendMetric =\n        (outputIndex: number, metricName: string,\n         metricTensor: LossOrMetricFn) => {\n          if (this.outputNames.length > 1) {\n            metricName = this.outputNames[outputIndex] + '_' + metricName;\n          }\n          this.metricsNames.push(metricName);\n          this.metricsTensors.push([metricTensor, outputIndex]);\n        };\n\n    nameScope('metric', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        const outputMetrics = nestedMetrics[i];\n        // TODO(cais): Add weights and outputWeightedMetrics.\n\n        // TODO(cais): Add optional arg `weights` to the following function.\n        const handleMetrics = (metrics: Array<string|LossOrMetricFn>) => {\n          const metricNamePrefix = '';\n          let metricName: string;\n          let accFn: LossOrMetricFn;\n          let weightedMetricFn: LossOrMetricFn;\n          //  TODO(cais): Use 'weights_' for weighted metrics.\n\n          for (const metric of metrics) {\n            if (typeof metric === 'string' &&\n                ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !==\n                    -1) {\n              const outputShape = this.internalOutputShapes[i];\n\n              if (outputShape[outputShape.length - 1] === 1 ||\n                  this.lossFunctions[i] === losses.binaryCrossentropy) {\n                // case: binary accuracy/crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryCrossentropy;\n                }\n              } else if (\n                  this.lossFunctions[i] ===\n                  losses.sparseCategoricalCrossentropy) {\n                // case: categorical accuracy / crossentropy with sparse\n                // targets.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalCrossentropy;\n                }\n              } else {\n                // case: categorical accuracy / crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalCrossentropy;\n                }\n              }\n              let suffix: string;\n              if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                suffix = 'acc';\n              } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                suffix = 'ce';\n              }\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = accFn;\n              metricName = metricNamePrefix + suffix;\n            } else {\n              const metricFn = Metrics.get(metric);\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = metricFn;\n              metricName =\n                  metricNamePrefix + Metrics.getLossOrMetricName(metric);\n            }\n\n            // TODO(cais): Add weighting and masking to metricResult.\n            let metricResult: LossOrMetricFn;\n            nameScope(metricName, () => {\n              metricResult = weightedMetricFn;\n            });\n            appendMetric(i, metricName, metricResult);\n          }\n        };\n\n        handleMetrics(outputMetrics);\n        // TODO(cais): Call handleMetrics with weights.\n      }\n    });\n\n    // Porting Notes: Given the imperative backend of tfjs-core,\n    //   there is no need for constructing the symbolic graph and placeholders.\n    this.collectedTrainableWeights = this.trainableWeights;\n  }\n\n  /**\n   * Check trainable weights count consistency.\n   *\n   * This will raise a warning if `this.trainableWeights` and\n   * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n   * numbers of parameters).\n   * Inconsistency will typically arise when one modifies `model.trainable`\n   * without calling `model.compile()` again.\n   */\n  protected checkTrainableWeightsConsistency(): void {\n    if (this.collectedTrainableWeights == null) {\n      return;\n    }\n    if (this.trainableWeights.length !==\n        this.collectedTrainableWeights.length) {\n      console.warn(\n          'Discrepancy between trainableweights and collected trainable ' +\n          'weights. Did you set `model.trainable` without calling ' +\n          '`model.compile()` afterwards?');\n    }\n  }\n\n  /**\n   * Returns the loss value & metrics values for the model in test mode.\n   *\n   * Loss and metrics are specified during `compile()`, which needs to happen\n   * before calls to `evaluate()`.\n   *\n   * Computation is done in batches.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const result = model.evaluate(\n   *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n   * result.print();\n   * ```\n   *\n   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple inputs.\n   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple outputs.\n   * @param args A `ModelEvaluateArgs`, containing optional fields.\n   *\n   * @return `Scalar` test loss (if the model has a single output and no\n   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n   *   and/or metrics). The attribute `model.metricsNames`\n   *   will give you the display labels for the scalar outputs.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  evaluate(\n      x: Tensor|Tensor[], y: Tensor|Tensor[],\n      args: ModelEvaluateArgs = {}): Scalar|Scalar[] {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize);\n\n    // TODO(cais): Standardize `config.sampleWeights` as well.\n    // Validate user data.\n    const checkBatchAxis = true;\n    const standardizedOuts =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    try {\n      // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n      // of the input to 0.\n      const ins = standardizedOuts[0].concat(standardizedOuts[1]);\n      this.makeTestFunction();\n      const f = this.testFunction;\n      const testOuts =\n          this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n      return singletonOrArray(testOuts);\n    } finally {\n      disposeNewTensors(standardizedOuts[0], x);\n      disposeNewTensors(standardizedOuts[1], y);\n    }\n  }\n\n  // TODO(cais): Add code snippet below once real dataset objects are\n  //   available.\n  /**\n   * Evaluate model using a dataset object.\n   *\n   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for evaluation. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs. Of the two items in the array, the\n   *   first is the input feature(s) and the second is the output target(s).\n   * @param args A configuration object for the dataset-based evaluation.\n   * @returns Loss and metric values as an Array of `Scalar` objects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async evaluateDataset(dataset: Dataset<{}>, args?: ModelEvaluateDatasetArgs):\n      Promise<Scalar|Scalar[]> {\n    this.makeTestFunction();\n    return evaluateDataset(this, dataset, args);\n  }\n\n  /**\n   * Get number of samples provided for training, evaluation or prediction.\n   *\n   * @param ins Input `tf.Tensor`.\n   * @param batchSize Integer batch size, optional.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring loop finished. Optional.\n   * @param stepsName The public API's parameter name for `steps`.\n   * @returns Number of samples provided.\n   */\n  private checkNumSamples(\n      ins: Tensor|Tensor[], batchSize?: number, steps?: number,\n      stepsName = 'steps'): number {\n    let numSamples: number;\n    if (steps != null) {\n      numSamples = null;\n      if (batchSize != null) {\n        throw new ValueError(\n            `If ${stepsName} is set, batchSize must be null or undefined.` +\n            `Got batchSize = ${batchSize}`);\n      }\n    } else if (ins != null) {\n      if (Array.isArray(ins)) {\n        numSamples = ins[0].shape[0];\n      } else {\n        numSamples = ins.shape[0];\n      }\n    } else {\n      throw new ValueError(\n          `Either the input data should have a defined shape, or ` +\n          `${stepsName} shoud be specified.`);\n    }\n    return numSamples;\n  }\n\n  /**\n   * Execute internal tensors of the model with input data feed.\n   * @param inputs Input data feed. Must match the inputs of the model.\n   * @param outputs Names of the output tensors to be fetched. Must match\n   *   names of the SymbolicTensors that belong to the graph.\n   * @returns Fetched values for `outputs`.\n   */\n  execute(inputs: Tensor|Tensor[]|NamedTensorMap, outputs: string|string[]):\n      Tensor|Tensor[] {\n    if (Array.isArray(outputs) && outputs.length === 0) {\n      throw new ValueError(\n          '`outputs` is an empty Array, which is not allowed.');\n    }\n\n    const outputsIsArray = Array.isArray(outputs);\n    const outputNames =\n        (outputsIsArray ? outputs : [outputs]);\n    const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n\n    // Format the input into a FeedDict.\n    const feedDict = new FeedDict();\n    if (inputs instanceof Tensor) {\n      inputs = [inputs];\n    }\n    if (Array.isArray(inputs)) {\n      if (inputs.length !== this.inputs.length) {\n        throw new ValueError(\n            `The number of inputs provided (${inputs.length}) ` +\n            `does not match the number of inputs of this model ` +\n            `(${this.inputs.length}).`);\n      }\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n    } else {\n      for (const input of this.inputs) {\n        const tensorValue = inputs[input.name];\n        if (tensorValue == null) {\n          throw new ValueError(\n              `No value is provided for the model's input ${input.name}`);\n        }\n        feedDict.add(input, tensorValue);\n      }\n    }\n\n    // Run execution.\n    const executeOutputs = execute(outputSymbolicTensors, feedDict) as Tensor[];\n    return outputsIsArray ? executeOutputs : executeOutputs[0];\n  }\n\n  /**\n   * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n   */\n  private retrieveSymbolicTensors(symbolicTensorNames: string[]):\n      SymbolicTensor[] {\n    const outputSymbolicTensors: SymbolicTensor[] =\n        pyListRepeat(null, symbolicTensorNames.length);\n    let outputsRemaining = symbolicTensorNames.length;\n    for (const layer of this.layers) {\n      const layerOutputs: SymbolicTensor[] =\n          Array.isArray(layer.output) ? layer.output : [layer.output];\n      const layerOutputNames = layerOutputs.map(output => output.name);\n      for (let i = 0; i < symbolicTensorNames.length; ++i) {\n        const index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n        if (index !== -1) {\n          outputSymbolicTensors[i] = layerOutputs[index];\n          outputsRemaining--;\n        }\n        if (outputsRemaining === 0) {\n          break;\n        }\n      }\n      if (outputsRemaining === 0) {\n        break;\n      }\n    }\n\n    if (outputsRemaining > 0) {\n      const remainingNames: string[] = [];\n      outputSymbolicTensors.forEach((tensor, i) => {\n        if (tensor == null) {\n          remainingNames.push(symbolicTensorNames[i]);\n        }\n      });\n      throw new ValueError(\n          `Cannot find SymbolicTensors for output name(s): ` +\n          `${JSON.stringify(remainingNames)}`);\n    }\n    return outputSymbolicTensors;\n  }\n\n  /**\n   * Helper method to loop over some data in batches.\n   *\n   * Porting Note: Not using the functional approach in the Python equivalent\n   *   due to the imperative backend.\n   * Porting Note: Does not support step mode currently.\n   *\n   * @param ins: input data\n   * @param batchSize: integer batch size.\n   * @param verbose: verbosity model\n   * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n   *   `tf.Tensor` (if multipe outputs).\n   */\n  private predictLoop(ins: Tensor|Tensor[], batchSize = 32, verbose = false):\n      Tensor|Tensor[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins);\n      if (verbose) {\n        throw new NotImplementedError(\n            'Verbose predictLoop() is not implemented yet.');\n      }\n\n      // Sample-based predictions.\n      // Porting Note: Tensor currently does not support sliced assignments as\n      //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n      //   iterating over the batches.\n\n      const batches = makeBatches(numSamples, batchSize);\n      const outsBatches: Tensor[][] = this.outputs.map(output => []);\n\n      // TODO(cais): Can the scope() be pushed down inside the for loop?\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchOuts = tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          // TODO(cais): Take care of the case of the last element is a flag for\n          //   training/test.\n          const insBatch = sliceArrays(ins, batchStart, batchEnd);\n\n          // Construct the feeds for execute();\n          const feeds = [];\n          if (Array.isArray(insBatch)) {\n            for (let i = 0; i < insBatch.length; ++i) {\n              feeds.push({key: this.inputs[i], value: insBatch[i]});\n            }\n          } else {\n            feeds.push({key: this.inputs[0], value: insBatch});\n          }\n          const feedDict = new FeedDict(feeds);\n          return execute(this.outputs, feedDict) as Tensor[];\n        });\n        batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));\n      }\n      return singletonOrArray(\n          outsBatches.map(batches => tfc.concat(batches, 0)));\n    });\n  }\n\n  /**\n   * Generates output predictions for the input samples.\n   *\n   * Computation is done in batches.\n   *\n   * Note: the \"step\" mode of predict() is currently not supported.\n   *   This is because the TensorFlow.js core backend is imperative only.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n   * ```\n   *\n   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n   *   the model has multiple inputs.\n   * @param args A `ModelPredictArgs` object containing optional fields.\n   *\n   * @return Prediction results as a `tf.Tensor`(s).\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and the model's expectations, or in case a stateful model receives a\n   *   number of samples that is not a multiple of the batch size.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(x: Tensor|Tensor[], args: ModelPredictArgs = {}): Tensor|Tensor[] {\n    const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n    checkInputData(\n        xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n    try {\n      // TODO(cais): Take care of stateful models.\n      //   if (this.stateful) ...\n      // TODO(cais): Take care of the learning_phase boolean flag.\n      //   if (this.useLearningPhase) ...\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      return this.predictLoop(xsRank2OrHigher, batchSize);\n    } finally {\n      disposeNewTensors(xsRank2OrHigher, x);\n    }\n  }\n\n  /**\n   * Returns predictions for a single batch of samples.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predictOnBatch(tf.ones([8, 10])).print();\n   * ```\n   * @param x: Input samples, as a Tensor (for models with exactly one\n   *   input) or an array of Tensors (for models with more than one input).\n   * @return Tensor(s) of predictions\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predictOnBatch(x: Tensor|Tensor[]): Tensor|Tensor[] {\n    checkInputData(x, this.inputNames, this.feedInputShapes, true);\n    // TODO(cais): Take care of the learning_phase boolean flag.\n    //   if (this.useLearningPhase) ...\n    const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n    return this.predictLoop(x, batchSize);\n  }\n\n  protected standardizeUserDataXY(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor}, checkBatchAxis = true,\n      batchSize?: number): [Tensor[], Tensor[]] {\n    // TODO(cais): Add sampleWeight, classWeight\n    if (this.optimizer_ == null) {\n      throw new RuntimeError(\n          'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileArgs).');\n    }\n    const outputShapes: Shape[] = [];\n    for (let i = 0; i < this.feedOutputShapes.length; ++i) {\n      const outputShape = this.feedOutputShapes[i];\n      const lossFn = this.feedLossFns[i];\n      if (lossFn === losses.sparseCategoricalCrossentropy) {\n        outputShapes.push(\n            outputShape.slice(0, outputShape.length - 1).concat([1]));\n      } else {\n        // Porting Note: Because of strong typing `lossFn` must be a function.\n        outputShapes.push(outputShape);\n      }\n    }\n    x = standardizeInputData(\n        x, this.feedInputNames, this.feedInputShapes, false, 'input');\n    y = standardizeInputData(\n        y, this.feedOutputNames, outputShapes, false, 'target');\n    // TODO(cais): Standardize sampleWeights & classWeights.\n    checkArrayLengths(x, y, null);\n    // TODO(cais): Check sampleWeights as well.\n    checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n    if (this.stateful && batchSize != null && batchSize > 0) {\n      if (x[0].shape[0] % batchSize !== 0) {\n        throw new ValueError(\n            `In a stateful network, you should only pass inputs with a ` +\n            `number of samples that is divisible by the batch size ` +\n            `${batchSize}. Found: ${x[0].shape[0]} sample(s).`);\n      }\n    }\n    return [x, y];\n  }\n\n  protected async standardizeUserData(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      sampleWeight?: Tensor|Tensor[]|{[outputName: string]: Tensor},\n      classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap,\n      checkBatchAxis = true,\n      batchSize?: number): Promise<[Tensor[], Tensor[], Tensor[]]> {\n    const [standardXs, standardYs] =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    // TODO(cais): Handle sampleWeights.\n    if (sampleWeight != null) {\n      throw new Error('sample weight is not supported yet.');\n    }\n\n    let standardSampleWeights: Tensor[] = null;\n    if (classWeight != null) {\n      const classWeights =\n          standardizeClassWeights(classWeight, this.outputNames);\n      standardSampleWeights = [];\n      for (let i = 0; i < classWeights.length; ++i) {\n        standardSampleWeights.push(\n            await standardizeWeights(standardYs[i], null, classWeights[i]));\n      }\n    }\n\n    // TODO(cais): Deal with the case of model.stateful == true.\n    return [standardXs, standardYs, standardSampleWeights];\n  }\n\n  /**\n   * Loop over some test data in batches.\n   * @param f A Function returning a list of tensors.\n   * @param ins Array of tensors to be fed to `f`.\n   * @param batchSize Integer batch size or `null` / `undefined`.\n   * @param verbose verbosity mode.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring test finished. Ignored with the default value of `null` /\n   * `undefined`.\n   * @returns Array of Scalars.\n   */\n  private testLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], batchSize?: number,\n      verbose = 0, steps?: number): Scalar[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins, batchSize, steps, 'steps');\n      const outs: Scalar[] = [];\n      if (verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n      }\n      // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n      if (steps != null) {\n        throw new NotImplementedError(\n            'steps mode in testLoop() is not implemented yet');\n      } else {\n        const batches = makeBatches(numSamples, batchSize);\n        const indexArray = tensor1d(range(0, numSamples));\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds =\n              K.sliceAlongFirstAxis(\n                  indexArray, batchStart, batchEnd - batchStart) as Tensor1D;\n          // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n          const insBatch = sliceArraysByIndices(ins, batchIds) as Scalar[];\n          const batchOuts = f(insBatch);\n          if (batchIndex === 0) {\n            for (let i = 0; i < batchOuts.length; ++i) {\n              outs.push(scalar(0));\n            }\n          }\n          for (let i = 0; i < batchOuts.length; ++i) {\n            const batchOut = batchOuts[i];\n            outs[i] =\n                tfc.add(outs[i], tfc.mul(batchEnd - batchStart, batchOut));\n          }\n        }\n        for (let i = 0; i < outs.length; ++i) {\n          outs[i] = tfc.div(outs[i], numSamples);\n        }\n      }\n      return outs;\n    });\n  }\n\n  protected getDedupedMetricsNames(): string[] {\n    const outLabels = this.metricsNames;\n    // Rename duplicated metrics names (can happen with an output layer\n    // shared among multiple dataflows).\n    const dedupedOutLabels = [];\n    for (let i = 0; i < outLabels.length; ++i) {\n      const label = outLabels[i];\n      let newLabel = label;\n      if (count(outLabels, label) > 1) {\n        const dupIndex = count(outLabels.slice(0, i), label);\n        newLabel += `_${dupIndex}`;\n      }\n      dedupedOutLabels.push(newLabel);\n    }\n    return dedupedOutLabels;\n  }\n\n  /**\n   * Creates a function that performs the following actions:\n   *\n   * 1. computes the losses\n   * 2. sums them to get the total loss\n   * 3. call the optimizer computes the gradients of the LayersModel's\n   *    trainable weights w.r.t. the total loss and update the variables\n   * 4. calculates the metrics\n   * 5. returns the values of the losses and metrics.\n   */\n  protected makeTrainFunction(): (data: Tensor[]) => Scalar[] {\n    return (data: Tensor[]) => {\n      const lossValues: Scalar[] = [];\n\n      const inputs = data.slice(0, this.inputs.length);\n      const targets = data.slice(\n          this.inputs.length, this.inputs.length + this.outputs.length);\n      const sampleWeights = data.slice(\n          this.inputs.length + this.outputs.length,\n          this.inputs.length + this.outputs.length * 2);\n\n      const metricsValues: Scalar[] = [];\n\n      // Create a function that computes the total loss based on the\n      // inputs. This function is used for obtaining gradients through\n      // backprop.\n      const totalLossFunction = () => {\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs =\n            execute(this.outputs, feedDict, {'training': true}) as Tensor[];\n        // TODO(cais): Take care of the case of multiple outputs from a\n        //   single layer?\n\n        let totalLoss: Tensor;\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          let loss = lossFunction(targets[i], outputs[i]);\n          if (sampleWeights[i] != null) {\n            loss = computeWeightedLoss(loss, sampleWeights[i]);\n          }\n\n          // TODO(cais): push Scalar instead.\n          const meanLoss: Scalar = tfc.mean(loss);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          lossValues.push(meanLoss);\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n        }\n\n        // Compute the metrics.\n        // TODO(cais): These should probably be calculated outside\n        //   totalLossFunction to benefit speed?\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          let weightedMetric: Scalar;\n\n          if (this.outputs.length > 1 && i < this.outputs.length) {\n            weightedMetric = lossValues[i];\n          } else {\n            const metric = this.metricsTensors[i][0];\n            const outputIndex = this.metricsTensors[i][1];\n            weightedMetric =\n                tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          }\n\n          tfc.keep(weightedMetric);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          metricsValues.push(weightedMetric);\n        }\n\n        totalLoss = tfc.mean(totalLoss);\n\n        // Add regularizer penalties.\n        this.calculateLosses().forEach(regularizerLoss => {\n          totalLoss = tfc.add(totalLoss, regularizerLoss);\n        });\n\n        return totalLoss as Scalar;\n      };\n\n      const variables = this.collectedTrainableWeights.map(\n          param => param.read() as tfc.Variable);\n      const returnCost = true;\n      const totalLossValue =\n          this.optimizer_.minimize(totalLossFunction, returnCost, variables);\n\n      return [totalLossValue].concat(metricsValues);\n    };\n  }\n\n  /**\n   * Create a function which, when invoked with an array of `tf.Tensor`s as a\n   * batch of inputs, returns the prespecified loss and metrics of the model\n   * under the batch of input data.\n   */\n  private makeTestFunction() {\n    this.testFunction = (data: Tensor[]) => {\n      return tfc.tidy(() => {\n        const valOutputs: Scalar[] = [];\n        let totalLoss: Scalar;\n        const inputs = data.slice(0, this.inputs.length);\n        const targets = data.slice(\n            this.inputs.length, this.inputs.length + this.outputs.length);\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs = execute(this.outputs, feedDict) as Tensor[];\n        // Compute total loss.\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          // TODO(cais): Add sample weighting and replace the simple\n          // averaging.\n          const loss: Scalar = tfc.mean(lossFunction(targets[i], outputs[i]));\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n          valOutputs.push(totalLoss);\n        }\n        // Compute the metrics.\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          const metric = this.metricsTensors[i][0];\n          const outputIndex = this.metricsTensors[i][1];\n          // TODO(cais): Replace K.mean() with a proper weighting function.\n          const meanMetric =\n              tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          valOutputs.push(meanMetric as Scalar);\n        }\n        return valOutputs;\n      });\n    };\n  }\n\n  /**\n   * Trains the model for a fixed number of epochs (iterations on a\n   * dataset).\n   *\n   * ```js\n   * const model = tf.sequential({\n   *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * for (let i = 1; i < 5 ; ++i) {\n   *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *       batchSize: 4,\n   *       epochs: 3\n   *   });\n   *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n   * }\n   * ```\n   *\n   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n   * model has multiple inputs. If all inputs in the model are named, you\n   * can also pass a dictionary mapping input names to `tf.Tensor`s.\n   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n   * the model has multiple outputs. If all outputs in the model are named,\n   * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n   * @param args A `ModelFitArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @exception ValueError In case of mismatch between the provided input\n   * data and what the model expects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fit(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      args: ModelFitArgs = {}): Promise<History> {\n    if (this.isTraining) {\n      throw new Error(\n          'Cannot start training because another fit() call is ongoing.');\n    }\n    this.isTraining = true;\n    let inputs: Tensor[];\n    let targets: Tensor[];\n    let originalInputs: Tensor[];\n    let originalTargets: Tensor[];\n    let inputValX: Tensor|Tensor[];\n    let inputValY: Tensor|Tensor[];\n    let valX: Tensor|Tensor[];\n    let valY: Tensor|Tensor[];\n    let sampleWeights: Tensor[];\n    try {\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n\n      // Validate user data.\n      // TODO(cais): Support sampleWeight.\n      const checkBatchAxis = false;\n      const standardizedOuts =\n          await this.standardizeUserData(\n              x, y, args.sampleWeight, args.classWeight, checkBatchAxis,\n              batchSize) as [Tensor[], Tensor[], Tensor[]];\n      inputs = standardizedOuts[0];\n      targets = standardizedOuts[1];\n      sampleWeights = standardizedOuts[2];\n\n      // Prepare validation data.\n      let doValidation = false;\n      let valIns: Tensor[];\n      if (args.validationData != null && args.validationData.length > 0) {\n        doValidation = true;\n        if (args.validationData.length === 2) {\n          // config.validationData consists of valX and valY.\n          inputValX = args.validationData[0];\n          inputValY = args.validationData[1];\n        } else if (args.validationData.length === 3) {\n          throw new NotImplementedError(\n              'validationData including sample weights is not supported yet.');\n        } else {\n          throw new ValueError(\n              `When passing validation data, it must contain 2 (valX, valY) ` +\n              `or 3 (valX, valY, valSampleWeight) items; ` +\n              `${args.validationData} is invalid.`);\n        }\n\n        const checkBatchAxis = true;\n        const valStandardized =\n            await this.standardizeUserData(\n                inputValX, inputValY, null, /** Unused sample weights. */\n                null,                       /** Unused class weights. */\n                checkBatchAxis, batchSize) as [Tensor[], Tensor[], Tensor[]];\n        valX = valStandardized[0];\n        valY = valStandardized[1];\n        valIns = valX.concat(valY);\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (\n          args.validationSplit != null && args.validationSplit > 0 &&\n          args.validationSplit < 1) {\n        doValidation = true;\n        // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n        const splitAt =\n            Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n        const originalBatchSize = inputs[0].shape[0];\n        valX = sliceArrays(inputs, splitAt, originalBatchSize) as Tensor[];\n        originalInputs = inputs;\n        inputs = sliceArrays(inputs, 0, splitAt) as Tensor[];\n        valY = sliceArrays(targets, splitAt, originalBatchSize) as Tensor[];\n        originalTargets = targets;\n        targets = sliceArrays(targets, 0, splitAt) as Tensor[];\n        // TODO(cais): Once sampleWeights becomes available, slice it to get\n        //   valSampleWeights.\n        valIns = valX.concat(valY);\n\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (args.validationSteps != null) {\n        doValidation = true;\n        // TODO(cais): Add useLearningPhase.\n      }\n\n      const ins = inputs.concat(targets).concat(sampleWeights);\n\n      this.checkTrainableWeightsConsistency();\n\n      // TODO(cais): Handle use_learning_phase and learning_phase?\n\n      // Porting Note: Here we see a key deviation of tfjs-layers from\n      // Keras.\n      //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n      //  we do not construct symbolic computation graphs to embody the\n      //  training process. Instead, we define a function that performs the\n      //  training action. In PyKeras, the data (inputs and targets) are fed\n      //  through graph placeholders. In tfjs-layers, the data are fed as\n      //  function arguments. Since the function are defined below in the\n      //  scope, we don't have equivalents of PyKeras's\n      //  `_make_train_funciton`.\n      const trainFunction = this.makeTrainFunction();\n      const outLabels = this.getDedupedMetricsNames();\n\n      let valFunction: (data: Tensor[]) => Scalar[];\n      let callbackMetrics: string[];\n      if (doValidation) {\n        this.makeTestFunction();\n        valFunction = this.testFunction;\n        callbackMetrics =\n            outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n      } else {\n        valFunction = null;\n        valIns = [];\n        callbackMetrics = outLabels.slice();\n      }\n\n      const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n      const out = await this.fitLoop(\n          trainFunction, ins, outLabels, batchSize, args.epochs,\n          args.verbose, callbacks, valFunction, valIns, args.shuffle,\n          callbackMetrics, args.initialEpoch, null, null);\n      return out;\n    } finally {\n      this.isTraining = false;\n      // Memory clean up.\n      disposeNewTensors(inputs, x);\n      disposeNewTensors(targets, y);\n      disposeNewTensors(originalInputs, x);\n      disposeNewTensors(originalTargets, y);\n      disposeNewTensors(valX as Tensor[], inputValX);\n      disposeNewTensors(valY as Tensor[], inputValY);\n      if (sampleWeights != null) {\n        tfc.dispose(sampleWeights);\n      }\n    }\n    // TODO(cais): Add value to outLabels.\n  }\n\n  /**\n   * Abstract fit function for `f(ins)`.\n   * @param f A Function returning a list of tensors. For training, this\n   *   function is expected to perform the updates to the variables.\n   * @param ins List of tensors to be fed to `f`.\n   * @param outLabels List of strings, display names of the outputs of `f`.\n   * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n   * @param epochs Number of times to iterate over the data. Default : 1.\n   * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n   * @param callbacks List of callbacks to be called during training.\n   * @param valF Function to call for validation.\n   * @param valIns List of tensors to be fed to `valF`.\n   * @param shuffle Whether to shuffle the data at the beginning of every\n   * epoch. Default : true.\n   * @param callbackMetrics List of strings, the display names of the metrics\n   *   passed to the callbacks. They should be the concatenation of the\n   *   display names of the outputs of `f` and the list of display names\n   *   of the outputs of `valF`.\n   * @param initialEpoch Epoch at which to start training (useful for\n   *   resuming a previous training run). Default : 0.\n   * @param stepsPerEpoch Total number of steps (batches on samples) before\n   *   declaring one epoch finished and starting the next epoch. Ignored with\n   *   the default value of `undefined` or `null`.\n   * @param validationSteps Number of steps to run validation for (only if\n   *   doing validation from data tensors). Not applicable for tfjs-layers.\n   * @returns A `History` object.\n   */\n  async fitLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], outLabels?:\n      string[], batchSize?: number, epochs?: number, verbose?: number,\n      callbacks?: BaseCallback[], valF?: (data: Tensor[]) => Scalar[], valIns?:\n      Tensor[], shuffle?: boolean|string, callbackMetrics?: string[],\n      initialEpoch?: number, stepsPerEpoch?: number, validationSteps?: number):\n      Promise<History> {\n    if (batchSize == null) {\n      batchSize = 32;\n    }\n    if (epochs == null) {\n      epochs = 1;\n    }\n    if (shuffle == null) {\n      shuffle = true;\n    }\n    if (initialEpoch == null) {\n      initialEpoch = 0;\n    }\n\n    // TODO(cais): Change const to let below when implementing validation.\n    let doValidation = false;\n    if (valF != null && valIns != null) {\n      doValidation = true;\n      // TODO(cais): verbose message.\n    }\n    if (validationSteps != null) {\n      doValidation = true;\n      if (stepsPerEpoch == null) {\n        throw new ValueError(\n            'Can only use `validationSteps` when doing step-wise training, ' +\n            'i.e., `stepsPerEpoch` must be set.');\n      }\n    }\n\n    const numTrainSamples =\n        this.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n    let indexArray: number[];\n    if (numTrainSamples != null) {\n      indexArray = range(0, numTrainSamples);\n    }\n\n    if (verbose == null) {\n      verbose = 1;\n    }\n\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, epochs, initialEpoch, numTrainSamples,\n        stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n    callbackList.setModel(this);\n    this.history = history;\n    await callbackList.onTrainBegin();\n    this.stopTraining_ = false;\n    // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n    // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n\n    for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n      await callbackList.onEpochBegin(epoch);\n      const epochLogs: UnresolvedLogs = {};\n      if (stepsPerEpoch != null) {\n        throw new NotImplementedError(\n            'stepsPerEpoch mode is not implemented yet.');\n      } else {\n        if (shuffle === 'batch') {\n          throw new NotImplementedError('batch shuffling is not implemneted'\n                                        + ' yet');\n        } else if (shuffle) {\n          util.shuffle(indexArray);\n        }\n        // Convert the potentially shuffled indices to Tensor1D, to avoid the\n        // cost of repeated creation of Array1Ds later on.\n        const epochIndexArray1D = tensor1d(indexArray);\n\n        const batches = makeBatches(numTrainSamples, batchSize);\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchLogs: UnresolvedLogs = {};\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          tfc.tidy(() => {\n            const batchStart = batches[batchIndex][0];\n            const batchEnd = batches[batchIndex][1];\n            const batchIds = K.sliceAlongFirstAxis(\n                                 epochIndexArray1D, batchStart,\n                                 batchEnd - batchStart) as Tensor1D;\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = batchEnd - batchStart;\n\n            // TODO(cais): In ins, train flag can be a number, instead of an\n            //   Tensor? Do we need to handle this in tfjs-layers?\n            const insBatch = sliceArraysByIndices(ins, batchIds) as Tensor[];\n            const outs = f(insBatch);\n            for (let i = 0; i < outLabels.length; ++i) {\n              const label = outLabels[i];\n              const out = outs[i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n              // TODO(cais): Use scope() to avoid ownership.\n            }\n\n            if (batchIndex === batches.length - 1) {  // Last batch.\n              if (doValidation) {\n                const valOuts = this.testLoop(valF, valIns, batchSize);\n                // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                for (let i = 0; i < outLabels.length; ++i) {\n                  const label = outLabels[i];\n                  const out = valOuts[i];\n                  tfc.keep(out);\n                  // TODO(cais): Use scope() to avoid ownership.\n                  epochLogs['val_' + label] = out;\n                }\n              }\n            }\n          });\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          if (this.stopTraining_) {\n            break;\n          }\n          // TODO(cais): return outs as list of Tensor.\n        }\n\n        epochIndexArray1D.dispose();\n      }\n      // TODO(cais): Run validation at the end of the epoch.\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      if (this.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n\n    await this.history.syncData();\n    return this.history;\n  }\n\n  // TODO(cais): Add code snippet below when it's possible to instantiate\n  //   actual dataset objects.\n  /**\n   * Trains the model using a dataset object.\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for training. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs.\n   *   Of the two items in the array, the first is the input feature(s) and\n   *   the second is the output target(s).\n   * @param args A `ModelFitDatasetArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fitDataset<T>(dataset: Dataset<T>, args: ModelFitDatasetArgs<T>):\n      Promise<History> {\n    return fitDataset(this, dataset, args);\n  }\n\n  /**\n   * Runs a single gradient update on a single batch of data.\n   *\n   * This method differs from `fit()` and `fitDataset()` in the following\n   * regards:\n   *   - It operates on exactly one batch of data.\n   *   - It returns only the loss and metric values, instead of\n   *     returning the batch-by-batch loss and metric values.\n   *   - It doesn't support fine-grained options such as verbosity and\n   *     callbacks.\n   *\n   * @param x Input data. It could be one of the following:\n   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n   *     multiple inputs).\n   *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n   *     model has named inputs).\n   * @param y Target data. It could be either a `tf.Tensor` or multiple\n   *   `tf.Tensor`s. It should be consistent with `x`.\n   * @returns Training loss or losses (in case the model has\n   *   multiple outputs), along with metrics (if any), as numbers.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async trainOnBatch(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|\n      {[inputName: string]: Tensor}): Promise<number|number[]> {\n    // TODO(cais): Support sampleWeight and classWeight.\n    // TODO(cais): Support Dataset objects.\n    const standardizeOut = await this.standardizeUserData(x, y);\n    const inputs = standardizeOut[0];\n    const targets = standardizeOut[1];\n    const trainFunction = this.makeTrainFunction();\n    const losses = trainFunction(inputs.concat(targets));\n    const lossValues: number[] = [];\n    for (const loss of losses) {\n      const v = await loss.data();\n      lossValues.push(v[0]);\n    }\n    tfc.dispose(losses);\n    disposeNewTensors(standardizeOut[0], x);\n    disposeNewTensors(standardizeOut[1], y);\n    return singletonOrArray(lossValues);\n  }\n\n  /**\n   * Extract weight values of the model.\n   *\n   * @param config: An instance of `io.SaveConfig`, which specifies\n   * model-saving options such as whether only trainable weights are to be\n   * saved.\n   * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n   *   non-uniqueified weight names) to their values.\n   */\n  protected getNamedWeights(config?: io.SaveConfig): NamedTensor[] {\n    const namedWeights: NamedTensor[] = [];\n\n    const trainableOnly = config != null && config.trainableOnly;\n    const weights = trainableOnly ? this.trainableWeights : this.weights;\n    const weightValues = this.getWeights(trainableOnly);\n    for (let i = 0; i < weights.length; ++i) {\n      if (trainableOnly && !weights[i].trainable) {\n        // Optionally skip non-trainable weights.\n        continue;\n      }\n      namedWeights.push(\n          {name: weights[i].originalName, tensor: weightValues[i]});\n    }\n    return namedWeights;\n  }\n\n  /**\n   * Setter used for force stopping of LayersModel.fit() (i.e., training).\n   *\n   * Example:\n   *\n   * ```js\n   * const input = tf.input({shape: [10]});\n   * const output = tf.layers.dense({units: 1}).apply(input);\n   * const model = tf.model({inputs: [input], outputs: [output]});\n   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n   * const xs = tf.ones([8, 10]);\n   * const ys = tf.zeros([8, 1]);\n   *\n   * const history = await model.fit(xs, ys, {\n   *   epochs: 10,\n   *   callbacks: {\n   *     onEpochEnd: async (epoch, logs) => {\n   *       if (epoch === 2) {\n   *         model.stopTraining = true;\n   *       }\n   *     }\n   *   }\n   * });\n   *\n   * // There should be only 3 values in the loss array, instead of 10\n   * values,\n   * // due to the stopping after 3 epochs.\n   * console.log(history.history.loss);\n   * ```\n   */\n  set stopTraining(stop: boolean) {\n    this.stopTraining_ = stop;\n  }\n\n  get stopTraining(): boolean {\n    return this.stopTraining_;\n  }\n\n  get optimizer(): Optimizer {\n    return this.optimizer_;\n  }\n\n  set optimizer(optimizer: Optimizer) {\n    if (this.optimizer_ !== optimizer) {\n      this.optimizer_ = optimizer;\n      this.isOptimizerOwned = false;\n    }\n  }\n\n  override dispose(): DisposeResult {\n    const result = super.dispose();\n    if (result.refCountAfterDispose === 0 && this.optimizer != null &&\n        this.isOptimizerOwned) {\n      const numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n      this.optimizer_.dispose();\n      result.numDisposedVariables +=\n          numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n    }\n    return result;\n  }\n\n  private getLossIdentifiers(): LossIdentifier|LossIdentifier[]|\n      {[outputName: string]: LossIdentifier} {\n    let lossNames: LossIdentifier|LossIdentifier[]|\n        {[outputName: string]: LossIdentifier};\n    if (typeof this.loss === 'string') {\n      lossNames = toSnakeCase(this.loss) as LossIdentifier;\n    } else if (Array.isArray(this.loss)) {\n      for (const loss of this.loss) {\n        if (typeof loss !== 'string') {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n      lossNames = (this.loss as string[]).map(name => toSnakeCase(name)) as\n          LossIdentifier[];\n    } else {\n      const outputNames = Object.keys(this.loss);\n      lossNames = {} as {[outputName: string]: LossIdentifier};\n      const losses =\n          this.loss as {[outputName: string]: LossOrMetricFn | string};\n      for (const outputName of outputNames) {\n        if (typeof losses[outputName] === 'string') {\n          lossNames[outputName] =\n              toSnakeCase(losses[outputName] as string) as LossIdentifier;\n        } else {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n    }\n    return lossNames;\n  }\n\n  private getMetricIdentifiers(): MetricsIdentifier[]|\n      {[key: string]: MetricsIdentifier} {\n    if (typeof this.metrics === 'string' ||\n        typeof this.metrics === 'function') {\n      return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n    } else if (Array.isArray(this.metrics)) {\n      return this.metrics.map(\n          metric => toSnakeCase(Metrics.getLossOrMetricName(metric)));\n    } else {\n      const metricsIdentifiers: {[key: string]: MetricsIdentifier} = {};\n      for (const key in this.metrics) {\n        metricsIdentifiers[key] =\n            toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n      }\n      return metricsIdentifiers;\n    }\n  }\n\n  protected getTrainingConfig(): TrainingConfig {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      } as OptimizerSerialization\n    };\n    // TODO(cais): Add weight_metrics when they are supported.\n    // TODO(cais): Add sample_weight_mode when it's supported.\n    // TODO(cais): Add loss_weights when it's supported.\n  }\n\n  loadTrainingConfig(trainingConfig: TrainingConfig) {\n    if (trainingConfig.weighted_metrics != null) {\n      throw new Error('Loading weight_metrics is not supported yet.');\n    }\n    if (trainingConfig.loss_weights != null) {\n      throw new Error('Loading loss_weights is not supported yet.');\n    }\n    if (trainingConfig.sample_weight_mode != null) {\n      throw new Error('Loading sample_weight_mode is not supported yet.');\n    }\n\n    const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config) as\n        serialization.ConfigDict;\n    const optimizer = deserialize(tsConfig) as Optimizer;\n\n    let loss;\n    if (typeof trainingConfig.loss === 'string') {\n      loss = toCamelCase(trainingConfig.loss);\n    } else if (Array.isArray(trainingConfig.loss)) {\n      loss = trainingConfig.loss.map(lossEntry => toCamelCase(lossEntry));\n    } else if (trainingConfig.loss != null) {\n      loss = {} as {[outputName: string]: LossIdentifier};\n      for (const key in trainingConfig.loss) {\n        loss[key] = toCamelCase(trainingConfig.loss[key]) as LossIdentifier;\n      }\n    }\n\n    let metrics;\n    if (Array.isArray(trainingConfig.metrics)) {\n      metrics = trainingConfig.metrics.map(metric => toCamelCase(metric));\n    } else if (trainingConfig.metrics != null) {\n      metrics = {} as {[outputName: string]: MetricsIdentifier};\n      for (const key in trainingConfig.metrics) {\n        metrics[key] = toCamelCase(trainingConfig.metrics[key]);\n      }\n    }\n\n    this.compile({loss, metrics, optimizer});\n  }\n\n  /**\n   * Save the configuration and/or weights of the LayersModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 2. Saving `model`'s topology and weights to browser\n   * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('indexeddb://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 3. Saving `model`'s topology and weights as two files\n   * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n   * browser.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('downloads://my-model-1');\n   * ```\n   *\n   * Example 4. Send  `model`'s topology and weights to an HTTP server.\n   * See the documentation of `tf.io.http` for more details\n   * including specifying request parameters and implementation of the\n   * server.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('http://my-server/model/upload');\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL: io.IOHandler|string, config?: io.SaveConfig):\n      Promise<io.SaveResult> {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new ValueError(\n            `Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new ValueError(\n            `Found more than one (${handlers.length}) save handlers for ` +\n            `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new ValueError(\n          'LayersModel.save() cannot proceed because the IOHandler ' +\n          'provided does not have the `save` attribute defined.');\n    }\n\n    const weightDataAndSpecs =\n        await io.encodeWeights(this.getNamedWeights(config));\n\n    const returnString = false;\n    const unusedArg: {} = null;\n    const modelConfig = this.toJSON(unusedArg, returnString);\n    const modelArtifacts: io.ModelArtifacts = {\n      modelTopology: modelConfig,\n      format: LAYERS_MODEL_FORMAT_NAME,\n      generatedBy: `TensorFlow.js tfjs-layers v${version}`,\n      convertedBy: null,\n    };\n\n    const includeOptimizer = config == null ? false : config.includeOptimizer;\n    if (includeOptimizer && this.optimizer != null) {\n      modelArtifacts.trainingConfig = this.getTrainingConfig();\n      const weightType = 'optimizer';\n      const {data: optimizerWeightData, specs: optimizerWeightSpecs} =\n          await io.encodeWeights(await this.optimizer.getWeights(), weightType);\n      weightDataAndSpecs.specs.push(...optimizerWeightSpecs);\n      weightDataAndSpecs.data = io.concatenateArrayBuffers(\n          [weightDataAndSpecs.data, optimizerWeightData]);\n    }\n\n    if (this.userDefinedMetadata != null) {\n      // Check serialized size of user-defined metadata.\n      const checkSize = true;\n      checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n      modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n    }\n\n    modelArtifacts.weightData = weightDataAndSpecs.data;\n    modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n    return handlerOrURL.save(modelArtifacts);\n  }\n\n  /**\n   * Set user-defined metadata.\n   *\n   * The set metadata will be serialized together with the topology\n   * and weights of the model during `save()` calls.\n   *\n   * @param setUserDefinedMetadata\n   */\n  setUserDefinedMetadata(userDefinedMetadata: {}): void {\n    checkUserDefinedMetadata(userDefinedMetadata, this.name);\n    this.userDefinedMetadata = userDefinedMetadata;\n  }\n\n  /**\n   * Get user-defined metadata.\n   *\n   * The metadata is supplied via one of the two routes:\n   *   1. By calling `setUserDefinedMetadata()`.\n   *   2. Loaded during model loading (if the model is constructed\n   *      via `tf.loadLayersModel()`.)\n   *\n   * If no user-defined metadata is available from either of the\n   * two routes, this function will return `undefined`.\n   */\n  getUserDefinedMetadata(): {} {\n    return this.userDefinedMetadata;\n  }\n}\nserialization.registerClass(LayersModel);\n\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nexport class Functional extends LayersModel {\n  static override className = 'Functional';\n}\nserialization.registerClass(Functional);\n"],"mappings":"AAAA;;;;;;;;;AAUA;AAEA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,EAAE,EAA0DC,SAAS,EAAUC,MAAM,EAAEC,aAAa,EAAEC,MAAM,EAAYC,QAAQ,EAAEC,IAAI,QAAO,uBAAuB;AAE5K,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAsBC,kBAAkB,EAAkCC,oBAAoB,QAAO,mBAAmB;AACxH,SAAQC,SAAS,QAAO,WAAW;AACnC,SAAQC,mBAAmB,EAAEC,YAAY,EAAEC,UAAU,QAAO,WAAW;AAKvE,SAAQC,WAAW,QAAO,yBAAyB;AACnD,SAASC,oBAAoB,QAAwB,SAAS;AAC9D,OAAO,KAAKC,MAAM,MAAM,WAAW;AACnC,OAAO,KAAKC,OAAO,MAAM,YAAY;AACrC,OAAO,KAAKC,UAAU,MAAM,eAAe;AAE3C,SAAQC,wBAAwB,QAAO,0BAA0B;AACjE,SAAQC,KAAK,EAAEC,YAAY,EAAEC,gBAAgB,EAAEC,WAAW,EAAEC,WAAW,EAAEC,MAAM,QAAO,wBAAwB;AAC9G,SAAQC,YAAY,QAAO,sBAAsB;AACjD,SAAQC,KAAK,QAAO,qBAAqB;AACzC,SAAQC,mBAAmB,QAAO,8BAA8B;AAEhE,SAAQC,OAAO,QAAO,YAAY;AAElC,SAAQC,SAAS,QAAsB,aAAa;AAEpD,SAAQC,OAAO,EAAEC,QAAQ,QAAO,YAAY;AAE5C,SAAQC,eAAe,EAAEC,UAAU,QAAsD,oBAAoB;AAC7G,SAAQC,cAAc,EAAEC,iBAAiB,EAAEC,0BAA0B,EAAEC,WAAW,EAAgBC,WAAW,EAAEC,oBAAoB,QAAO,oBAAoB;AAC9J,SAAqCC,mBAAmB,EAAEC,uBAAuB,EAAEC,kBAAkB,QAAO,kBAAkB;AAE9H;;;AAGA,OAAM,SAAUC,YAAYA,CAACC,CAC+B;EAC1D,OAAOA,CAAC,YAAYzC,MAAM;AAC5B;AAEA;;;AAGA,OAAM,SAAU0C,WAAWA,CAACD,CAC6B;EACvD,OAAOE,KAAK,CAACC,OAAO,CAACH,CAAC,CAAC;AACzB;AAEA;;;AAGA,OAAM,SAAUI,UAAUA,CAACJ,CAC6B;EACtD,OAAO,CAACD,YAAY,CAACC,CAAC,CAAC,IAAI,CAACC,WAAW,CAACD,CAAC,CAAC;AAC5C;AAEA;;;;;;;;;;;AAWA,OAAM,SAAUK,oBAAoBA,CAChCC,IAAmD,EAAEC,KAAe,EACpEC,MAAgB,EAAEC,cAAc,GAAG,IAAI,EAAEC,eAAe,GAAG,EAAE;EAC/D,IAAIH,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACI,MAAM,KAAK,CAAC,EAAE;IACvC;IACA;IACA,IAAIL,IAAI,IAAI,IAAI,EAAE;MAChB,IAAIM,iBAAiB,GAAG,KAAK;MAC7B,IAAIX,WAAW,CAACK,IAAI,CAAC,IAAKA,IAAiB,CAACK,MAAM,GAAG,CAAC,EAAE;QACtDC,iBAAiB,GAAG,IAAI;OACzB,MAAM,IAAIR,UAAU,CAACE,IAAI,CAAC,EAAE;QAC3B,KAAK,MAAMO,GAAG,IAAIP,IAAI,EAAE;UACtB,IAAIA,IAAI,CAACQ,cAAc,CAACD,GAAG,CAAC,EAAE;YAC5BD,iBAAiB,GAAG,IAAI;YACxB;;;OAGL,MAAM;QACL;QACAA,iBAAiB,GAAG,IAAI;;MAE1B,IAAIA,iBAAiB,EAAE;QACrB,MAAM,IAAI5C,UAAU,CAChB,6BAA6B0C,eAAe,qBAAqB,GACjE,WAAWJ,IAAI,EAAE,CAAC;;;IAG1B,OAAO,EAAE;;EAEX,IAAIA,IAAI,IAAI,IAAI,EAAE;IAChB,OAAOC,KAAK,CAACQ,GAAG,CAACC,IAAI,IAAI,IAAI,CAAC;;EAGhC,IAAIC,MAAgB;EACpB,IAAIb,UAAU,CAACE,IAAI,CAAC,EAAE;IACpBA,IAAI,GAAGA,IAAqC;IAC5CW,MAAM,GAAG,EAAE;IACX,KAAK,MAAMD,IAAI,IAAIT,KAAK,EAAE;MACxB,IAAID,IAAI,CAACU,IAAI,CAAC,IAAI,IAAI,EAAE;QACtB,MAAM,IAAIhD,UAAU,CAChB,yBAAyBgD,IAAI,gCAAgC,GAC7D,GAAGT,KAAK,EAAE,CAAC;;MAEjBU,MAAM,CAACC,IAAI,CAACZ,IAAI,CAACU,IAAI,CAAC,CAAC;;GAE1B,MAAM,IAAIf,WAAW,CAACK,IAAI,CAAC,EAAE;IAC5BA,IAAI,GAAGA,IAAgB;IACvB,IAAIA,IAAI,CAACK,MAAM,KAAKJ,KAAK,CAACI,MAAM,EAAE;MAChC,MAAM,IAAI3C,UAAU,CAChB,6BAA6B0C,eAAe,iBAAiB,GAC7D,iEAAiE,GACjE,mCAAmCH,KAAK,CAACI,MAAM,kBAAkB,GACjE,gDAAgDL,IAAI,EAAE,CAAC;;IAE7DW,MAAM,GAAGX,IAAI;GACd,MAAM;IACLA,IAAI,GAAGA,IAAc;IACrB,IAAIC,KAAK,CAACI,MAAM,GAAG,CAAC,EAAE;MACpB,MAAM,IAAI3C,UAAU,CAChB,aAAa0C,eAAe,YAAYH,KAAK,CAACI,MAAM,cAAc,GAClE,0DACIL,IAAI,CAACa,KAAK,EAAE,CAAC;;IAEvBF,MAAM,GAAG,CAACX,IAAI,CAAC;;EAGjBW,MAAM,GAAGzB,0BAA0B,CAACyB,MAAM,CAAC;EAE3C;EACA,IAAIT,MAAM,IAAI,IAAI,EAAE;IAClB,KAAK,IAAIY,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGb,KAAK,CAACI,MAAM,EAAE,EAAES,CAAC,EAAE;MACrC,IAAIZ,MAAM,CAACY,CAAC,CAAC,IAAI,IAAI,EAAE;QACrB;;MAEF,MAAMC,KAAK,GAAGJ,MAAM,CAACG,CAAC,CAAC;MACvB,IAAIC,KAAK,CAACF,KAAK,CAACR,MAAM,KAAKH,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,EAAE;QAC3C,MAAM,IAAI3C,UAAU,CAChB,uBAAuB0C,eAAe,cAAcH,KAAK,CAACa,CAAC,CAAC,GAAG,GAC/D,WAAWZ,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,oCAAoC,GAC/D,SAASU,KAAK,CAACF,KAAK,EAAE,CAAC;;MAE7B,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGd,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,EAAE,EAAEW,CAAC,EAAE;QACzC,IAAIA,CAAC,KAAK,CAAC,IAAI,CAACb,cAAc,EAAE;UAC9B;UACA;;QAEF,MAAMc,GAAG,GAAGF,KAAK,CAACF,KAAK,CAACG,CAAC,CAAC;QAC1B,MAAME,MAAM,GAAGhB,MAAM,CAACY,CAAC,CAAC,CAACE,CAAC,CAAC;QAC3B,IAAIE,MAAM,IAAI,IAAI,IAAIA,MAAM,IAAI,CAAC,IAAID,GAAG,KAAKC,MAAM,EAAE;UACnD,MAAM,IAAIxD,UAAU,CAChB,GAAG0C,eAAe,2CAA2C,GAC7D,sBAAsBF,MAAM,CAACY,CAAC,CAAC,CAACK,KAAK,CAAC,CAAC,EAAEjB,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,CAAC,IAAI,GAC9D,yBACIH,MAAM,CAACY,CAAC,CAAC,CAACK,KAAK,CAAC,CAAC,EAAEjB,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,CAAC,IAAI,GAC5C,YAAYD,eAAe,2BACvBW,KAAK,CAACF,KAAK,CAAC,CAAC,CAAC,EAAE,GACpB,+BACIE,KAAK,CAACF,KAAK,CAACM,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAACF,KAAK,CAACR,MAAM,CAAC,GAAG,GAC/C,mBAAmBU,KAAK,CAACF,KAAK,IAAI,CAAC;;;;;EAK/C,OAAOF,MAAM;AACf;AAEA;;;;;;;AAOA,OAAM,SAAUS,iBAAiBA,CAC7BC,MAAgB,EAAEC,OAAiB,EAAEC,OAAkB;EACzD,MAAMC,IAAI,GAAGlD,MAAM,CAAC+C,MAAM,CAACZ,GAAG,CAACgB,KAAK,IAAIA,KAAK,CAACZ,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;EACxDW,IAAI,CAACE,IAAI,EAAE;EACX,MAAMC,IAAI,GAAGrD,MAAM,CAACgD,OAAO,CAACb,GAAG,CAACmB,MAAM,IAAIA,MAAM,CAACf,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;EAC3Dc,IAAI,CAACD,IAAI,EAAE;EACX;EACA,IAAIF,IAAI,CAACnB,MAAM,GAAG,CAAC,EAAE;IACnB,MAAM,IAAI3C,UAAU,CAChB,gEAAgE,GAChE,oBAAoB,GACpB,GAAGmE,IAAI,CAACC,SAAS,CAACT,MAAM,CAACZ,GAAG,CAACgB,KAAK,IAAIA,KAAK,CAACZ,KAAK,CAAC,CAAC,EAAE,CAAC;;EAE5D,IAAIc,IAAI,CAACtB,MAAM,GAAG,CAAC,EAAE;IACnB,MAAM,IAAI3C,UAAU,CAChB,iEAAiE,GACjE,oBAAoB,GACpB,GAAGmE,IAAI,CAACC,SAAS,CAACR,OAAO,CAACb,GAAG,CAACmB,MAAM,IAAIA,MAAM,CAACf,KAAK,CAAC,CAAC,EAAE,CAAC;;EAE/D,IAAIW,IAAI,CAACnB,MAAM,GAAG,CAAC,IAAIsB,IAAI,CAACtB,MAAM,GAAG,CAAC,IAAI,CAAClD,IAAI,CAAC4E,WAAW,CAACP,IAAI,EAAEG,IAAI,CAAC,EAAE;IACvE,MAAM,IAAIjE,UAAU,CAChB,iEAAiE,GACjE,kBAAkB8D,IAAI,CAAC,CAAC,CAAC,wBAAwBG,IAAI,CAAC,CAAC,CAAC,UAAU,GAClE,YAAY,CAAC;;AAErB;AAEA;;;;;;;;;AASA,SAASK,+BAA+BA,CACpCV,OAAiB,EAAEW,OAAyB,EAAEC,YAAqB;EACrE;EACA,MAAMC,SAAS,GAAG,CAChBtE,MAAM,CAACuE,gBAAgB,EAAEvE,MAAM,CAACwE,kBAAkB,EAClDxE,MAAM,CAACyE,uBAAuB,CAC/B;EACD,KAAK,IAAIxB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGQ,OAAO,CAACjB,MAAM,EAAE,EAAES,CAAC,EAAE;IACvC,MAAMyB,CAAC,GAAGjB,OAAO,CAACR,CAAC,CAAC;IACpB,MAAM0B,IAAI,GAAGP,OAAO,CAACnB,CAAC,CAAC;IACvB,MAAMD,KAAK,GAAGqB,YAAY,CAACpB,CAAC,CAAC;IAC7B,IAAI0B,IAAI,IAAI,IAAI,EAAE;MAChB;;IAEF,IAAIA,IAAI,KAAK3E,MAAM,CAACyE,uBAAuB,EAAE;MAC3C,IAAIC,CAAC,CAAC1B,KAAK,CAAC0B,CAAC,CAAC1B,KAAK,CAACR,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,EAAE;QACrC,MAAM,IAAI3C,UAAU,CAChB,2CAA2C6E,CAAC,CAAC1B,KAAK,eAAe,GACjE,+DAA+D,GAC/D,6DAA6D,GAC7D,qBAAqB,CAAC;QAC1B;;;IAGJ,IAAIsB,SAAS,CAACM,OAAO,CAACD,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;MAClC,MAAME,YAAY,GAAGH,CAAC,CAAC1B,KAAK,CAACM,KAAK,CAAC,CAAC,CAAC;MACrC,MAAMwB,WAAW,GAAG9B,KAAK,CAACM,KAAK,CAAC,CAAC,CAAC;MAClC,KAAK,IAAIH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0B,YAAY,CAACrC,MAAM,EAAE,EAAEW,CAAC,EAAE;QAC5C,MAAM4B,SAAS,GAAGF,YAAY,CAAC1B,CAAC,CAAC;QACjC,MAAM6B,MAAM,GAAGF,WAAW,CAAC3B,CAAC,CAAC;QAC7B,IAAI6B,MAAM,IAAI,IAAI,IAAID,SAAS,KAAKC,MAAM,EAAE;UAC1C,MAAM,IAAInF,UAAU,CAChB,8BAA8B6E,CAAC,CAAC1B,KAAK,qBAAqB,GAC1D,mBAAmBA,KAAK,qCAAqC,GAC7D,uDAAuD,CAAC;;;;;AAKtE;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA,SAASiC,cAAcA,CACnB9C,IAAqB,EAAEC,KAAe,EAAEC,MAAgB,EACxDC,cAAc,GAAG,IAAI,EAAEC,eAAe,GAAG,EAAE;EAC7C,IAAIO,MAAgB;EACpB,IAAIf,KAAK,CAACC,OAAO,CAACG,IAAI,CAAC,EAAE;IACvB,IAAIA,IAAI,CAACK,MAAM,KAAKJ,KAAK,CAACI,MAAM,EAAE;MAChC,MAAM,IAAI3C,UAAU,CAChB,6BAA6B0C,eAAe,iBAAiB,GAC7D,iEAAiE,GACjE,uCAAuCH,KAAK,CAACI,MAAM,aAAa,GAChE,oBAAoBL,IAAI,CAACK,MAAM,cAAc,CAAC;;IAEpDM,MAAM,GAAGX,IAAI;GACd,MAAM;IACL,IAAIC,KAAK,CAACI,MAAM,GAAG,CAAC,EAAE;MACpB,MAAM,IAAI3C,UAAU,CAChB,qBAAqBuC,KAAK,CAACI,MAAM,IAAID,eAAe,YAAY,GAChE,wDAAwD,GACxD,GAAGyB,IAAI,CAACC,SAAS,CAAC9B,IAAI,CAACa,KAAK,CAAC,GAAG,CAAC;;IAEvCF,MAAM,GAAG,CAACX,IAAI,CAAC;;EAGjB,IAAIE,MAAM,IAAI,IAAI,EAAE;IAClB,KAAK,IAAIY,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGb,KAAK,CAACI,MAAM,EAAE,EAAES,CAAC,EAAE;MACrC,IAAIZ,MAAM,CAACY,CAAC,CAAC,IAAI,IAAI,EAAE;QACrB;;MAEF,MAAMC,KAAK,GAAGJ,MAAM,CAACG,CAAC,CAAC;MACvB,IAAIC,KAAK,CAACF,KAAK,CAACR,MAAM,KAAKH,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,EAAE;QAC3C,MAAM,IAAI3C,UAAU,CAChB,uBAAuB0C,eAAe,cAAcH,KAAK,CAACa,CAAC,CAAC,GAAG,GAC/D,WAAWZ,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,oCAAoC,GAC/D,SAASwB,IAAI,CAACC,SAAS,CAACf,KAAK,CAACF,KAAK,CAAC,EAAE,CAAC;;MAE7C,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGd,MAAM,CAACY,CAAC,CAAC,CAACT,MAAM,EAAE,EAAEW,CAAC,EAAE;QACzC,IAAIA,CAAC,KAAK,CAAC,IAAI,CAACb,cAAc,EAAE;UAC9B;;QAEF,MAAMc,GAAG,GAAGF,KAAK,CAACF,KAAK,CAACG,CAAC,CAAC;QAC1B,MAAME,MAAM,GAAGhB,MAAM,CAACY,CAAC,CAAC,CAACE,CAAC,CAAC;QAC3B,IAAIE,MAAM,IAAI,IAAI,EAAE;UAClB,IAAIA,MAAM,KAAKD,GAAG,EAAE;YAClB,MAAM,IAAIvD,UAAU,CAChB,uBAAuB0C,eAAe,aAAa,GACnD,GAAGH,KAAK,CAACa,CAAC,CAAC,kBAAkBe,IAAI,CAACC,SAAS,CAAC5B,MAAM,CAACY,CAAC,CAAC,CAAC,OAAO,GAC7D,wBAAwBe,IAAI,CAACC,SAAS,CAACf,KAAK,CAACF,KAAK,CAAC,GAAG,CAAC;;;;;;AAMvE;AAEA;;;;;;;;;;;;;AAaA,OAAM,SAAUkC,cAAcA,CAC1BC,OAC+C,EAC/CC,WAAqB;EACvB,IAAID,OAAO,IAAI,IAAI,IAAIpD,KAAK,CAACC,OAAO,CAACmD,OAAO,CAAC,IAAIA,OAAO,CAAC3C,MAAM,KAAK,CAAC,EAAE;IACrE,OAAO4C,WAAW,CAACxC,GAAG,CAACC,IAAI,IAAI,EAAE,CAAC;;EAGpC,IAAIwC,cAC+C;EACnD,IAAI,OAAOF,OAAO,KAAK,QAAQ,IAAI,OAAOA,OAAO,KAAK,UAAU,EAAE;IAChEE,cAAc,GAAG,CAACF,OAAO,CAAC;GAC3B,MAAM,IAAIpD,KAAK,CAACC,OAAO,CAACmD,OAAO,CAAC,IAAI,OAAOA,OAAO,KAAK,QAAQ,EAAE;IAChEE,cAAc,GAAGF,OAC0D;GAC5E,MAAM;IACL,MAAM,IAAIG,SAAS,CACf,8DAA8D,GAC9D,sCAAsCH,OAAO,EAAE,CAAC;;EAGtD,IAAIpD,KAAK,CAACC,OAAO,CAACqD,cAAc,CAAC,EAAE;IACjC;IACA,OAAOD,WAAW,CAACxC,GAAG,CAClBC,IAAI,IAAIwC,cAA8C,CAAC;GAC5D,MAAM;IACL;IACA,MAAME,aAAa,GAAwC,EAAE;IAC7D,KAAK,MAAM1C,IAAI,IAAIuC,WAAW,EAAE;MAC9B,IAAII,aAAa,GACbH,cAAc,CAAC1C,cAAc,CAACE,IAAI,CAAC,GAAGwC,cAAc,CAACxC,IAAI,CAAC,GAAG,EAAE;MACnE,IAAI,CAACd,KAAK,CAACC,OAAO,CAACwD,aAAa,CAAC,EAAE;QACjCA,aAAa,GAAG,CAACA,aAAa,CAAC;;MAEjCD,aAAa,CAACxC,IAAI,CAACyC,aAAa,CAAC;;IAEnC,OAAOD,aAAa;;AAExB;AA2DA,MAAME,wBAAwB,GAAG,cAAc;AAE/C;;;;;;;;;;;;AAYA,MAAaC,WAAY,SAAQ5E,SAAS;EA4CxC6E,YAAYC,IAAmB;IAC7B,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACC,UAAU,GAAG,KAAK;EACzB;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAmCAC,OAAOA,CACHC,UAAmB,EAAEC,SAAoB,EACzCC,OAAA,GAEoDC,OAAO,CAACC,GAAG;IACjE,IAAI,CAAC,IAAI,CAACC,KAAK,EAAE;MACf,MAAM,IAAIvG,UAAU,CAChB,mEAAmE,GACnE,+DAA+D,GAC/D,gDAAgD,CAAC;;IAEvDa,YAAY,CAAC,IAAI,EAAEqF,UAAU,EAAEC,SAAS,EAAEC,OAAO,CAAC;EACpD;EAEA;;;;;;;;;;EAUAI,OAAOA,CAACT,IAAsB;IAC5B,IAAIA,IAAI,CAACjB,IAAI,IAAI,IAAI,EAAE;MACrBiB,IAAI,CAACjB,IAAI,GAAG,EAAE;;IAEhB,IAAI,CAACA,IAAI,GAAGiB,IAAI,CAACjB,IAAI;IAErB,IAAI,OAAOiB,IAAI,CAACU,SAAS,KAAK,QAAQ,EAAE;MACtC,IAAI,CAACC,UAAU,GAAGrG,UAAU,CAACsG,YAAY,CAACZ,IAAI,CAACU,SAAS,CAAC;MACzD,IAAI,CAACG,gBAAgB,GAAG,IAAI;KAC7B,MAAM;MACL,IAAI,EAAEb,IAAI,CAACU,SAAS,YAAYrH,SAAS,CAAC,EAAE;QAC1C,MAAM,IAAIY,UAAU,CAChB,6DAA6D,CAAC;;MAEpE,IAAI,CAAC0G,UAAU,GAAGX,IAAI,CAACU,SAAS;MAChC,IAAI,CAACG,gBAAgB,GAAG,KAAK;;IAG/B;IACA;IAEA;IACA,IAAIC,aAAa,GAAqB,EAAE;IACxC,IAAI,CAAC3E,KAAK,CAACC,OAAO,CAAC4D,IAAI,CAACjB,IAAI,CAAC,IAAI,OAAOiB,IAAI,CAACjB,IAAI,KAAK,QAAQ,IAC1D,OAAOiB,IAAI,CAACjB,IAAI,KAAK,UAAU,EAAE;MACnCiB,IAAI,CAACjB,IAAI,GAAGiB,IAAI,CAACjB,IAAsC;MACvD,KAAK,MAAM9B,IAAI,IAAI+C,IAAI,CAACjB,IAAI,EAAE;QAC5B,IAAI,IAAI,CAACS,WAAW,CAACR,OAAO,CAAC/B,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;UACzC,MAAM,IAAIhD,UAAU,CAChB,sCAAsCgD,IAAI,KAAK,GAC/C,qCAAqC,IAAI,CAACuC,WAAW,EAAE,CAAC;;;MAGhE,KAAK,MAAMvC,IAAI,IAAI,IAAI,CAACuC,WAAW,EAAE;QACnC,IAAIQ,IAAI,CAACjB,IAAI,CAAC9B,IAAI,CAAC,IAAI,IAAI,EAAE;UAC3BqD,OAAO,CAACS,IAAI,CACR,WAAW9D,IAAI,+CAA+C,GAC9D,8DAA8D,GAC9D,mBAAmBA,IAAI,kBAAkB,CAAC;;QAEhD6D,aAAa,CAAC3D,IAAI,CAAC/C,MAAM,CAAC4G,GAAG,CAAChB,IAAI,CAACjB,IAAI,CAAC9B,IAAI,CAAC,CAAC,CAAC;;KAElD,MAAM,IAAId,KAAK,CAACC,OAAO,CAAC4D,IAAI,CAACjB,IAAI,CAAC,EAAE;MACnC,IAAIiB,IAAI,CAACjB,IAAI,CAACnC,MAAM,KAAK,IAAI,CAACqE,OAAO,CAACrE,MAAM,EAAE;QAC5C,MAAM,IAAI3C,UAAU,CAChB,8DAA8D,GAC9D,+BAA+B,IAAI,CAACgH,OAAO,CAACrE,MAAM,cAAc,GAChE,uBAAuBoD,IAAI,CAACjB,IAAI,GAAG,CAAC;;MAE1C,MAAMmC,SAAS,GAAGlB,IAAI,CAACjB,IAAoC;MAC3D+B,aAAa,GAAGI,SAAS,CAAClE,GAAG,CAACmE,CAAC,IAAI/G,MAAM,CAAC4G,GAAG,CAACG,CAAC,CAAC,CAAC;KAClD,MAAM;MACL,MAAMC,YAAY,GAAGhH,MAAM,CAAC4G,GAAG,CAAChB,IAAI,CAACjB,IAAI,CAAC;MAC1C,IAAI,CAACkC,OAAO,CAACI,OAAO,CAACC,CAAC,IAAG;QACvBR,aAAa,CAAC3D,IAAI,CAACiE,YAAY,CAAC;MAClC,CAAC,CAAC;;IAGJ,IAAI,CAACN,aAAa,GAAGA,aAAa;IAElC,IAAI,CAACS,eAAe,GAAG,EAAE;IACzB,IAAI,CAACC,gBAAgB,GAAG,EAAE;IAC1B,IAAI,CAACC,WAAW,GAAG,EAAE;IACrB,KAAK,IAAIpE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC4D,OAAO,CAACrE,MAAM,EAAE,EAAES,CAAC,EAAE;MAC5C;MACA,MAAMD,KAAK,GAAG,IAAI,CAACsE,oBAAoB,CAACrE,CAAC,CAAC;MAC1C,MAAMJ,IAAI,GAAG,IAAI,CAACuC,WAAW,CAACnC,CAAC,CAAC;MAChC,IAAI,CAACkE,eAAe,CAACpE,IAAI,CAACF,IAAI,CAAC;MAC/B,IAAI,CAACuE,gBAAgB,CAACrE,IAAI,CAACC,KAAK,CAAC;MACjC,IAAI,CAACqE,WAAW,CAACtE,IAAI,CAAC,IAAI,CAAC2D,aAAa,CAACzD,CAAC,CAAC,CAAC;;IAG9C;IACA;IACA,MAAMsE,iBAAiB,GAAa,EAAE;IAEtC;IACA,IAAI,CAACpC,OAAO,GAAGS,IAAI,CAACT,OAAO;IAC3B;IACA,IAAI,CAACqC,YAAY,GAAG,CAAC,MAAM,CAAC;IAC5B,IAAI,CAACC,cAAc,GAAG,EAAE;IAExB;IACA;IACA;IACA;IACA/H,SAAS,CAAC,MAAM,EAAE,MAAK;MACrB,KAAK,IAAIuD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC4D,OAAO,CAACrE,MAAM,EAAE,EAAES,CAAC,EAAE;QAC5C,IAAIsE,iBAAiB,CAAC3C,OAAO,CAAC3B,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;UACvC;;QAEF;QACA;QACA,MAAMyE,YAAY,GAAG,IAAI,CAAChB,aAAa,CAACzD,CAAC,CAAC;QAC1C,IAAI,IAAI,CAAC4D,OAAO,CAACrE,MAAM,GAAG,CAAC,EAAE;UAC3B,IAAI,CAACiF,cAAc,CAAC1E,IAAI,CAAC,CAAC2E,YAAY,EAAEzE,CAAC,CAAC,CAAC;UAC3C,IAAI,CAACuE,YAAY,CAACzE,IAAI,CAAC,IAAI,CAACqC,WAAW,CAACnC,CAAC,CAAC,GAAG,OAAO,CAAC;;;MAIzD;MACA;IACF,CAAC,CAAC;IAEF,MAAMsC,aAAa,GAAGL,cAAc,CAACU,IAAI,CAACT,OAAO,EAAE,IAAI,CAACC,WAAW,CAAC;IACpE;IAEA;;;IAGA,MAAMuC,YAAY,GACdA,CAACC,WAAmB,EAAEC,UAAkB,EACvCC,YAA4B,KAAI;MAC/B,IAAI,IAAI,CAAC1C,WAAW,CAAC5C,MAAM,GAAG,CAAC,EAAE;QAC/BqF,UAAU,GAAG,IAAI,CAACzC,WAAW,CAACwC,WAAW,CAAC,GAAG,GAAG,GAAGC,UAAU;;MAE/D,IAAI,CAACL,YAAY,CAACzE,IAAI,CAAC8E,UAAU,CAAC;MAClC,IAAI,CAACJ,cAAc,CAAC1E,IAAI,CAAC,CAAC+E,YAAY,EAAEF,WAAW,CAAC,CAAC;IACvD,CAAC;IAELlI,SAAS,CAAC,QAAQ,EAAE,MAAK;MACvB,KAAK,IAAIuD,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC4D,OAAO,CAACrE,MAAM,EAAE,EAAES,CAAC,EAAE;QAC5C,IAAIsE,iBAAiB,CAAC3C,OAAO,CAAC3B,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;UACvC;;QAEF,MAAMuC,aAAa,GAAGD,aAAa,CAACtC,CAAC,CAAC;QACtC;QAEA;QACA,MAAM8E,aAAa,GAAI5C,OAAqC,IAAI;UAC9D,MAAM6C,gBAAgB,GAAG,EAAE;UAC3B,IAAIH,UAAkB;UACtB,IAAII,KAAqB;UACzB,IAAIC,gBAAgC;UACpC;UAEA,KAAK,MAAMC,MAAM,IAAIhD,OAAO,EAAE;YAC5B,IAAI,OAAOgD,MAAM,KAAK,QAAQ,IAC1B,CAAC,UAAU,EAAE,KAAK,EAAE,cAAc,EAAE,IAAI,CAAC,CAACvD,OAAO,CAACuD,MAAM,CAAC,KACrD,CAAC,CAAC,EAAE;cACV,MAAMC,WAAW,GAAG,IAAI,CAACd,oBAAoB,CAACrE,CAAC,CAAC;cAEhD,IAAImF,WAAW,CAACA,WAAW,CAAC5F,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,IACzC,IAAI,CAACkE,aAAa,CAACzD,CAAC,CAAC,KAAKjD,MAAM,CAACwE,kBAAkB,EAAE;gBACvD;gBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAACI,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBAC9CF,KAAK,GAAGhI,OAAO,CAACoI,cAAc;iBAC/B,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAACzD,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBACxDF,KAAK,GAAGhI,OAAO,CAACuE,kBAAkB;;eAErC,MAAM,IACH,IAAI,CAACkC,aAAa,CAACzD,CAAC,CAAC,KACrBjD,MAAM,CAACsI,6BAA6B,EAAE;gBACxC;gBACA;gBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC1D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBAC9CF,KAAK,GAAGhI,OAAO,CAACsI,yBAAyB;iBAC1C,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC3D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBACxDF,KAAK,GAAGhI,OAAO,CAACqI,6BAA6B;;eAEhD,MAAM;gBACL;gBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC1D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBAC9CF,KAAK,GAAGhI,OAAO,CAACuI,mBAAmB;iBACpC,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC5D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;kBACxDF,KAAK,GAAGhI,OAAO,CAACwE,uBAAuB;;;cAG3C,IAAIgE,MAAc;cAClB,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC7D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;gBAC9CM,MAAM,GAAG,KAAK;eACf,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC7D,OAAO,CAACuD,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;gBACxDM,MAAM,GAAG,IAAI;;cAEf;cACAP,gBAAgB,GAAGD,KAAK;cACxBJ,UAAU,GAAGG,gBAAgB,GAAGS,MAAM;aACvC,MAAM;cACL,MAAMC,QAAQ,GAAGzI,OAAO,CAAC2G,GAAG,CAACuB,MAAM,CAAC;cACpC;cACAD,gBAAgB,GAAGQ,QAAQ;cAC3Bb,UAAU,GACNG,gBAAgB,GAAG/H,OAAO,CAAC0I,mBAAmB,CAACR,MAAM,CAAC;;YAG5D;YACA,IAAIS,YAA4B;YAChClJ,SAAS,CAACmI,UAAU,EAAE,MAAK;cACzBe,YAAY,GAAGV,gBAAgB;YACjC,CAAC,CAAC;YACFP,YAAY,CAAC1E,CAAC,EAAE4E,UAAU,EAAEe,YAAY,CAAC;;QAE7C,CAAC;QAEDb,aAAa,CAACvC,aAAa,CAAC;QAC5B;;IAEJ,CAAC,CAAC;IAEF;IACA;IACA,IAAI,CAACqD,yBAAyB,GAAG,IAAI,CAACC,gBAAgB;EACxD;EAEA;;;;;;;;;EASUC,gCAAgCA,CAAA;IACxC,IAAI,IAAI,CAACF,yBAAyB,IAAI,IAAI,EAAE;MAC1C;;IAEF,IAAI,IAAI,CAACC,gBAAgB,CAACtG,MAAM,KAC5B,IAAI,CAACqG,yBAAyB,CAACrG,MAAM,EAAE;MACzC0D,OAAO,CAACS,IAAI,CACR,+DAA+D,GAC/D,yDAAyD,GACzD,+BAA+B,CAAC;;EAExC;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA+BAqC,QAAQA,CACJnH,CAAkB,EAAE6C,CAAkB,EACtCkB,IAAA,GAA0B,EAAE;IAC9B,MAAMqD,SAAS,GAAGrD,IAAI,CAACqD,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGrD,IAAI,CAACqD,SAAS;IAC9D9H,cAAc,CAAC8H,SAAS,CAAC;IAEzB;IACA;IACA,MAAM3G,cAAc,GAAG,IAAI;IAC3B,MAAM4G,gBAAgB,GAClB,IAAI,CAACC,qBAAqB,CAACtH,CAAC,EAAE6C,CAAC,EAAEpC,cAAc,EAAE2G,SAAS,CAAC;IAC/D,IAAI;MACF;MACA;MACA,MAAMG,GAAG,GAAGF,gBAAgB,CAAC,CAAC,CAAC,CAACG,MAAM,CAACH,gBAAgB,CAAC,CAAC,CAAC,CAAC;MAC3D,IAAI,CAACI,gBAAgB,EAAE;MACvB,MAAMC,CAAC,GAAG,IAAI,CAACC,YAAY;MAC3B,MAAMC,QAAQ,GACV,IAAI,CAACC,QAAQ,CAACH,CAAC,EAAEH,GAAG,EAAEH,SAAS,EAAErD,IAAI,CAAC+D,OAAO,EAAE/D,IAAI,CAACgE,KAAK,CAAC;MAC9D,OAAOtJ,gBAAgB,CAACmJ,QAAQ,CAAC;KAClC,SAAS;MACRrI,iBAAiB,CAAC8H,gBAAgB,CAAC,CAAC,CAAC,EAAErH,CAAC,CAAC;MACzCT,iBAAiB,CAAC8H,gBAAgB,CAAC,CAAC,CAAC,EAAExE,CAAC,CAAC;;EAE7C;EAEA;EACA;EACA;;;;;;;;;;;;;;;;;;;;EAoBA,MAAMzD,eAAeA,CAAC4I,OAAoB,EAAEjE,IAA+B;IAEzE,IAAI,CAAC0D,gBAAgB,EAAE;IACvB,OAAOrI,eAAe,CAAC,IAAI,EAAE4I,OAAO,EAAEjE,IAAI,CAAC;EAC7C;EAEA;;;;;;;;;;EAUQkE,eAAeA,CACnBV,GAAoB,EAAEH,SAAkB,EAAEW,KAAc,EACxDG,SAAS,GAAG,OAAO;IACrB,IAAIC,UAAkB;IACtB,IAAIJ,KAAK,IAAI,IAAI,EAAE;MACjBI,UAAU,GAAG,IAAI;MACjB,IAAIf,SAAS,IAAI,IAAI,EAAE;QACrB,MAAM,IAAIpJ,UAAU,CAChB,MAAMkK,SAAS,+CAA+C,GAC9D,mBAAmBd,SAAS,EAAE,CAAC;;KAEtC,MAAM,IAAIG,GAAG,IAAI,IAAI,EAAE;MACtB,IAAIrH,KAAK,CAACC,OAAO,CAACoH,GAAG,CAAC,EAAE;QACtBY,UAAU,GAAGZ,GAAG,CAAC,CAAC,CAAC,CAACpG,KAAK,CAAC,CAAC,CAAC;OAC7B,MAAM;QACLgH,UAAU,GAAGZ,GAAG,CAACpG,KAAK,CAAC,CAAC,CAAC;;KAE5B,MAAM;MACL,MAAM,IAAInD,UAAU,CAChB,wDAAwD,GACxD,GAAGkK,SAAS,sBAAsB,CAAC;;IAEzC,OAAOC,UAAU;EACnB;EAEA;;;;;;;EAOAjJ,OAAOA,CAACyC,MAAsC,EAAEqD,OAAwB;IAEtE,IAAI9E,KAAK,CAACC,OAAO,CAAC6E,OAAO,CAAC,IAAIA,OAAO,CAACrE,MAAM,KAAK,CAAC,EAAE;MAClD,MAAM,IAAI3C,UAAU,CAChB,oDAAoD,CAAC;;IAG3D,MAAMoK,cAAc,GAAGlI,KAAK,CAACC,OAAO,CAAC6E,OAAO,CAAC;IAC7C,MAAMzB,WAAW,GACZ6E,cAAc,GAAGpD,OAAO,GAAG,CAACA,OAAO,CAAE;IAC1C,MAAMqD,qBAAqB,GAAG,IAAI,CAACC,uBAAuB,CAAC/E,WAAW,CAAC;IAEvE;IACA,MAAMgF,QAAQ,GAAG,IAAIpJ,QAAQ,EAAE;IAC/B,IAAIwC,MAAM,YAAYpE,MAAM,EAAE;MAC5BoE,MAAM,GAAG,CAACA,MAAM,CAAC;;IAEnB,IAAIzB,KAAK,CAACC,OAAO,CAACwB,MAAM,CAAC,EAAE;MACzB,IAAIA,MAAM,CAAChB,MAAM,KAAK,IAAI,CAACgB,MAAM,CAAChB,MAAM,EAAE;QACxC,MAAM,IAAI3C,UAAU,CAChB,kCAAkC2D,MAAM,CAAChB,MAAM,IAAI,GACnD,oDAAoD,GACpD,IAAI,IAAI,CAACgB,MAAM,CAAChB,MAAM,IAAI,CAAC;;MAEjC,KAAK,IAAIS,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACO,MAAM,CAAChB,MAAM,EAAE,EAAES,CAAC,EAAE;QAC3CmH,QAAQ,CAACC,GAAG,CAAC,IAAI,CAAC7G,MAAM,CAACP,CAAC,CAAC,EAAEO,MAAM,CAACP,CAAC,CAAC,CAAC;;KAE1C,MAAM;MACL,KAAK,MAAMW,KAAK,IAAI,IAAI,CAACJ,MAAM,EAAE;QAC/B,MAAM8G,WAAW,GAAG9G,MAAM,CAACI,KAAK,CAACf,IAAI,CAAC;QACtC,IAAIyH,WAAW,IAAI,IAAI,EAAE;UACvB,MAAM,IAAIzK,UAAU,CAChB,8CAA8C+D,KAAK,CAACf,IAAI,EAAE,CAAC;;QAEjEuH,QAAQ,CAACC,GAAG,CAACzG,KAAK,EAAE0G,WAAW,CAAC;;;IAIpC;IACA,MAAMC,cAAc,GAAGxJ,OAAO,CAACmJ,qBAAqB,EAAEE,QAAQ,CAAa;IAC3E,OAAOH,cAAc,GAAGM,cAAc,GAAGA,cAAc,CAAC,CAAC,CAAC;EAC5D;EAEA;;;EAGQJ,uBAAuBA,CAACK,mBAA6B;IAE3D,MAAMN,qBAAqB,GACvB7J,YAAY,CAAC,IAAI,EAAEmK,mBAAmB,CAAChI,MAAM,CAAC;IAClD,IAAIiI,gBAAgB,GAAGD,mBAAmB,CAAChI,MAAM;IACjD,KAAK,MAAMkI,KAAK,IAAI,IAAI,CAACC,MAAM,EAAE;MAC/B,MAAMC,YAAY,GACd7I,KAAK,CAACC,OAAO,CAAC0I,KAAK,CAACG,MAAM,CAAC,GAAGH,KAAK,CAACG,MAAM,GAAG,CAACH,KAAK,CAACG,MAAM,CAAC;MAC/D,MAAMC,gBAAgB,GAAGF,YAAY,CAAChI,GAAG,CAACiI,MAAM,IAAIA,MAAM,CAAChI,IAAI,CAAC;MAChE,KAAK,IAAII,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGuH,mBAAmB,CAAChI,MAAM,EAAE,EAAES,CAAC,EAAE;QACnD,MAAM8H,KAAK,GAAGD,gBAAgB,CAAClG,OAAO,CAAC4F,mBAAmB,CAACvH,CAAC,CAAC,CAAC;QAC9D,IAAI8H,KAAK,KAAK,CAAC,CAAC,EAAE;UAChBb,qBAAqB,CAACjH,CAAC,CAAC,GAAG2H,YAAY,CAACG,KAAK,CAAC;UAC9CN,gBAAgB,EAAE;;QAEpB,IAAIA,gBAAgB,KAAK,CAAC,EAAE;UAC1B;;;MAGJ,IAAIA,gBAAgB,KAAK,CAAC,EAAE;QAC1B;;;IAIJ,IAAIA,gBAAgB,GAAG,CAAC,EAAE;MACxB,MAAMO,cAAc,GAAa,EAAE;MACnCd,qBAAqB,CAACjD,OAAO,CAAC,CAACgE,MAAM,EAAEhI,CAAC,KAAI;QAC1C,IAAIgI,MAAM,IAAI,IAAI,EAAE;UAClBD,cAAc,CAACjI,IAAI,CAACyH,mBAAmB,CAACvH,CAAC,CAAC,CAAC;;MAE/C,CAAC,CAAC;MACF,MAAM,IAAIpD,UAAU,CAChB,kDAAkD,GAClD,GAAGmE,IAAI,CAACC,SAAS,CAAC+G,cAAc,CAAC,EAAE,CAAC;;IAE1C,OAAOd,qBAAqB;EAC9B;EAEA;;;;;;;;;;;;;EAaQgB,WAAWA,CAAC9B,GAAoB,EAAEH,SAAS,GAAG,EAAE,EAAEU,OAAO,GAAG,KAAK;IAEvE,OAAO5K,GAAG,CAACoM,IAAI,CAAC,MAAK;MACnB,MAAMnB,UAAU,GAAG,IAAI,CAACF,eAAe,CAACV,GAAG,CAAC;MAC5C,IAAIO,OAAO,EAAE;QACX,MAAM,IAAIhK,mBAAmB,CACzB,+CAA+C,CAAC;;MAGtD;MACA;MACA;MACA;MAEA,MAAMyL,OAAO,GAAG9J,WAAW,CAAC0I,UAAU,EAAEf,SAAS,CAAC;MAClD,MAAMoC,WAAW,GAAe,IAAI,CAACxE,OAAO,CAACjE,GAAG,CAACiI,MAAM,IAAI,EAAE,CAAC;MAE9D;MACA,KAAK,IAAIS,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAGF,OAAO,CAAC5I,MAAM,EAAE,EAAE8I,UAAU,EAAE;QAClE,MAAMC,SAAS,GAAGxM,GAAG,CAACoM,IAAI,CAAC,MAAK;UAC9B,MAAMK,UAAU,GAAGJ,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;UACzC,MAAMG,QAAQ,GAAGL,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;UACvC;UACA;UACA,MAAMI,QAAQ,GAAGnK,WAAW,CAAC6H,GAAG,EAAEoC,UAAU,EAAEC,QAAQ,CAAC;UAEvD;UACA,MAAME,KAAK,GAAG,EAAE;UAChB,IAAI5J,KAAK,CAACC,OAAO,CAAC0J,QAAQ,CAAC,EAAE;YAC3B,KAAK,IAAIzI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGyI,QAAQ,CAAClJ,MAAM,EAAE,EAAES,CAAC,EAAE;cACxC0I,KAAK,CAAC5I,IAAI,CAAC;gBAACL,GAAG,EAAE,IAAI,CAACc,MAAM,CAACP,CAAC,CAAC;gBAAE2I,KAAK,EAAEF,QAAQ,CAACzI,CAAC;cAAC,CAAC,CAAC;;WAExD,MAAM;YACL0I,KAAK,CAAC5I,IAAI,CAAC;cAACL,GAAG,EAAE,IAAI,CAACc,MAAM,CAAC,CAAC,CAAC;cAAEoI,KAAK,EAAEF;YAAQ,CAAC,CAAC;;UAEpD,MAAMtB,QAAQ,GAAG,IAAIpJ,QAAQ,CAAC2K,KAAK,CAAC;UACpC,OAAO5K,OAAO,CAAC,IAAI,CAAC8F,OAAO,EAAEuD,QAAQ,CAAa;QACpD,CAAC,CAAC;QACFmB,SAAS,CAACtE,OAAO,CAAC,CAAC4E,QAAQ,EAAE5I,CAAC,KAAKoI,WAAW,CAACpI,CAAC,CAAC,CAACF,IAAI,CAAC8I,QAAQ,CAAC,CAAC;;MAEnE,OAAOvL,gBAAgB,CACnB+K,WAAW,CAACzI,GAAG,CAACwI,OAAO,IAAIrM,GAAG,CAACsK,MAAM,CAAC+B,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC;IACzD,CAAC,CAAC;EACJ;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;EA2BAU,OAAOA,CAACjK,CAAkB,EAAE+D,IAAA,GAAyB,EAAE;IACrD,MAAMmG,eAAe,GAAG1K,0BAA0B,CAACQ,CAAC,CAAC;IACrDoD,cAAc,CACV8G,eAAe,EAAE,IAAI,CAACC,UAAU,EAAE,IAAI,CAACC,eAAe,EAAE,KAAK,CAAC;IAClE,IAAI;MACF;MACA;MACA;MACA;MACA,MAAMhD,SAAS,GAAGrD,IAAI,CAACqD,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGrD,IAAI,CAACqD,SAAS;MAC9D9H,cAAc,CAAC8H,SAAS,CAAC;MACzB,OAAO,IAAI,CAACiC,WAAW,CAACa,eAAe,EAAE9C,SAAS,CAAC;KACpD,SAAS;MACR7H,iBAAiB,CAAC2K,eAAe,EAAElK,CAAC,CAAC;;EAEzC;EAEA;;;;;;;;;;;;;;;EAeAqK,cAAcA,CAACrK,CAAkB;IAC/BoD,cAAc,CAACpD,CAAC,EAAE,IAAI,CAACmK,UAAU,EAAE,IAAI,CAACC,eAAe,EAAE,IAAI,CAAC;IAC9D;IACA;IACA,MAAMhD,SAAS,GAAG,CAAClH,KAAK,CAACC,OAAO,CAACH,CAAC,CAAC,GAAGA,CAAC,CAAC,CAAC,CAAC,GAAGA,CAAC,EAAEmB,KAAK,CAAC,CAAC,CAAC;IACxD,OAAO,IAAI,CAACkI,WAAW,CAACrJ,CAAC,EAAEoH,SAAS,CAAC;EACvC;EAEUE,qBAAqBA,CAC3BtH,CAAgD,EAChD6C,CAAgD,EAAEpC,cAAc,GAAG,IAAI,EACvE2G,SAAkB;IACpB;IACA,IAAI,IAAI,CAAC1C,UAAU,IAAI,IAAI,EAAE;MAC3B,MAAM,IAAI3G,YAAY,CAClB,wDAAwD,GACxD,wCAAwC,CAAC;;IAE/C,MAAMyE,YAAY,GAAY,EAAE;IAChC,KAAK,IAAIpB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACmE,gBAAgB,CAAC5E,MAAM,EAAE,EAAES,CAAC,EAAE;MACrD,MAAMmF,WAAW,GAAG,IAAI,CAAChB,gBAAgB,CAACnE,CAAC,CAAC;MAC5C,MAAMkJ,MAAM,GAAG,IAAI,CAAC9E,WAAW,CAACpE,CAAC,CAAC;MAClC,IAAIkJ,MAAM,KAAKnM,MAAM,CAACsI,6BAA6B,EAAE;QACnDjE,YAAY,CAACtB,IAAI,CACbqF,WAAW,CAAC9E,KAAK,CAAC,CAAC,EAAE8E,WAAW,CAAC5F,MAAM,GAAG,CAAC,CAAC,CAAC6G,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;OAC9D,MAAM;QACL;QACAhF,YAAY,CAACtB,IAAI,CAACqF,WAAW,CAAC;;;IAGlCvG,CAAC,GAAGK,oBAAoB,CACpBL,CAAC,EAAE,IAAI,CAACuK,cAAc,EAAE,IAAI,CAACH,eAAe,EAAE,KAAK,EAAE,OAAO,CAAC;IACjEvH,CAAC,GAAGxC,oBAAoB,CACpBwC,CAAC,EAAE,IAAI,CAACyC,eAAe,EAAE9C,YAAY,EAAE,KAAK,EAAE,QAAQ,CAAC;IAC3D;IACAd,iBAAiB,CAAC1B,CAAC,EAAE6C,CAAC,EAAE,IAAI,CAAC;IAC7B;IACAP,+BAA+B,CAACO,CAAC,EAAE,IAAI,CAAC2C,WAAW,EAAE,IAAI,CAACD,gBAAgB,CAAC;IAC3E,IAAI,IAAI,CAACiF,QAAQ,IAAIpD,SAAS,IAAI,IAAI,IAAIA,SAAS,GAAG,CAAC,EAAE;MACvD,IAAIpH,CAAC,CAAC,CAAC,CAAC,CAACmB,KAAK,CAAC,CAAC,CAAC,GAAGiG,SAAS,KAAK,CAAC,EAAE;QACnC,MAAM,IAAIpJ,UAAU,CAChB,4DAA4D,GAC5D,wDAAwD,GACxD,GAAGoJ,SAAS,YAAYpH,CAAC,CAAC,CAAC,CAAC,CAACmB,KAAK,CAAC,CAAC,CAAC,aAAa,CAAC;;;IAG3D,OAAO,CAACnB,CAAC,EAAE6C,CAAC,CAAC;EACf;EAEU,MAAM4H,mBAAmBA,CAC/BzK,CAAgD,EAChD6C,CAAgD,EAChD6H,YAA6D,EAC7DC,WAAsD,EACtDlK,cAAc,GAAG,IAAI,EACrB2G,SAAkB;IACpB,MAAM,CAACwD,UAAU,EAAEC,UAAU,CAAC,GAC1B,IAAI,CAACvD,qBAAqB,CAACtH,CAAC,EAAE6C,CAAC,EAAEpC,cAAc,EAAE2G,SAAS,CAAC;IAC/D;IACA,IAAIsD,YAAY,IAAI,IAAI,EAAE;MACxB,MAAM,IAAII,KAAK,CAAC,qCAAqC,CAAC;;IAGxD,IAAIC,qBAAqB,GAAa,IAAI;IAC1C,IAAIJ,WAAW,IAAI,IAAI,EAAE;MACvB,MAAMK,YAAY,GACdnL,uBAAuB,CAAC8K,WAAW,EAAE,IAAI,CAACpH,WAAW,CAAC;MAC1DwH,qBAAqB,GAAG,EAAE;MAC1B,KAAK,IAAI3J,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG4J,YAAY,CAACrK,MAAM,EAAE,EAAES,CAAC,EAAE;QAC5C2J,qBAAqB,CAAC7J,IAAI,CACtB,MAAMpB,kBAAkB,CAAC+K,UAAU,CAACzJ,CAAC,CAAC,EAAE,IAAI,EAAE4J,YAAY,CAAC5J,CAAC,CAAC,CAAC,CAAC;;;IAIvE;IACA,OAAO,CAACwJ,UAAU,EAAEC,UAAU,EAAEE,qBAAqB,CAAC;EACxD;EAEA;;;;;;;;;;;EAWQlD,QAAQA,CACZH,CAA+B,EAAEH,GAAa,EAAEH,SAAkB,EAClEU,OAAO,GAAG,CAAC,EAAEC,KAAc;IAC7B,OAAO7K,GAAG,CAACoM,IAAI,CAAC,MAAK;MACnB,MAAMnB,UAAU,GAAG,IAAI,CAACF,eAAe,CAACV,GAAG,EAAEH,SAAS,EAAEW,KAAK,EAAE,OAAO,CAAC;MACvE,MAAMkD,IAAI,GAAa,EAAE;MACzB,IAAInD,OAAO,GAAG,CAAC,EAAE;QACf,MAAM,IAAIhK,mBAAmB,CAAC,sCAAsC,CAAC;;MAEvE;MACA,IAAIiK,KAAK,IAAI,IAAI,EAAE;QACjB,MAAM,IAAIjK,mBAAmB,CACzB,iDAAiD,CAAC;OACvD,MAAM;QACL,MAAMyL,OAAO,GAAG9J,WAAW,CAAC0I,UAAU,EAAEf,SAAS,CAAC;QAClD,MAAM8D,UAAU,GAAG1N,QAAQ,CAACsB,KAAK,CAAC,CAAC,EAAEqJ,UAAU,CAAC,CAAC;QACjD,KAAK,IAAIsB,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAGF,OAAO,CAAC5I,MAAM,EAAE,EAAE8I,UAAU,EAAE;UAClE,MAAME,UAAU,GAAGJ,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;UACzC,MAAMG,QAAQ,GAAGL,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;UACvC,MAAM0B,QAAQ,GACVzN,CAAC,CAAC0N,mBAAmB,CACjBF,UAAU,EAAEvB,UAAU,EAAEC,QAAQ,GAAGD,UAAU,CAAa;UAClE;UACA;UACA,MAAME,QAAQ,GAAGlK,oBAAoB,CAAC4H,GAAG,EAAE4D,QAAQ,CAAa;UAChE,MAAMzB,SAAS,GAAGhC,CAAC,CAACmC,QAAQ,CAAC;UAC7B,IAAIJ,UAAU,KAAK,CAAC,EAAE;YACpB,KAAK,IAAIrI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGsI,SAAS,CAAC/I,MAAM,EAAE,EAAES,CAAC,EAAE;cACzC6J,IAAI,CAAC/J,IAAI,CAAC7D,MAAM,CAAC,CAAC,CAAC,CAAC;;;UAGxB,KAAK,IAAI+D,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGsI,SAAS,CAAC/I,MAAM,EAAE,EAAES,CAAC,EAAE;YACzC,MAAM4I,QAAQ,GAAGN,SAAS,CAACtI,CAAC,CAAC;YAC7B6J,IAAI,CAAC7J,CAAC,CAAC,GACHlE,GAAG,CAACsL,GAAG,CAACyC,IAAI,CAAC7J,CAAC,CAAC,EAAElE,GAAG,CAACmO,GAAG,CAACzB,QAAQ,GAAGD,UAAU,EAAEK,QAAQ,CAAC,CAAC;;;QAGlE,KAAK,IAAI5I,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG6J,IAAI,CAACtK,MAAM,EAAE,EAAES,CAAC,EAAE;UACpC6J,IAAI,CAAC7J,CAAC,CAAC,GAAGlE,GAAG,CAACoO,GAAG,CAACL,IAAI,CAAC7J,CAAC,CAAC,EAAE+G,UAAU,CAAC;;;MAG1C,OAAO8C,IAAI;IACb,CAAC,CAAC;EACJ;EAEUM,sBAAsBA,CAAA;IAC9B,MAAMC,SAAS,GAAG,IAAI,CAAC7F,YAAY;IACnC;IACA;IACA,MAAM8F,gBAAgB,GAAG,EAAE;IAC3B,KAAK,IAAIrK,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoK,SAAS,CAAC7K,MAAM,EAAE,EAAES,CAAC,EAAE;MACzC,MAAMsK,KAAK,GAAGF,SAAS,CAACpK,CAAC,CAAC;MAC1B,IAAIuK,QAAQ,GAAGD,KAAK;MACpB,IAAInN,KAAK,CAACiN,SAAS,EAAEE,KAAK,CAAC,GAAG,CAAC,EAAE;QAC/B,MAAME,QAAQ,GAAGrN,KAAK,CAACiN,SAAS,CAAC/J,KAAK,CAAC,CAAC,EAAEL,CAAC,CAAC,EAAEsK,KAAK,CAAC;QACpDC,QAAQ,IAAI,IAAIC,QAAQ,EAAE;;MAE5BH,gBAAgB,CAACvK,IAAI,CAACyK,QAAQ,CAAC;;IAEjC,OAAOF,gBAAgB;EACzB;EAEA;;;;;;;;;;EAUUI,iBAAiBA,CAAA;IACzB,OAAQvL,IAAc,IAAI;MACxB,MAAMwL,UAAU,GAAa,EAAE;MAE/B,MAAMnK,MAAM,GAAGrB,IAAI,CAACmB,KAAK,CAAC,CAAC,EAAE,IAAI,CAACE,MAAM,CAAChB,MAAM,CAAC;MAChD,MAAMiB,OAAO,GAAGtB,IAAI,CAACmB,KAAK,CACtB,IAAI,CAACE,MAAM,CAAChB,MAAM,EAAE,IAAI,CAACgB,MAAM,CAAChB,MAAM,GAAG,IAAI,CAACqE,OAAO,CAACrE,MAAM,CAAC;MACjE,MAAMoL,aAAa,GAAGzL,IAAI,CAACmB,KAAK,CAC5B,IAAI,CAACE,MAAM,CAAChB,MAAM,GAAG,IAAI,CAACqE,OAAO,CAACrE,MAAM,EACxC,IAAI,CAACgB,MAAM,CAAChB,MAAM,GAAG,IAAI,CAACqE,OAAO,CAACrE,MAAM,GAAG,CAAC,CAAC;MAEjD,MAAMqL,aAAa,GAAa,EAAE;MAElC;MACA;MACA;MACA,MAAMC,iBAAiB,GAAGA,CAAA,KAAK;QAC7B,MAAMnC,KAAK,GAAG,EAAE;QAChB,KAAK,IAAI1I,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACO,MAAM,CAAChB,MAAM,EAAE,EAAES,CAAC,EAAE;UAC3C0I,KAAK,CAAC5I,IAAI,CAAC;YAACL,GAAG,EAAE,IAAI,CAACc,MAAM,CAACP,CAAC,CAAC;YAAE2I,KAAK,EAAEpI,MAAM,CAACP,CAAC;UAAC,CAAC,CAAC;;QAErD,MAAMmH,QAAQ,GAAG,IAAIpJ,QAAQ,CAAC2K,KAAK,CAAC;QACpC,MAAM9E,OAAO,GACT9F,OAAO,CAAC,IAAI,CAAC8F,OAAO,EAAEuD,QAAQ,EAAE;UAAC,UAAU,EAAE;QAAI,CAAC,CAAa;QACnE;QACA;QAEA,IAAI2D,SAAiB;QACrB,KAAK,IAAI9K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACyD,aAAa,CAAClE,MAAM,EAAE,EAAES,CAAC,EAAE;UAClD,MAAM+D,YAAY,GAAG,IAAI,CAACN,aAAa,CAACzD,CAAC,CAAC;UAC1C,IAAI0B,IAAI,GAAGqC,YAAY,CAACvD,OAAO,CAACR,CAAC,CAAC,EAAE4D,OAAO,CAAC5D,CAAC,CAAC,CAAC;UAC/C,IAAI2K,aAAa,CAAC3K,CAAC,CAAC,IAAI,IAAI,EAAE;YAC5B0B,IAAI,GAAGlD,mBAAmB,CAACkD,IAAI,EAAEiJ,aAAa,CAAC3K,CAAC,CAAC,CAAC;;UAGpD;UACA,MAAM+K,QAAQ,GAAWjP,GAAG,CAACkP,IAAI,CAACtJ,IAAI,CAAC;UACvC;UACAgJ,UAAU,CAAC5K,IAAI,CAACiL,QAAQ,CAAC;UACzB,IAAI/K,CAAC,KAAK,CAAC,EAAE;YACX8K,SAAS,GAAGpJ,IAAI;WACjB,MAAM;YACLoJ,SAAS,GAAGhP,GAAG,CAACsL,GAAG,CAAC0D,SAAS,EAAEpJ,IAAI,CAAC;;;QAIxC;QACA;QACA;QACA,KAAK,IAAI1B,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACwE,cAAc,CAACjF,MAAM,EAAE,EAAES,CAAC,EAAE;UACnD,IAAIiL,cAAsB;UAE1B,IAAI,IAAI,CAACrH,OAAO,CAACrE,MAAM,GAAG,CAAC,IAAIS,CAAC,GAAG,IAAI,CAAC4D,OAAO,CAACrE,MAAM,EAAE;YACtD0L,cAAc,GAAGP,UAAU,CAAC1K,CAAC,CAAC;WAC/B,MAAM;YACL,MAAMkF,MAAM,GAAG,IAAI,CAACV,cAAc,CAACxE,CAAC,CAAC,CAAC,CAAC,CAAC;YACxC,MAAM2E,WAAW,GAAG,IAAI,CAACH,cAAc,CAACxE,CAAC,CAAC,CAAC,CAAC,CAAC;YAC7CiL,cAAc,GACVnP,GAAG,CAACkP,IAAI,CAAC9F,MAAM,CAAC1E,OAAO,CAACmE,WAAW,CAAC,EAAEf,OAAO,CAACe,WAAW,CAAC,CAAC,CAAC;;UAGlE7I,GAAG,CAACoP,IAAI,CAACD,cAAc,CAAC;UACxB;UACAL,aAAa,CAAC9K,IAAI,CAACmL,cAAc,CAAC;;QAGpCH,SAAS,GAAGhP,GAAG,CAACkP,IAAI,CAACF,SAAS,CAAC;QAE/B;QACA,IAAI,CAACK,eAAe,EAAE,CAACnH,OAAO,CAACoH,eAAe,IAAG;UAC/CN,SAAS,GAAGhP,GAAG,CAACsL,GAAG,CAAC0D,SAAS,EAAEM,eAAe,CAAC;QACjD,CAAC,CAAC;QAEF,OAAON,SAAmB;MAC5B,CAAC;MAED,MAAMO,SAAS,GAAG,IAAI,CAACzF,yBAAyB,CAACjG,GAAG,CAChD2L,KAAK,IAAIA,KAAK,CAACC,IAAI,EAAkB,CAAC;MAC1C,MAAMC,UAAU,GAAG,IAAI;MACvB,MAAMC,cAAc,GAChB,IAAI,CAACnI,UAAU,CAACoI,QAAQ,CAACb,iBAAiB,EAAEW,UAAU,EAAEH,SAAS,CAAC;MAEtE,OAAO,CAACI,cAAc,CAAC,CAACrF,MAAM,CAACwE,aAAa,CAAC;IAC/C,CAAC;EACH;EAEA;;;;;EAKQvE,gBAAgBA,CAAA;IACtB,IAAI,CAACE,YAAY,GAAIrH,IAAc,IAAI;MACrC,OAAOpD,GAAG,CAACoM,IAAI,CAAC,MAAK;QACnB,MAAMyD,UAAU,GAAa,EAAE;QAC/B,IAAIb,SAAiB;QACrB,MAAMvK,MAAM,GAAGrB,IAAI,CAACmB,KAAK,CAAC,CAAC,EAAE,IAAI,CAACE,MAAM,CAAChB,MAAM,CAAC;QAChD,MAAMiB,OAAO,GAAGtB,IAAI,CAACmB,KAAK,CACtB,IAAI,CAACE,MAAM,CAAChB,MAAM,EAAE,IAAI,CAACgB,MAAM,CAAChB,MAAM,GAAG,IAAI,CAACqE,OAAO,CAACrE,MAAM,CAAC;QACjE,MAAMmJ,KAAK,GAAG,EAAE;QAChB,KAAK,IAAI1I,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACO,MAAM,CAAChB,MAAM,EAAE,EAAES,CAAC,EAAE;UAC3C0I,KAAK,CAAC5I,IAAI,CAAC;YAACL,GAAG,EAAE,IAAI,CAACc,MAAM,CAACP,CAAC,CAAC;YAAE2I,KAAK,EAAEpI,MAAM,CAACP,CAAC;UAAC,CAAC,CAAC;;QAErD,MAAMmH,QAAQ,GAAG,IAAIpJ,QAAQ,CAAC2K,KAAK,CAAC;QACpC,MAAM9E,OAAO,GAAG9F,OAAO,CAAC,IAAI,CAAC8F,OAAO,EAAEuD,QAAQ,CAAa;QAC3D;QACA,KAAK,IAAInH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACyD,aAAa,CAAClE,MAAM,EAAE,EAAES,CAAC,EAAE;UAClD,MAAM+D,YAAY,GAAG,IAAI,CAACN,aAAa,CAACzD,CAAC,CAAC;UAC1C;UACA;UACA,MAAM0B,IAAI,GAAW5F,GAAG,CAACkP,IAAI,CAACjH,YAAY,CAACvD,OAAO,CAACR,CAAC,CAAC,EAAE4D,OAAO,CAAC5D,CAAC,CAAC,CAAC,CAAC;UACnE,IAAIA,CAAC,KAAK,CAAC,EAAE;YACX8K,SAAS,GAAGpJ,IAAI;WACjB,MAAM;YACLoJ,SAAS,GAAGhP,GAAG,CAACsL,GAAG,CAAC0D,SAAS,EAAEpJ,IAAI,CAAC;;UAEtCiK,UAAU,CAAC7L,IAAI,CAACgL,SAAS,CAAC;;QAE5B;QACA,KAAK,IAAI9K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACwE,cAAc,CAACjF,MAAM,EAAE,EAAES,CAAC,EAAE;UACnD,MAAMkF,MAAM,GAAG,IAAI,CAACV,cAAc,CAACxE,CAAC,CAAC,CAAC,CAAC,CAAC;UACxC,MAAM2E,WAAW,GAAG,IAAI,CAACH,cAAc,CAACxE,CAAC,CAAC,CAAC,CAAC,CAAC;UAC7C;UACA,MAAM4L,UAAU,GACZ9P,GAAG,CAACkP,IAAI,CAAC9F,MAAM,CAAC1E,OAAO,CAACmE,WAAW,CAAC,EAAEf,OAAO,CAACe,WAAW,CAAC,CAAC,CAAC;UAChEgH,UAAU,CAAC7L,IAAI,CAAC8L,UAAoB,CAAC;;QAEvC,OAAOD,UAAU;MACnB,CAAC,CAAC;IACJ,CAAC;EACH;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAkCA,MAAME,GAAGA,CACLjN,CAAgD,EAChD6C,CAAgD,EAChDkB,IAAA,GAAqB,EAAE;IACzB,IAAI,IAAI,CAACC,UAAU,EAAE;MACnB,MAAM,IAAI8G,KAAK,CACX,8DAA8D,CAAC;;IAErE,IAAI,CAAC9G,UAAU,GAAG,IAAI;IACtB,IAAIrC,MAAgB;IACpB,IAAIC,OAAiB;IACrB,IAAIsL,cAAwB;IAC5B,IAAIC,eAAyB;IAC7B,IAAIC,SAA0B;IAC9B,IAAIC,SAA0B;IAC9B,IAAIC,IAAqB;IACzB,IAAIC,IAAqB;IACzB,IAAIxB,aAAuB;IAC3B,IAAI;MACF,MAAM3E,SAAS,GAAGrD,IAAI,CAACqD,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGrD,IAAI,CAACqD,SAAS;MAC9D9H,cAAc,CAAC8H,SAAS,CAAC;MAEzB;MACA;MACA,MAAM3G,cAAc,GAAG,KAAK;MAC5B,MAAM4G,gBAAgB,GAClB,MAAM,IAAI,CAACoD,mBAAmB,CAC1BzK,CAAC,EAAE6C,CAAC,EAAEkB,IAAI,CAAC2G,YAAY,EAAE3G,IAAI,CAAC4G,WAAW,EAAElK,cAAc,EACzD2G,SAAS,CAAmC;MACpDzF,MAAM,GAAG0F,gBAAgB,CAAC,CAAC,CAAC;MAC5BzF,OAAO,GAAGyF,gBAAgB,CAAC,CAAC,CAAC;MAC7B0E,aAAa,GAAG1E,gBAAgB,CAAC,CAAC,CAAC;MAEnC;MACA,IAAImG,YAAY,GAAG,KAAK;MACxB,IAAIC,MAAgB;MACpB,IAAI1J,IAAI,CAAC2J,cAAc,IAAI,IAAI,IAAI3J,IAAI,CAAC2J,cAAc,CAAC/M,MAAM,GAAG,CAAC,EAAE;QACjE6M,YAAY,GAAG,IAAI;QACnB,IAAIzJ,IAAI,CAAC2J,cAAc,CAAC/M,MAAM,KAAK,CAAC,EAAE;UACpC;UACAyM,SAAS,GAAGrJ,IAAI,CAAC2J,cAAc,CAAC,CAAC,CAAC;UAClCL,SAAS,GAAGtJ,IAAI,CAAC2J,cAAc,CAAC,CAAC,CAAC;SACnC,MAAM,IAAI3J,IAAI,CAAC2J,cAAc,CAAC/M,MAAM,KAAK,CAAC,EAAE;UAC3C,MAAM,IAAI7C,mBAAmB,CACzB,+DAA+D,CAAC;SACrE,MAAM;UACL,MAAM,IAAIE,UAAU,CAChB,+DAA+D,GAC/D,4CAA4C,GAC5C,GAAG+F,IAAI,CAAC2J,cAAc,cAAc,CAAC;;QAG3C,MAAMjN,cAAc,GAAG,IAAI;QAC3B,MAAMkN,eAAe,GACjB,MAAM,IAAI,CAAClD,mBAAmB,CAC1B2C,SAAS,EAAEC,SAAS,EAAE,IAAI,EAAE,6BAC5B,IAAI,EAAwB,4BAC5B5M,cAAc,EAAE2G,SAAS,CAAmC;QACpEkG,IAAI,GAAGK,eAAe,CAAC,CAAC,CAAC;QACzBJ,IAAI,GAAGI,eAAe,CAAC,CAAC,CAAC;QACzBF,MAAM,GAAGH,IAAI,CAAC9F,MAAM,CAAC+F,IAAI,CAAC;QAC1B;OACD,MAAM,IACHxJ,IAAI,CAAC6J,eAAe,IAAI,IAAI,IAAI7J,IAAI,CAAC6J,eAAe,GAAG,CAAC,IACxD7J,IAAI,CAAC6J,eAAe,GAAG,CAAC,EAAE;QAC5BJ,YAAY,GAAG,IAAI;QACnB;QACA,MAAMK,OAAO,GACTC,IAAI,CAACC,KAAK,CAACpM,MAAM,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC,GAAG4C,IAAI,CAAC6J,eAAe,CAAC,CAAC;QAC/D,MAAMI,iBAAiB,GAAGrM,MAAM,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC;QAC5CmM,IAAI,GAAG5N,WAAW,CAACiC,MAAM,EAAEkM,OAAO,EAAEG,iBAAiB,CAAa;QAClEd,cAAc,GAAGvL,MAAM;QACvBA,MAAM,GAAGjC,WAAW,CAACiC,MAAM,EAAE,CAAC,EAAEkM,OAAO,CAAa;QACpDN,IAAI,GAAG7N,WAAW,CAACkC,OAAO,EAAEiM,OAAO,EAAEG,iBAAiB,CAAa;QACnEb,eAAe,GAAGvL,OAAO;QACzBA,OAAO,GAAGlC,WAAW,CAACkC,OAAO,EAAE,CAAC,EAAEiM,OAAO,CAAa;QACtD;QACA;QACAJ,MAAM,GAAGH,IAAI,CAAC9F,MAAM,CAAC+F,IAAI,CAAC;QAE1B;OACD,MAAM,IAAIxJ,IAAI,CAACkK,eAAe,IAAI,IAAI,EAAE;QACvCT,YAAY,GAAG,IAAI;QACnB;;MAGF,MAAMjG,GAAG,GAAG5F,MAAM,CAAC6F,MAAM,CAAC5F,OAAO,CAAC,CAAC4F,MAAM,CAACuE,aAAa,CAAC;MAExD,IAAI,CAAC7E,gCAAgC,EAAE;MAEvC;MAEA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA,MAAMgH,aAAa,GAAG,IAAI,CAACrC,iBAAiB,EAAE;MAC9C,MAAML,SAAS,GAAG,IAAI,CAACD,sBAAsB,EAAE;MAE/C,IAAI4C,WAAyC;MAC7C,IAAIC,eAAyB;MAC7B,IAAIZ,YAAY,EAAE;QAChB,IAAI,CAAC/F,gBAAgB,EAAE;QACvB0G,WAAW,GAAG,IAAI,CAACxG,YAAY;QAC/ByG,eAAe,GACX5C,SAAS,CAAC/J,KAAK,EAAE,CAAC+F,MAAM,CAACgE,SAAS,CAACzK,GAAG,CAACsN,CAAC,IAAI,MAAM,GAAGA,CAAC,CAAC,CAAC;OAC7D,MAAM;QACLF,WAAW,GAAG,IAAI;QAClBV,MAAM,GAAG,EAAE;QACXW,eAAe,GAAG5C,SAAS,CAAC/J,KAAK,EAAE;;MAGrC,MAAM6M,SAAS,GAAG1Q,oBAAoB,CAACmG,IAAI,CAACuK,SAAS,EAAEvK,IAAI,CAACwK,UAAU,CAAC;MACvE,MAAMC,GAAG,GAAG,MAAM,IAAI,CAACC,OAAO,CAC1BP,aAAa,EAAE3G,GAAG,EAAEiE,SAAS,EAAEpE,SAAS,EAAErD,IAAI,CAAC2K,MAAM,EACrD3K,IAAI,CAAC+D,OAAO,EAAEwG,SAAS,EAAEH,WAAW,EAAEV,MAAM,EAAE1J,IAAI,CAAC4K,OAAO,EAC1DP,eAAe,EAAErK,IAAI,CAAC6K,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC;MACnD,OAAOJ,GAAG;KACX,SAAS;MACR,IAAI,CAACxK,UAAU,GAAG,KAAK;MACvB;MACAzE,iBAAiB,CAACoC,MAAM,EAAE3B,CAAC,CAAC;MAC5BT,iBAAiB,CAACqC,OAAO,EAAEiB,CAAC,CAAC;MAC7BtD,iBAAiB,CAAC2N,cAAc,EAAElN,CAAC,CAAC;MACpCT,iBAAiB,CAAC4N,eAAe,EAAEtK,CAAC,CAAC;MACrCtD,iBAAiB,CAAC+N,IAAgB,EAAEF,SAAS,CAAC;MAC9C7N,iBAAiB,CAACgO,IAAgB,EAAEF,SAAS,CAAC;MAC9C,IAAItB,aAAa,IAAI,IAAI,EAAE;QACzB7O,GAAG,CAAC2R,OAAO,CAAC9C,aAAa,CAAC;;;IAG9B;EACF;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;EA2BA,MAAM0C,OAAOA,CACT/G,CAA+B,EAAEH,GAAa,EAAEiE,SACxC,EAAEpE,SAAkB,EAAEsH,MAAe,EAAE5G,OAAgB,EAC/DwG,SAA0B,EAAEQ,IAAmC,EAAErB,MACzD,EAAEkB,OAAwB,EAAEP,eAA0B,EAC9DQ,YAAqB,EAAEG,aAAsB,EAAEd,eAAwB;IAEzE,IAAI7G,SAAS,IAAI,IAAI,EAAE;MACrBA,SAAS,GAAG,EAAE;;IAEhB,IAAIsH,MAAM,IAAI,IAAI,EAAE;MAClBA,MAAM,GAAG,CAAC;;IAEZ,IAAIC,OAAO,IAAI,IAAI,EAAE;MACnBA,OAAO,GAAG,IAAI;;IAEhB,IAAIC,YAAY,IAAI,IAAI,EAAE;MACxBA,YAAY,GAAG,CAAC;;IAGlB;IACA,IAAIpB,YAAY,GAAG,KAAK;IACxB,IAAIsB,IAAI,IAAI,IAAI,IAAIrB,MAAM,IAAI,IAAI,EAAE;MAClCD,YAAY,GAAG,IAAI;MACnB;;IAEF,IAAIS,eAAe,IAAI,IAAI,EAAE;MAC3BT,YAAY,GAAG,IAAI;MACnB,IAAIuB,aAAa,IAAI,IAAI,EAAE;QACzB,MAAM,IAAI/Q,UAAU,CAChB,gEAAgE,GAChE,oCAAoC,CAAC;;;IAI7C,MAAMgR,eAAe,GACjB,IAAI,CAAC/G,eAAe,CAACV,GAAG,EAAEH,SAAS,EAAE2H,aAAa,EAAE,iBAAiB,CAAC;IAC1E,IAAI7D,UAAoB;IACxB,IAAI8D,eAAe,IAAI,IAAI,EAAE;MAC3B9D,UAAU,GAAGpM,KAAK,CAAC,CAAC,EAAEkQ,eAAe,CAAC;;IAGxC,IAAIlH,OAAO,IAAI,IAAI,EAAE;MACnBA,OAAO,GAAG,CAAC;;IAGb,MAAM;MAACmH,YAAY;MAAEC;IAAO,CAAC,GAAGvR,kBAAkB,CAC9C2Q,SAAS,EAAExG,OAAO,EAAE4G,MAAM,EAAEE,YAAY,EAAEI,eAAe,EACzDD,aAAa,EAAE3H,SAAS,EAAEoG,YAAY,EAAEY,eAAe,CAAC;IAC5Da,YAAY,CAACE,QAAQ,CAAC,IAAI,CAAC;IAC3B,IAAI,CAACD,OAAO,GAAGA,OAAO;IACtB,MAAMD,YAAY,CAACG,YAAY,EAAE;IACjC,IAAI,CAACC,aAAa,GAAG,KAAK;IAC1B;IACA;IAEA,KAAK,IAAIC,KAAK,GAAGV,YAAY,EAAEU,KAAK,GAAGZ,MAAM,EAAE,EAAEY,KAAK,EAAE;MACtD,MAAML,YAAY,CAACM,YAAY,CAACD,KAAK,CAAC;MACtC,MAAME,SAAS,GAAmB,EAAE;MACpC,IAAIT,aAAa,IAAI,IAAI,EAAE;QACzB,MAAM,IAAIjR,mBAAmB,CACzB,4CAA4C,CAAC;OAClD,MAAM;QACL,IAAI6Q,OAAO,KAAK,OAAO,EAAE;UACvB,MAAM,IAAI7Q,mBAAmB,CAAC,oCAAoC,GAClC,MAAM,CAAC;SACxC,MAAM,IAAI6Q,OAAO,EAAE;UAClBlR,IAAI,CAACkR,OAAO,CAACzD,UAAU,CAAC;;QAE1B;QACA;QACA,MAAMuE,iBAAiB,GAAGjS,QAAQ,CAAC0N,UAAU,CAAC;QAE9C,MAAM3B,OAAO,GAAG9J,WAAW,CAACuP,eAAe,EAAE5H,SAAS,CAAC;QACvD,KAAK,IAAIqC,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAGF,OAAO,CAAC5I,MAAM,EAAE,EAAE8I,UAAU,EAAE;UAClE,MAAMiG,SAAS,GAAmB,EAAE;UACpC,MAAMT,YAAY,CAACU,YAAY,CAAClG,UAAU,EAAEiG,SAAS,CAAC;UAEtDxS,GAAG,CAACoM,IAAI,CAAC,MAAK;YACZ,MAAMK,UAAU,GAAGJ,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;YACzC,MAAMG,QAAQ,GAAGL,OAAO,CAACE,UAAU,CAAC,CAAC,CAAC,CAAC;YACvC,MAAM0B,QAAQ,GAAGzN,CAAC,CAAC0N,mBAAmB,CACjBqE,iBAAiB,EAAE9F,UAAU,EAC7BC,QAAQ,GAAGD,UAAU,CAAa;YACvD+F,SAAS,CAAC,OAAO,CAAC,GAAGjG,UAAU;YAC/BiG,SAAS,CAAC,MAAM,CAAC,GAAG9F,QAAQ,GAAGD,UAAU;YAEzC;YACA;YACA,MAAME,QAAQ,GAAGlK,oBAAoB,CAAC4H,GAAG,EAAE4D,QAAQ,CAAa;YAChE,MAAMF,IAAI,GAAGvD,CAAC,CAACmC,QAAQ,CAAC;YACxB,KAAK,IAAIzI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoK,SAAS,CAAC7K,MAAM,EAAE,EAAES,CAAC,EAAE;cACzC,MAAMsK,KAAK,GAAGF,SAAS,CAACpK,CAAC,CAAC;cAC1B,MAAMoN,GAAG,GAAGvD,IAAI,CAAC7J,CAAC,CAAC;cACnBsO,SAAS,CAAChE,KAAK,CAAC,GAAG8C,GAAG;cACtBtR,GAAG,CAACoP,IAAI,CAACkC,GAAG,CAAC;cACb;;YAGF,IAAI/E,UAAU,KAAKF,OAAO,CAAC5I,MAAM,GAAG,CAAC,EAAE;cAAG;cACxC,IAAI6M,YAAY,EAAE;gBAChB,MAAMoC,OAAO,GAAG,IAAI,CAAC/H,QAAQ,CAACiH,IAAI,EAAErB,MAAM,EAAErG,SAAS,CAAC;gBACtD;gBACA,KAAK,IAAIhG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGoK,SAAS,CAAC7K,MAAM,EAAE,EAAES,CAAC,EAAE;kBACzC,MAAMsK,KAAK,GAAGF,SAAS,CAACpK,CAAC,CAAC;kBAC1B,MAAMoN,GAAG,GAAGoB,OAAO,CAACxO,CAAC,CAAC;kBACtBlE,GAAG,CAACoP,IAAI,CAACkC,GAAG,CAAC;kBACb;kBACAgB,SAAS,CAAC,MAAM,GAAG9D,KAAK,CAAC,GAAG8C,GAAG;;;;UAIvC,CAAC,CAAC;UAEF,MAAMS,YAAY,CAACY,UAAU,CAACpG,UAAU,EAAEiG,SAAS,CAAC;UACpDxR,oBAAoB,CAACwR,SAAS,CAAC;UAE/B,IAAI,IAAI,CAACL,aAAa,EAAE;YACtB;;UAEF;;QAGFI,iBAAiB,CAACZ,OAAO,EAAE;;MAE7B;MACA,MAAMI,YAAY,CAACa,UAAU,CAACR,KAAK,EAAEE,SAAS,CAAC;MAC/C,IAAI,IAAI,CAACH,aAAa,EAAE;QACtB;;;IAGJ,MAAMJ,YAAY,CAACc,UAAU,EAAE;IAE/B,MAAM,IAAI,CAACb,OAAO,CAACc,QAAQ,EAAE;IAC7B,OAAO,IAAI,CAACd,OAAO;EACrB;EAEA;EACA;EACA;;;;;;;;;;;;;;;;;;;;;EAqBA,MAAM7P,UAAUA,CAAI2I,OAAmB,EAAEjE,IAA4B;IAEnE,OAAO1E,UAAU,CAAC,IAAI,EAAE2I,OAAO,EAAEjE,IAAI,CAAC;EACxC;EAEA;;;;;;;;;;;;;;;;;;;;;;;EAuBA,MAAMkM,YAAYA,CACdjQ,CAAgD,EAChD6C,CAC6B;IAC/B;IACA;IACA,MAAMqN,cAAc,GAAG,MAAM,IAAI,CAACzF,mBAAmB,CAACzK,CAAC,EAAE6C,CAAC,CAAC;IAC3D,MAAMlB,MAAM,GAAGuO,cAAc,CAAC,CAAC,CAAC;IAChC,MAAMtO,OAAO,GAAGsO,cAAc,CAAC,CAAC,CAAC;IACjC,MAAMhC,aAAa,GAAG,IAAI,CAACrC,iBAAiB,EAAE;IAC9C,MAAM1N,MAAM,GAAG+P,aAAa,CAACvM,MAAM,CAAC6F,MAAM,CAAC5F,OAAO,CAAC,CAAC;IACpD,MAAMkK,UAAU,GAAa,EAAE;IAC/B,KAAK,MAAMhJ,IAAI,IAAI3E,MAAM,EAAE;MACzB,MAAMgS,CAAC,GAAG,MAAMrN,IAAI,CAACxC,IAAI,EAAE;MAC3BwL,UAAU,CAAC5K,IAAI,CAACiP,CAAC,CAAC,CAAC,CAAC,CAAC;;IAEvBjT,GAAG,CAAC2R,OAAO,CAAC1Q,MAAM,CAAC;IACnBoB,iBAAiB,CAAC2Q,cAAc,CAAC,CAAC,CAAC,EAAElQ,CAAC,CAAC;IACvCT,iBAAiB,CAAC2Q,cAAc,CAAC,CAAC,CAAC,EAAErN,CAAC,CAAC;IACvC,OAAOpE,gBAAgB,CAACqN,UAAU,CAAC;EACrC;EAEA;;;;;;;;;EASUsE,eAAeA,CAACC,MAAsB;IAC9C,MAAMC,YAAY,GAAkB,EAAE;IAEtC,MAAMC,aAAa,GAAGF,MAAM,IAAI,IAAI,IAAIA,MAAM,CAACE,aAAa;IAC5D,MAAM1O,OAAO,GAAG0O,aAAa,GAAG,IAAI,CAACtJ,gBAAgB,GAAG,IAAI,CAACpF,OAAO;IACpE,MAAM2O,YAAY,GAAG,IAAI,CAACC,UAAU,CAACF,aAAa,CAAC;IACnD,KAAK,IAAInP,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGS,OAAO,CAAClB,MAAM,EAAE,EAAES,CAAC,EAAE;MACvC,IAAImP,aAAa,IAAI,CAAC1O,OAAO,CAACT,CAAC,CAAC,CAACsP,SAAS,EAAE;QAC1C;QACA;;MAEFJ,YAAY,CAACpP,IAAI,CACb;QAACF,IAAI,EAAEa,OAAO,CAACT,CAAC,CAAC,CAACuP,YAAY;QAAEvH,MAAM,EAAEoH,YAAY,CAACpP,CAAC;MAAC,CAAC,CAAC;;IAE/D,OAAOkP,YAAY;EACrB;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA8BA,IAAIM,YAAYA,CAACC,IAAa;IAC5B,IAAI,CAACxB,aAAa,GAAGwB,IAAI;EAC3B;EAEA,IAAID,YAAYA,CAAA;IACd,OAAO,IAAI,CAACvB,aAAa;EAC3B;EAEA,IAAI5K,SAASA,CAAA;IACX,OAAO,IAAI,CAACC,UAAU;EACxB;EAEA,IAAID,SAASA,CAACA,SAAoB;IAChC,IAAI,IAAI,CAACC,UAAU,KAAKD,SAAS,EAAE;MACjC,IAAI,CAACC,UAAU,GAAGD,SAAS;MAC3B,IAAI,CAACG,gBAAgB,GAAG,KAAK;;EAEjC;EAESiK,OAAOA,CAAA;IACd,MAAMiC,MAAM,GAAG,KAAK,CAACjC,OAAO,EAAE;IAC9B,IAAIiC,MAAM,CAACC,oBAAoB,KAAK,CAAC,IAAI,IAAI,CAACtM,SAAS,IAAI,IAAI,IAC3D,IAAI,CAACG,gBAAgB,EAAE;MACzB,MAAMoM,gCAAgC,GAAG9T,GAAG,CAAC+T,MAAM,EAAE,CAACC,UAAU;MAChE,IAAI,CAACxM,UAAU,CAACmK,OAAO,EAAE;MACzBiC,MAAM,CAACK,oBAAoB,IACvBH,gCAAgC,GAAG9T,GAAG,CAAC+T,MAAM,EAAE,CAACC,UAAU;;IAEhE,OAAOJ,MAAM;EACf;EAEQM,kBAAkBA,CAAA;IAExB,IAAIC,SACsC;IAC1C,IAAI,OAAO,IAAI,CAACvO,IAAI,KAAK,QAAQ,EAAE;MACjCuO,SAAS,GAAG1S,WAAW,CAAC,IAAI,CAACmE,IAAI,CAAmB;KACrD,MAAM,IAAI5C,KAAK,CAACC,OAAO,CAAC,IAAI,CAAC2C,IAAI,CAAC,EAAE;MACnC,KAAK,MAAMA,IAAI,IAAI,IAAI,CAACA,IAAI,EAAE;QAC5B,IAAI,OAAOA,IAAI,KAAK,QAAQ,EAAE;UAC5B,MAAM,IAAIgI,KAAK,CAAC,oDAAoD,CAAC;;;MAGzEuG,SAAS,GAAI,IAAI,CAACvO,IAAiB,CAAC/B,GAAG,CAACC,IAAI,IAAIrC,WAAW,CAACqC,IAAI,CAAC,CAC7C;KACrB,MAAM;MACL,MAAMuC,WAAW,GAAG+N,MAAM,CAACC,IAAI,CAAC,IAAI,CAACzO,IAAI,CAAC;MAC1CuO,SAAS,GAAG,EAA4C;MACxD,MAAMlT,MAAM,GACR,IAAI,CAAC2E,IAAuD;MAChE,KAAK,MAAM0O,UAAU,IAAIjO,WAAW,EAAE;QACpC,IAAI,OAAOpF,MAAM,CAACqT,UAAU,CAAC,KAAK,QAAQ,EAAE;UAC1CH,SAAS,CAACG,UAAU,CAAC,GACjB7S,WAAW,CAACR,MAAM,CAACqT,UAAU,CAAW,CAAmB;SAChE,MAAM;UACL,MAAM,IAAI1G,KAAK,CAAC,oDAAoD,CAAC;;;;IAI3E,OAAOuG,SAAS;EAClB;EAEQI,oBAAoBA,CAAA;IAE1B,IAAI,OAAO,IAAI,CAACnO,OAAO,KAAK,QAAQ,IAChC,OAAO,IAAI,CAACA,OAAO,KAAK,UAAU,EAAE;MACtC,OAAO,CAAC3E,WAAW,CAACP,OAAO,CAAC0I,mBAAmB,CAAC,IAAI,CAACxD,OAAO,CAAC,CAAC,CAAC;KAChE,MAAM,IAAIpD,KAAK,CAACC,OAAO,CAAC,IAAI,CAACmD,OAAO,CAAC,EAAE;MACtC,OAAO,IAAI,CAACA,OAAO,CAACvC,GAAG,CACnBuF,MAAM,IAAI3H,WAAW,CAACP,OAAO,CAAC0I,mBAAmB,CAACR,MAAM,CAAC,CAAC,CAAC;KAChE,MAAM;MACL,MAAMoL,kBAAkB,GAAuC,EAAE;MACjE,KAAK,MAAM7Q,GAAG,IAAI,IAAI,CAACyC,OAAO,EAAE;QAC9BoO,kBAAkB,CAAC7Q,GAAG,CAAC,GACnBlC,WAAW,CAACP,OAAO,CAAC0I,mBAAmB,CAAC,IAAI,CAACxD,OAAO,CAACzC,GAAG,CAAC,CAAC,CAAC;;MAEjE,OAAO6Q,kBAAkB;;EAE7B;EAEUC,iBAAiBA,CAAA;IACzB,OAAO;MACL7O,IAAI,EAAE,IAAI,CAACsO,kBAAkB,EAAE;MAC/B9N,OAAO,EAAE,IAAI,CAACmO,oBAAoB,EAAE;MACpCG,gBAAgB,EAAE;QAChBC,UAAU,EAAE,IAAI,CAACpN,SAAS,CAACqN,YAAY,EAAE;QACzCzB,MAAM,EAAE,IAAI,CAAC5L,SAAS,CAACsN,SAAS;;KAEnC;IACD;IACA;IACA;EACF;EAEAC,kBAAkBA,CAACC,cAA8B;IAC/C,IAAIA,cAAc,CAACC,gBAAgB,IAAI,IAAI,EAAE;MAC3C,MAAM,IAAIpH,KAAK,CAAC,8CAA8C,CAAC;;IAEjE,IAAImH,cAAc,CAACE,YAAY,IAAI,IAAI,EAAE;MACvC,MAAM,IAAIrH,KAAK,CAAC,4CAA4C,CAAC;;IAE/D,IAAImH,cAAc,CAACG,kBAAkB,IAAI,IAAI,EAAE;MAC7C,MAAM,IAAItH,KAAK,CAAC,kDAAkD,CAAC;;IAGrE,MAAMuH,QAAQ,GAAGtT,mBAAmB,CAACkT,cAAc,CAACL,gBAAgB,CACxC;IAC5B,MAAMnN,SAAS,GAAGxG,WAAW,CAACoU,QAAQ,CAAc;IAEpD,IAAIvP,IAAI;IACR,IAAI,OAAOmP,cAAc,CAACnP,IAAI,KAAK,QAAQ,EAAE;MAC3CA,IAAI,GAAGpE,WAAW,CAACuT,cAAc,CAACnP,IAAI,CAAC;KACxC,MAAM,IAAI5C,KAAK,CAACC,OAAO,CAAC8R,cAAc,CAACnP,IAAI,CAAC,EAAE;MAC7CA,IAAI,GAAGmP,cAAc,CAACnP,IAAI,CAAC/B,GAAG,CAACuR,SAAS,IAAI5T,WAAW,CAAC4T,SAAS,CAAC,CAAC;KACpE,MAAM,IAAIL,cAAc,CAACnP,IAAI,IAAI,IAAI,EAAE;MACtCA,IAAI,GAAG,EAA4C;MACnD,KAAK,MAAMjC,GAAG,IAAIoR,cAAc,CAACnP,IAAI,EAAE;QACrCA,IAAI,CAACjC,GAAG,CAAC,GAAGnC,WAAW,CAACuT,cAAc,CAACnP,IAAI,CAACjC,GAAG,CAAC,CAAmB;;;IAIvE,IAAIyC,OAAO;IACX,IAAIpD,KAAK,CAACC,OAAO,CAAC8R,cAAc,CAAC3O,OAAO,CAAC,EAAE;MACzCA,OAAO,GAAG2O,cAAc,CAAC3O,OAAO,CAACvC,GAAG,CAACuF,MAAM,IAAI5H,WAAW,CAAC4H,MAAM,CAAC,CAAC;KACpE,MAAM,IAAI2L,cAAc,CAAC3O,OAAO,IAAI,IAAI,EAAE;MACzCA,OAAO,GAAG,EAA+C;MACzD,KAAK,MAAMzC,GAAG,IAAIoR,cAAc,CAAC3O,OAAO,EAAE;QACxCA,OAAO,CAACzC,GAAG,CAAC,GAAGnC,WAAW,CAACuT,cAAc,CAAC3O,OAAO,CAACzC,GAAG,CAAC,CAAC;;;IAI3D,IAAI,CAAC2D,OAAO,CAAC;MAAC1B,IAAI;MAAEQ,OAAO;MAAEmB;IAAS,CAAC,CAAC;EAC1C;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAiFA,MAAM8N,IAAIA,CAACC,YAAiC,EAAEnC,MAAsB;IAElE,IAAI,OAAOmC,YAAY,KAAK,QAAQ,EAAE;MACpC,MAAMC,QAAQ,GAAGtV,EAAE,CAACuV,eAAe,CAACF,YAAY,CAAC;MACjD,IAAIC,QAAQ,CAAC9R,MAAM,KAAK,CAAC,EAAE;QACzB,MAAM,IAAI3C,UAAU,CAChB,0CAA0CwU,YAAY,GAAG,CAAC;OAC/D,MAAM,IAAIC,QAAQ,CAAC9R,MAAM,GAAG,CAAC,EAAE;QAC9B,MAAM,IAAI3C,UAAU,CAChB,wBAAwByU,QAAQ,CAAC9R,MAAM,sBAAsB,GAC7D,QAAQ6R,YAAY,GAAG,CAAC;;MAE9BA,YAAY,GAAGC,QAAQ,CAAC,CAAC,CAAC;;IAE5B,IAAID,YAAY,CAACD,IAAI,IAAI,IAAI,EAAE;MAC7B,MAAM,IAAIvU,UAAU,CAChB,0DAA0D,GAC1D,sDAAsD,CAAC;;IAG7D,MAAM2U,kBAAkB,GACpB,MAAMxV,EAAE,CAACyV,aAAa,CAAC,IAAI,CAACxC,eAAe,CAACC,MAAM,CAAC,CAAC;IAExD,MAAMwC,YAAY,GAAG,KAAK;IAC1B,MAAMC,SAAS,GAAO,IAAI;IAC1B,MAAMC,WAAW,GAAG,IAAI,CAACC,MAAM,CAACF,SAAS,EAAED,YAAY,CAAC;IACxD,MAAMI,cAAc,GAAsB;MACxCC,aAAa,EAAEH,WAAW;MAC1BI,MAAM,EAAEvP,wBAAwB;MAChCwP,WAAW,EAAE,8BAA8BpU,OAAO,EAAE;MACpDqU,WAAW,EAAE;KACd;IAED,MAAMC,gBAAgB,GAAGjD,MAAM,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAACiD,gBAAgB;IACzE,IAAIA,gBAAgB,IAAI,IAAI,CAAC7O,SAAS,IAAI,IAAI,EAAE;MAC9CwO,cAAc,CAAChB,cAAc,GAAG,IAAI,CAACN,iBAAiB,EAAE;MACxD,MAAM4B,UAAU,GAAG,WAAW;MAC9B,MAAM;QAACjT,IAAI,EAAEkT,mBAAmB;QAAEC,KAAK,EAAEC;MAAoB,CAAC,GAC1D,MAAMvW,EAAE,CAACyV,aAAa,CAAC,MAAM,IAAI,CAACnO,SAAS,CAACgM,UAAU,EAAE,EAAE8C,UAAU,CAAC;MACzEZ,kBAAkB,CAACc,KAAK,CAACvS,IAAI,CAAC,GAAGwS,oBAAoB,CAAC;MACtDf,kBAAkB,CAACrS,IAAI,GAAGnD,EAAE,CAACwW,uBAAuB,CAChD,CAAChB,kBAAkB,CAACrS,IAAI,EAAEkT,mBAAmB,CAAC,CAAC;;IAGrD,IAAI,IAAI,CAACI,mBAAmB,IAAI,IAAI,EAAE;MACpC;MACA,MAAMC,SAAS,GAAG,IAAI;MACtBvV,wBAAwB,CAAC,IAAI,CAACsV,mBAAmB,EAAE,IAAI,CAAC5S,IAAI,EAAE6S,SAAS,CAAC;MACxEZ,cAAc,CAACW,mBAAmB,GAAG,IAAI,CAACA,mBAAmB;;IAG/DX,cAAc,CAACa,UAAU,GAAGnB,kBAAkB,CAACrS,IAAI;IACnD2S,cAAc,CAACc,WAAW,GAAGpB,kBAAkB,CAACc,KAAK;IACrD,OAAOjB,YAAY,CAACD,IAAI,CAACU,cAAc,CAAC;EAC1C;EAEA;;;;;;;;EAQAe,sBAAsBA,CAACJ,mBAAuB;IAC5CtV,wBAAwB,CAACsV,mBAAmB,EAAE,IAAI,CAAC5S,IAAI,CAAC;IACxD,IAAI,CAAC4S,mBAAmB,GAAGA,mBAAmB;EAChD;EAEA;;;;;;;;;;;EAWAK,sBAAsBA,CAAA;IACpB,OAAO,IAAI,CAACL,mBAAmB;EACjC;;AAtrDA;AACA;AACA;AACO/P,WAAA,CAAAqQ,SAAS,GAAG,OAAO;SAJfrQ,WAAW;AAyrDxBvG,aAAa,CAAC6W,aAAa,CAACtQ,WAAW,CAAC;AAExC;;;;;;AAMA;AACA,MAAauQ,UAAW,SAAQvQ,WAAW;AACzBuQ,UAAA,CAAAF,SAAS,GAAG,YAAY;SAD7BE,UAAU;AAGvB9W,aAAa,CAAC6W,aAAa,CAACC,UAAU,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}