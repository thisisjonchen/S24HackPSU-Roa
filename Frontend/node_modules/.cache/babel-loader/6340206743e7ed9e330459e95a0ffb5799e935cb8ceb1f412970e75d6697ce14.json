{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Basic Layers.\n */\nimport { any, cast, mul, notEqual, reshape, serialization, tidy, transpose, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger, mapActivationToFusedKernel } from '../utils/generic_utils';\nimport { arrayProd, range } from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nclass Dropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.rate = Math.max(Math.min(args.rate, 1), 0);\n    // So that the scalar doesn't get tidied up between executions.\n    this.noiseShape = args.noiseShape;\n    this.seed = args.seed;\n    this.supportsMasking = true;\n  }\n  getNoiseShape(input) {\n    if (this.noiseShape == null) {\n      return this.noiseShape;\n    }\n    const inputShape = input.shape;\n    const noiseShape = [];\n    for (let i = 0; i < this.noiseShape.length; ++i) {\n      noiseShape.push(this.noiseShape[i] == null ? inputShape[i] : this.noiseShape[i]);\n    }\n    return noiseShape;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (0 < this.rate && this.rate < 1) {\n        const training = kwargs['training'] == null ? false : kwargs['training'];\n        const noiseShape = this.getNoiseShape(input);\n        const output = K.inTrainPhase(() => K.dropout(input, this.rate, noiseShape, this.seed), () => input, training);\n        return output;\n      }\n      return inputs;\n    });\n  }\n  getConfig() {\n    const config = {\n      rate: this.rate,\n      noiseShape: this.noiseShape,\n      seed: this.seed\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  dispose() {\n    return super.dispose();\n  }\n}\n/** @nocollapse */\nDropout.className = 'Dropout';\nexport { Dropout };\nserialization.registerClass(Dropout);\nclass SpatialDropout1D extends Dropout {\n  constructor(args) {\n    super(args);\n    this.inputSpec = [{\n      ndim: 3\n    }];\n  }\n  getNoiseShape(input) {\n    const inputShape = input.shape;\n    return [inputShape[0], 1, inputShape[2]];\n  }\n}\n/** @nocollapse */\nSpatialDropout1D.className = 'SpatialDropout1D';\nexport { SpatialDropout1D };\nserialization.registerClass(SpatialDropout1D);\nclass Dense extends Layer {\n  constructor(args) {\n    super(args);\n    // Default activation: Linear (none).\n    this.activation = null;\n    this.useBias = true;\n    this.kernel = null;\n    this.bias = null;\n    this.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    this.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    if (args.batchInputShape == null && args.inputShape == null && args.inputDim != null) {\n      // This logic is copied from Layer's constructor, since we can't\n      // do exactly what the Python constructor does for Dense().\n      let batchSize = null;\n      if (args.batchSize != null) {\n        batchSize = args.batchSize;\n      }\n      this.batchInputShape = [batchSize, args.inputDim];\n    }\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation);\n    if (args.useBias != null) {\n      this.useBias = args.useBias;\n    }\n    this.kernelInitializer = getInitializer(args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.biasInitializer = getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.activityRegularizer = getRegularizer(args.activityRegularizer);\n    this.supportsMasking = true;\n    this.inputSpec = [{\n      minNDim: 2\n    }];\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputLastDim = inputShape[inputShape.length - 1];\n    if (this.kernel == null) {\n      this.kernel = this.addWeight('kernel', [inputLastDim, this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      if (this.useBias) {\n        this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      }\n    }\n    this.inputSpec = [{\n      minNDim: 2,\n      axes: {\n        [-1]: inputLastDim\n      }\n    }];\n    this.built = true;\n  }\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const outputShape = inputShape.slice();\n    outputShape[outputShape.length - 1] = this.units;\n    return outputShape;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      // Dense layer accepts only a single input.\n      const input = getExactlyOneTensor(inputs);\n      const fusedActivationName = mapActivationToFusedKernel(this.activation.getClassName());\n      let output;\n      if (fusedActivationName != null) {\n        output = K.dot(input, this.kernel.read(), fusedActivationName, this.bias ? this.bias.read() : null);\n      } else {\n        output = K.dot(input, this.kernel.read());\n        if (this.bias != null) {\n          output = K.biasAdd(output, this.bias.read());\n        }\n        if (this.activation != null) {\n          output = this.activation.apply(output);\n        }\n      }\n      return output;\n    });\n  }\n  getConfig() {\n    const config = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nDense.className = 'Dense';\nexport { Dense };\nserialization.registerClass(Dense);\nclass Flatten extends Layer {\n  constructor(args) {\n    args = args || {};\n    super(args);\n    this.inputSpec = [{\n      minNDim: 3\n    }];\n    this.dataFormat = args.dataFormat;\n  }\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    for (const dim of inputShape.slice(1)) {\n      if (dim == null) {\n        throw new ValueError(`The shape of the input to \"Flatten\" is not fully defined ` + `(got ${inputShape.slice(1)}). Make sure to pass a complete ` + `\"input_shape\" or \"batch_input_shape\" argument to the first ` + `layer in your model.`);\n      }\n    }\n    return [inputShape[0], arrayProd(inputShape, 1)];\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      let input = getExactlyOneTensor(inputs);\n      if (this.dataFormat === 'channelsFirst' && input.rank > 1) {\n        const permutation = [0];\n        for (let i = 2; i < input.rank; ++i) {\n          permutation.push(i);\n        }\n        permutation.push(1);\n        input = transpose(input, permutation);\n      }\n      return K.batchFlatten(input);\n    });\n  }\n  getConfig() {\n    const config = {};\n    if (this.dataFormat != null) {\n      config['dataFormat'] = this.dataFormat;\n    }\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nFlatten.className = 'Flatten';\nexport { Flatten };\nserialization.registerClass(Flatten);\nclass Activation extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.activation = getActivation(args.activation);\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      return this.activation.apply(input);\n    });\n  }\n  getConfig() {\n    const config = {\n      activation: serializeActivation(this.activation)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nActivation.className = 'Activation';\nexport { Activation };\nserialization.registerClass(Activation);\nclass RepeatVector extends Layer {\n  constructor(args) {\n    super(args);\n    this.n = args.n;\n    this.inputSpec = [{\n      ndim: 2\n    }];\n  }\n  computeOutputShape(inputShape) {\n    return [inputShape[0], this.n, inputShape[1]];\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      inputs = getExactlyOneTensor(inputs);\n      return K.repeat(inputs, this.n);\n    });\n  }\n  getConfig() {\n    const config = {\n      n: this.n\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nRepeatVector.className = 'RepeatVector';\nexport { RepeatVector };\nserialization.registerClass(RepeatVector);\nclass Reshape extends Layer {\n  constructor(args) {\n    super(args);\n    this.targetShape = args.targetShape;\n    // Make sure that all unknown dimensions are represented as `null`.\n    for (let i = 0; i < this.targetShape.length; ++i) {\n      if (this.isUnknown(this.targetShape[i])) {\n        this.targetShape[i] = null;\n      }\n    }\n  }\n  isUnknown(dim) {\n    return dim < 0 || dim == null;\n  }\n  /**\n   * Finds and replaces a missing dimension in output shape.\n   *\n   * This is a near direct port of the internal Numpy function\n   * `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`.\n   *\n   * @param inputShape: Original shape of array begin reshape.\n   * @param outputShape: Target shape of the array, with at most a single\n   * `null` or negative number, which indicates an underdetermined dimension\n   * that should be derived from `inputShape` and the known dimensions of\n   *   `outputShape`.\n   * @returns: The output shape with `null` replaced with its computed value.\n   * @throws: ValueError: If `inputShape` and `outputShape` do not match.\n   */\n  fixUnknownDimension(inputShape, outputShape) {\n    const errorMsg = 'Total size of new array must be unchanged.';\n    const finalShape = outputShape.slice();\n    let known = 1;\n    let unknown = null;\n    for (let i = 0; i < finalShape.length; ++i) {\n      const dim = finalShape[i];\n      if (this.isUnknown(dim)) {\n        if (unknown === null) {\n          unknown = i;\n        } else {\n          throw new ValueError('Can only specifiy one unknown dimension.');\n        }\n      } else {\n        known *= dim;\n      }\n    }\n    const originalSize = arrayProd(inputShape);\n    if (unknown !== null) {\n      if (known === 0 || originalSize % known !== 0) {\n        throw new ValueError(errorMsg);\n      }\n      finalShape[unknown] = originalSize / known;\n    } else if (originalSize !== known) {\n      throw new ValueError(errorMsg);\n    }\n    return finalShape;\n  }\n  computeOutputShape(inputShape) {\n    let anyUnknownDims = false;\n    for (let i = 0; i < inputShape.length; ++i) {\n      if (this.isUnknown(inputShape[i])) {\n        anyUnknownDims = true;\n        break;\n      }\n    }\n    if (anyUnknownDims) {\n      return inputShape.slice(0, 1).concat(this.targetShape);\n    } else {\n      return inputShape.slice(0, 1).concat(this.fixUnknownDimension(inputShape.slice(1), this.targetShape));\n    }\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const outputShape = inputShape.slice(0, 1).concat(this.fixUnknownDimension(inputShape.slice(1), this.targetShape));\n      return reshape(input, outputShape);\n    });\n  }\n  getConfig() {\n    const config = {\n      targetShape: this.targetShape\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nReshape.className = 'Reshape';\nexport { Reshape };\nserialization.registerClass(Reshape);\nclass Permute extends Layer {\n  constructor(args) {\n    super(args);\n    if (args.dims == null) {\n      throw new Error('Required configuration field `dims` is missing during Permute ' + 'constructor call.');\n    }\n    if (!Array.isArray(args.dims)) {\n      throw new Error('Permute constructor requires `dims` to be an Array, but received ' + `${args.dims} instead.`);\n    }\n    // Check the validity of the permutation indices.\n    const expectedSortedIndices = range(1, args.dims.length + 1);\n    if (!util.arraysEqual(args.dims.slice().sort(), expectedSortedIndices)) {\n      throw new Error('Invalid permutation `dims`: ' + JSON.stringify(args.dims) + ' `dims` must contain consecutive integers starting from 1.');\n    }\n    this.dims = args.dims;\n    this.dimsIncludingBatch = [0].concat(this.dims);\n    this.inputSpec = [new InputSpec({\n      ndim: this.dims.length + 1\n    })];\n  }\n  computeOutputShape(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const outputShape = inputShape.slice();\n    this.dims.forEach((dim, i) => {\n      outputShape[i + 1] = inputShape[dim];\n    });\n    return outputShape;\n  }\n  call(inputs, kwargs) {\n    return transpose(getExactlyOneTensor(inputs), this.dimsIncludingBatch);\n  }\n  getConfig() {\n    const config = {\n      dims: this.dims\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nPermute.className = 'Permute';\nexport { Permute };\nserialization.registerClass(Permute);\nclass Masking extends Layer {\n  constructor(args) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maskValue = args.maskValue == null ? 0 : args.maskValue;\n    } else {\n      this.maskValue = 0;\n    }\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      maskValue: this.maskValue\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  computeMask(inputs, mask) {\n    const input = getExactlyOneTensor(inputs);\n    const axis = -1;\n    return any(notEqual(input, this.maskValue), axis);\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const axis = -1;\n      const keepDims = true;\n      const booleanMask = any(notEqual(input, this.maskValue), axis, keepDims);\n      const output = mul(input, cast(booleanMask, input.dtype));\n      return output;\n    });\n  }\n}\n/** @nocollapse */\nMasking.className = 'Masking';\nexport { Masking };\nserialization.registerClass(Masking);","map":{"version":3,"names":["any","cast","mul","notEqual","reshape","serialization","tidy","transpose","util","getActivation","serializeActivation","K","getConstraint","serializeConstraint","InputSpec","Layer","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","assertPositiveInteger","mapActivationToFusedKernel","arrayProd","range","getExactlyOneShape","getExactlyOneTensor","Dropout","constructor","args","rate","Math","max","min","noiseShape","seed","supportsMasking","getNoiseShape","input","inputShape","shape","i","length","push","call","inputs","kwargs","invokeCallHook","training","output","inTrainPhase","dropout","getConfig","config","baseConfig","Object","assign","dispose","className","registerClass","SpatialDropout1D","inputSpec","ndim","Dense","activation","useBias","kernel","bias","DEFAULT_KERNEL_INITIALIZER","DEFAULT_BIAS_INITIALIZER","batchInputShape","inputDim","batchSize","units","kernelInitializer","biasInitializer","kernelConstraint","biasConstraint","kernelRegularizer","biasRegularizer","activityRegularizer","minNDim","build","inputLastDim","addWeight","axes","built","computeOutputShape","outputShape","slice","fusedActivationName","getClassName","dot","read","biasAdd","apply","Flatten","dataFormat","dim","rank","permutation","batchFlatten","Activation","RepeatVector","n","repeat","Reshape","targetShape","isUnknown","fixUnknownDimension","errorMsg","finalShape","known","unknown","originalSize","anyUnknownDims","concat","Permute","dims","Error","Array","isArray","expectedSortedIndices","arraysEqual","sort","JSON","stringify","dimsIncludingBatch","forEach","Masking","maskValue","computeMask","mask","axis","keepDims","booleanMask","dtype"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-layers/src/layers/core.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Basic Layers.\n */\n\nimport {any, cast, mul, notEqual, reshape, serialization, Tensor, tidy, transpose, util} from '@tensorflow/tfjs-core';\n\nimport {Activation as ActivationFn, getActivation, serializeActivation} from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {DisposeResult, InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {ActivationIdentifier} from '../keras_format/activation_config';\nimport {DataFormat, Shape} from '../keras_format/common';\nimport {LayerConfig} from '../keras_format/topology_config';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport {assertPositiveInteger, mapActivationToFusedKernel} from '../utils/generic_utils';\nimport {arrayProd, range} from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nexport declare interface DropoutLayerArgs extends LayerArgs {\n  /** Float between 0 and 1. Fraction of the input units to drop. */\n  rate: number;\n\n  /**\n   * Integer array representing the shape of the binary dropout mask that will\n   * be multiplied with the input.\n   *\n   * For instance, if your inputs have shape `(batchSize, timesteps, features)`\n   * and you want the dropout mask to be the same for all timesteps, you can use\n   * `noise_shape=(batch_size, 1, features)`.\n   */\n  noiseShape?: number[];\n\n  /** An integer to use as random seed. */\n  seed?: number;\n}\n\nexport class Dropout extends Layer {\n  /** @nocollapse */\n  static className = 'Dropout';\n  private readonly rate: number;\n  private readonly noiseShape: number[];\n  private readonly seed: number;\n\n  constructor(args: DropoutLayerArgs) {\n    super(args);\n    this.rate = Math.max(Math.min(args.rate, 1), 0);\n    // So that the scalar doesn't get tidied up between executions.\n    this.noiseShape = args.noiseShape;\n    this.seed = args.seed;\n    this.supportsMasking = true;\n  }\n\n  protected getNoiseShape(input: Tensor): Shape {\n    if (this.noiseShape == null) {\n      return this.noiseShape;\n    }\n    const inputShape = input.shape;\n    const noiseShape: Shape = [];\n    for (let i = 0; i < this.noiseShape.length; ++i) {\n      noiseShape.push(\n          this.noiseShape[i] == null ? inputShape[i] : this.noiseShape[i]);\n    }\n    return noiseShape;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (0 < this.rate && this.rate < 1) {\n        const training =\n            kwargs['training'] == null ? false : kwargs['training'];\n        const noiseShape = this.getNoiseShape(input);\n        const output = K.inTrainPhase(\n            () => K.dropout(input, this.rate, noiseShape, this.seed),\n            () => input, training);\n        return output;\n      }\n      return inputs;\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config = {\n      rate: this.rate,\n      noiseShape: this.noiseShape,\n      seed: this.seed,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override dispose(): DisposeResult {\n    return super.dispose();\n  }\n}\nserialization.registerClass(Dropout);\n\nexport declare interface DenseLayerArgs extends LayerArgs {\n  /** Positive integer, dimensionality of the output space. */\n  units: number;\n  /**\n   * Activation function to use.\n   *\n   * If unspecified, no activation is applied.\n   */\n  activation?: ActivationIdentifier;\n  /** Whether to apply a bias. */\n  useBias?: boolean;\n  /**\n   * Initializer for the dense kernel weights matrix.\n   */\n  kernelInitializer?: InitializerIdentifier|Initializer;\n  /**\n   * Initializer for the bias vector.\n   */\n  biasInitializer?: InitializerIdentifier|Initializer;\n  /**\n   * If specified, defines inputShape as `[inputDim]`.\n   */\n  inputDim?: number;\n\n  /**\n   * Constraint for the kernel weights.\n   */\n  kernelConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for the bias vector.\n   */\n  biasConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer function applied to the dense kernel weights matrix.\n   */\n  kernelRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the bias vector.\n   */\n  biasRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the activation.\n   */\n  activityRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport interface SpatialDropout1DLayerConfig extends LayerConfig {\n  /** Float between 0 and 1. Fraction of the input units to drop. */\n  rate: number;\n\n  /** An integer to use as random seed. */\n  seed?: number;\n}\n\nexport class SpatialDropout1D extends Dropout {\n  /** @nocollapse */\n  static override className = 'SpatialDropout1D';\n\n  constructor(args: SpatialDropout1DLayerConfig) {\n    super(args);\n    this.inputSpec = [{ndim: 3}];\n  }\n\n  protected override getNoiseShape(input: Tensor): Shape {\n    const inputShape = input.shape;\n    return [inputShape[0], 1, inputShape[2]];\n  }\n}\nserialization.registerClass(SpatialDropout1D);\n\nexport class Dense extends Layer {\n  /** @nocollapse */\n  static className = 'Dense';\n  private units: number;\n  // Default activation: Linear (none).\n  private activation: ActivationFn = null;\n  private useBias = true;\n  private kernelInitializer: Initializer;\n  private biasInitializer: Initializer;\n  private kernel: LayerVariable = null;\n  private bias: LayerVariable = null;\n\n  readonly DEFAULT_KERNEL_INITIALIZER: InitializerIdentifier = 'glorotNormal';\n  readonly DEFAULT_BIAS_INITIALIZER: InitializerIdentifier = 'zeros';\n  private readonly kernelConstraint?: Constraint;\n  private readonly biasConstraint?: Constraint;\n  private readonly kernelRegularizer?: Regularizer;\n  private readonly biasRegularizer?: Regularizer;\n\n  constructor(args: DenseLayerArgs) {\n    super(args);\n    if (args.batchInputShape == null && args.inputShape == null &&\n        args.inputDim != null) {\n      // This logic is copied from Layer's constructor, since we can't\n      // do exactly what the Python constructor does for Dense().\n      let batchSize: number = null;\n      if (args.batchSize != null) {\n        batchSize = args.batchSize;\n      }\n      this.batchInputShape = [batchSize, args.inputDim];\n    }\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(args.activation);\n    if (args.useBias != null) {\n      this.useBias = args.useBias;\n    }\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n    this.activityRegularizer = getRegularizer(args.activityRegularizer);\n    this.supportsMasking = true;\n\n    this.inputSpec = [{minNDim: 2}];\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputLastDim = inputShape[inputShape.length - 1];\n    if (this.kernel == null) {\n      this.kernel = this.addWeight(\n          'kernel', [inputLastDim, this.units], null, this.kernelInitializer,\n          this.kernelRegularizer, true, this.kernelConstraint);\n      if (this.useBias) {\n        this.bias = this.addWeight(\n            'bias', [this.units], null, this.biasInitializer,\n            this.biasRegularizer, true, this.biasConstraint);\n      }\n    }\n\n    this.inputSpec = [{minNDim: 2, axes: {[-1]: inputLastDim}}];\n    this.built = true;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const outputShape = inputShape.slice();\n    outputShape[outputShape.length - 1] = this.units;\n    return outputShape;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      // Dense layer accepts only a single input.\n      const input = getExactlyOneTensor(inputs);\n      const fusedActivationName =\n          mapActivationToFusedKernel(this.activation.getClassName());\n      let output: Tensor;\n\n      if (fusedActivationName != null) {\n        output = K.dot(\n            input, this.kernel.read(), fusedActivationName,\n            this.bias ? this.bias.read() : null);\n      } else {\n        output = K.dot(input, this.kernel.read());\n        if (this.bias != null) {\n          output = K.biasAdd(output, this.bias.read());\n        }\n        if (this.activation != null) {\n          output = this.activation.apply(output);\n        }\n      }\n\n      return output;\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Dense);\n\nexport declare interface FlattenLayerArgs extends LayerArgs {\n  /** Image data format: channelsLast (default) or channelsFirst. */\n  dataFormat?: DataFormat;\n}\n\nexport class Flatten extends Layer {\n  private dataFormat: DataFormat;\n\n  /** @nocollapse */\n  static className = 'Flatten';\n  constructor(args?: FlattenLayerArgs) {\n    args = args || {};\n    super(args);\n    this.inputSpec = [{minNDim: 3}];\n    this.dataFormat = args.dataFormat;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    for (const dim of inputShape.slice(1)) {\n      if (dim == null) {\n        throw new ValueError(\n            `The shape of the input to \"Flatten\" is not fully defined ` +\n            `(got ${inputShape.slice(1)}). Make sure to pass a complete ` +\n            `\"input_shape\" or \"batch_input_shape\" argument to the first ` +\n            `layer in your model.`);\n      }\n    }\n    return [inputShape[0], arrayProd(inputShape, 1)];\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n\n      let input = getExactlyOneTensor(inputs);\n      if (this.dataFormat === 'channelsFirst' && input.rank > 1) {\n        const permutation: number[] = [0];\n        for (let i = 2; i < input.rank; ++i) {\n          permutation.push(i);\n        }\n        permutation.push(1);\n        input = transpose(input, permutation);\n      }\n\n      return K.batchFlatten(input);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {};\n    if (this.dataFormat != null) {\n      config['dataFormat'] = this.dataFormat;\n    }\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Flatten);\n\nexport declare interface ActivationLayerArgs extends LayerArgs {\n  /**\n   * Name of the activation function to use.\n   */\n  activation: ActivationIdentifier;\n}\n\nexport class Activation extends Layer {\n  /** @nocollapse */\n  static className = 'Activation';\n  activation: ActivationFn;\n\n  constructor(args: ActivationLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.activation = getActivation(args.activation);\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      return this.activation.apply(input);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config = {activation: serializeActivation(this.activation)};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Activation);\n\nexport declare interface ReshapeLayerArgs extends LayerArgs {\n  /** The target shape. Does not include the batch axis. */\n  targetShape: Shape;\n}\n\nexport declare interface RepeatVectorLayerArgs extends LayerArgs {\n  /**\n   * The integer number of times to repeat the input.\n   */\n  n: number;\n}\n\nexport class RepeatVector extends Layer {\n  /** @nocollapse */\n  static className = 'RepeatVector';\n  readonly n: number;\n\n  constructor(args: RepeatVectorLayerArgs) {\n    super(args);\n    this.n = args.n;\n    this.inputSpec = [{ndim: 2}];\n  }\n\n  override computeOutputShape(inputShape: Shape): Shape {\n    return [inputShape[0], this.n, inputShape[1]];\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = getExactlyOneTensor(inputs);\n      return K.repeat(inputs, this.n);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config = {\n      n: this.n,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(RepeatVector);\n\nexport class Reshape extends Layer {\n  /** @nocollapse */\n  static className = 'Reshape';\n  private targetShape: Shape;\n\n  constructor(args: ReshapeLayerArgs) {\n    super(args);\n    this.targetShape = args.targetShape;\n\n    // Make sure that all unknown dimensions are represented as `null`.\n    for (let i = 0; i < this.targetShape.length; ++i) {\n      if (this.isUnknown(this.targetShape[i])) {\n        this.targetShape[i] = null;\n      }\n    }\n  }\n\n  private isUnknown(dim: number): boolean {\n    return dim < 0 || dim == null;\n  }\n\n  /**\n   * Finds and replaces a missing dimension in output shape.\n   *\n   * This is a near direct port of the internal Numpy function\n   * `_fix_unknown_dimension` in `numpy/core/src/multiarray/shape.c`.\n   *\n   * @param inputShape: Original shape of array begin reshape.\n   * @param outputShape: Target shape of the array, with at most a single\n   * `null` or negative number, which indicates an underdetermined dimension\n   * that should be derived from `inputShape` and the known dimensions of\n   *   `outputShape`.\n   * @returns: The output shape with `null` replaced with its computed value.\n   * @throws: ValueError: If `inputShape` and `outputShape` do not match.\n   */\n  private fixUnknownDimension(inputShape: Shape, outputShape: Shape): Shape {\n    const errorMsg = 'Total size of new array must be unchanged.';\n    const finalShape = outputShape.slice();\n    let known = 1;\n    let unknown = null;\n    for (let i = 0; i < finalShape.length; ++i) {\n      const dim = finalShape[i];\n      if (this.isUnknown(dim)) {\n        if (unknown === null) {\n          unknown = i;\n        } else {\n          throw new ValueError('Can only specifiy one unknown dimension.');\n        }\n      } else {\n        known *= dim;\n      }\n    }\n\n    const originalSize = arrayProd(inputShape);\n    if (unknown !== null) {\n      if (known === 0 || originalSize % known !== 0) {\n        throw new ValueError(errorMsg);\n      }\n      finalShape[unknown] = originalSize / known;\n    } else if (originalSize !== known) {\n      throw new ValueError(errorMsg);\n    }\n\n    return finalShape;\n  }\n\n  override computeOutputShape(inputShape: Shape): Shape {\n    let anyUnknownDims = false;\n    for (let i = 0; i < inputShape.length; ++i) {\n      if (this.isUnknown(inputShape[i])) {\n        anyUnknownDims = true;\n        break;\n      }\n    }\n\n    if (anyUnknownDims) {\n      return inputShape.slice(0, 1).concat(this.targetShape);\n    } else {\n      return inputShape.slice(0, 1).concat(\n          this.fixUnknownDimension(inputShape.slice(1), this.targetShape));\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const outputShape = inputShape.slice(0, 1).concat(\n          this.fixUnknownDimension(inputShape.slice(1), this.targetShape));\n      return reshape(input, outputShape);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config = {\n      targetShape: this.targetShape,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Reshape);\n\nexport declare interface PermuteLayerArgs extends LayerArgs {\n  /**\n   * Array of integers. Permutation pattern. Does not include the\n   * sample (batch) dimension. Index starts at 1.\n   * For instance, `[2, 1]` permutes the first and second dimensions\n   * of the input.\n   */\n  dims: number[];\n}\n\nexport class Permute extends Layer {\n  /** @nocollapse */\n  static className = 'Permute';\n  readonly dims: number[];\n  private readonly dimsIncludingBatch: number[];\n\n  constructor(args: PermuteLayerArgs) {\n    super(args);\n    if (args.dims == null) {\n      throw new Error(\n          'Required configuration field `dims` is missing during Permute ' +\n          'constructor call.');\n    }\n    if (!Array.isArray(args.dims)) {\n      throw new Error(\n          'Permute constructor requires `dims` to be an Array, but received ' +\n          `${args.dims} instead.`);\n    }\n\n    // Check the validity of the permutation indices.\n    const expectedSortedIndices = range(1, args.dims.length + 1);\n    if (!util.arraysEqual(args.dims.slice().sort(), expectedSortedIndices)) {\n      throw new Error(\n          'Invalid permutation `dims`: ' + JSON.stringify(args.dims) +\n          ' `dims` must contain consecutive integers starting from 1.');\n    }\n\n    this.dims = args.dims;\n    this.dimsIncludingBatch = [0].concat(this.dims);\n    this.inputSpec = [new InputSpec({ndim: this.dims.length + 1})];\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const outputShape = inputShape.slice();\n    this.dims.forEach((dim: number, i: number) => {\n      outputShape[i + 1] = (inputShape as Shape)[dim];\n    });\n    return outputShape;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return transpose(getExactlyOneTensor(inputs), this.dimsIncludingBatch);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config = {\n      dims: this.dims,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Permute);\n\nexport declare interface MaskingArgs extends LayerArgs {\n  /**\n   * Masking Value. Defaults to `0.0`.\n   */\n  maskValue?: number;\n}\n\nexport class Masking extends Layer {\n  /** @nocollapse */\n  static className = 'Masking';\n  maskValue: number;\n\n  constructor(args?: MaskingArgs) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maskValue = args.maskValue == null ? 0 : args.maskValue;\n    } else {\n      this.maskValue = 0;\n    }\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {maskValue: this.maskValue};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]):\n      Tensor {\n    const input = getExactlyOneTensor(inputs);\n    const axis = -1;\n    return any(notEqual(input, this.maskValue), axis);\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const axis = -1;\n      const keepDims = true;\n      const booleanMask = any(notEqual(input, this.maskValue), axis, keepDims);\n      const output = mul(input, cast(booleanMask, input.dtype));\n      return output;\n    });\n  }\n}\nserialization.registerClass(Masking);\n"],"mappings":"AAAA;;;;;;;;;AAUA;;;AAIA,SAAQA,GAAG,EAAEC,IAAI,EAAEC,GAAG,EAAEC,QAAQ,EAAEC,OAAO,EAAEC,aAAa,EAAUC,IAAI,EAAEC,SAAS,EAAEC,IAAI,QAAO,uBAAuB;AAErH,SAAoCC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AAC7F,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAA0CC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AACnG,SAAuBC,SAAS,EAAEC,KAAK,QAAkB,oBAAoB;AAC7E,SAAQC,UAAU,QAAO,WAAW;AACpC,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAIxG,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQC,qBAAqB,EAAEC,0BAA0B,QAAO,wBAAwB;AACxF,SAAQC,SAAS,EAAEC,KAAK,QAAO,qBAAqB;AACpD,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAqB5E,MAAaC,OAAQ,SAAQZ,KAAK;EAOhCa,YAAYC,IAAsB;IAChC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACC,IAAI,GAAGC,IAAI,CAACC,GAAG,CAACD,IAAI,CAACE,GAAG,CAACJ,IAAI,CAACC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC;IAC/C;IACA,IAAI,CAACI,UAAU,GAAGL,IAAI,CAACK,UAAU;IACjC,IAAI,CAACC,IAAI,GAAGN,IAAI,CAACM,IAAI;IACrB,IAAI,CAACC,eAAe,GAAG,IAAI;EAC7B;EAEUC,aAAaA,CAACC,KAAa;IACnC,IAAI,IAAI,CAACJ,UAAU,IAAI,IAAI,EAAE;MAC3B,OAAO,IAAI,CAACA,UAAU;;IAExB,MAAMK,UAAU,GAAGD,KAAK,CAACE,KAAK;IAC9B,MAAMN,UAAU,GAAU,EAAE;IAC5B,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACP,UAAU,CAACQ,MAAM,EAAE,EAAED,CAAC,EAAE;MAC/CP,UAAU,CAACS,IAAI,CACX,IAAI,CAACT,UAAU,CAACO,CAAC,CAAC,IAAI,IAAI,GAAGF,UAAU,CAACE,CAAC,CAAC,GAAG,IAAI,CAACP,UAAU,CAACO,CAAC,CAAC,CAAC;;IAEtE,OAAOP,UAAU;EACnB;EAESU,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAMR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACzC,IAAI,CAAC,GAAG,IAAI,CAACf,IAAI,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;QAClC,MAAMkB,QAAQ,GACVF,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;QAC3D,MAAMZ,UAAU,GAAG,IAAI,CAACG,aAAa,CAACC,KAAK,CAAC;QAC5C,MAAMW,MAAM,GAAGtC,CAAC,CAACuC,YAAY,CACzB,MAAMvC,CAAC,CAACwC,OAAO,CAACb,KAAK,EAAE,IAAI,CAACR,IAAI,EAAEI,UAAU,EAAE,IAAI,CAACC,IAAI,CAAC,EACxD,MAAMG,KAAK,EAAEU,QAAQ,CAAC;QAC1B,OAAOC,MAAM;;MAEf,OAAOJ,MAAM;IACf,CAAC,CAAC;EACJ;EAESO,SAASA,CAAA;IAChB,MAAMC,MAAM,GAAG;MACbvB,IAAI,EAAE,IAAI,CAACA,IAAI;MACfI,UAAU,EAAE,IAAI,CAACA,UAAU;MAC3BC,IAAI,EAAE,IAAI,CAACA;KACZ;IACD,MAAMmB,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;EAESI,OAAOA,CAAA;IACd,OAAO,KAAK,CAACA,OAAO,EAAE;EACxB;;AA1DA;AACO9B,OAAA,CAAA+B,SAAS,GAAG,SAAS;SAFjB/B,OAAO;AA6DpBtB,aAAa,CAACsD,aAAa,CAAChC,OAAO,CAAC;AA4DpC,MAAaiC,gBAAiB,SAAQjC,OAAO;EAI3CC,YAAYC,IAAiC;IAC3C,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACgC,SAAS,GAAG,CAAC;MAACC,IAAI,EAAE;IAAC,CAAC,CAAC;EAC9B;EAEmBzB,aAAaA,CAACC,KAAa;IAC5C,MAAMC,UAAU,GAAGD,KAAK,CAACE,KAAK;IAC9B,OAAO,CAACD,UAAU,CAAC,CAAC,CAAC,EAAE,CAAC,EAAEA,UAAU,CAAC,CAAC,CAAC,CAAC;EAC1C;;AAXA;AACgBqB,gBAAA,CAAAF,SAAS,GAAG,kBAAkB;SAFnCE,gBAAgB;AAc7BvD,aAAa,CAACsD,aAAa,CAACC,gBAAgB,CAAC;AAE7C,MAAaG,KAAM,SAAQhD,KAAK;EAmB9Ba,YAAYC,IAAoB;IAC9B,KAAK,CAACA,IAAI,CAAC;IAhBb;IACQ,KAAAmC,UAAU,GAAiB,IAAI;IAC/B,KAAAC,OAAO,GAAG,IAAI;IAGd,KAAAC,MAAM,GAAkB,IAAI;IAC5B,KAAAC,IAAI,GAAkB,IAAI;IAEzB,KAAAC,0BAA0B,GAA0B,cAAc;IAClE,KAAAC,wBAAwB,GAA0B,OAAO;IAQhE,IAAIxC,IAAI,CAACyC,eAAe,IAAI,IAAI,IAAIzC,IAAI,CAACU,UAAU,IAAI,IAAI,IACvDV,IAAI,CAAC0C,QAAQ,IAAI,IAAI,EAAE;MACzB;MACA;MACA,IAAIC,SAAS,GAAW,IAAI;MAC5B,IAAI3C,IAAI,CAAC2C,SAAS,IAAI,IAAI,EAAE;QAC1BA,SAAS,GAAG3C,IAAI,CAAC2C,SAAS;;MAE5B,IAAI,CAACF,eAAe,GAAG,CAACE,SAAS,EAAE3C,IAAI,CAAC0C,QAAQ,CAAC;;IAGnD,IAAI,CAACE,KAAK,GAAG5C,IAAI,CAAC4C,KAAK;IACvBpD,qBAAqB,CAAC,IAAI,CAACoD,KAAK,EAAE,OAAO,CAAC;IAC1C,IAAI,CAACT,UAAU,GAAGvD,aAAa,CAACoB,IAAI,CAACmC,UAAU,CAAC;IAChD,IAAInC,IAAI,CAACoC,OAAO,IAAI,IAAI,EAAE;MACxB,IAAI,CAACA,OAAO,GAAGpC,IAAI,CAACoC,OAAO;;IAE7B,IAAI,CAACS,iBAAiB,GAAGzD,cAAc,CACnCY,IAAI,CAAC6C,iBAAiB,IAAI,IAAI,CAACN,0BAA0B,CAAC;IAC9D,IAAI,CAACO,eAAe,GAChB1D,cAAc,CAACY,IAAI,CAAC8C,eAAe,IAAI,IAAI,CAACN,wBAAwB,CAAC;IACzE,IAAI,CAACO,gBAAgB,GAAGhE,aAAa,CAACiB,IAAI,CAAC+C,gBAAgB,CAAC;IAC5D,IAAI,CAACC,cAAc,GAAGjE,aAAa,CAACiB,IAAI,CAACgD,cAAc,CAAC;IACxD,IAAI,CAACC,iBAAiB,GAAG3D,cAAc,CAACU,IAAI,CAACiD,iBAAiB,CAAC;IAC/D,IAAI,CAACC,eAAe,GAAG5D,cAAc,CAACU,IAAI,CAACkD,eAAe,CAAC;IAC3D,IAAI,CAACC,mBAAmB,GAAG7D,cAAc,CAACU,IAAI,CAACmD,mBAAmB,CAAC;IACnE,IAAI,CAAC5C,eAAe,GAAG,IAAI;IAE3B,IAAI,CAACyB,SAAS,GAAG,CAAC;MAACoB,OAAO,EAAE;IAAC,CAAC,CAAC;EACjC;EAEgBC,KAAKA,CAAC3C,UAAyB;IAC7CA,UAAU,GAAGd,kBAAkB,CAACc,UAAU,CAAC;IAC3C,MAAM4C,YAAY,GAAG5C,UAAU,CAACA,UAAU,CAACG,MAAM,GAAG,CAAC,CAAC;IACtD,IAAI,IAAI,CAACwB,MAAM,IAAI,IAAI,EAAE;MACvB,IAAI,CAACA,MAAM,GAAG,IAAI,CAACkB,SAAS,CACxB,QAAQ,EAAE,CAACD,YAAY,EAAE,IAAI,CAACV,KAAK,CAAC,EAAE,IAAI,EAAE,IAAI,CAACC,iBAAiB,EAClE,IAAI,CAACI,iBAAiB,EAAE,IAAI,EAAE,IAAI,CAACF,gBAAgB,CAAC;MACxD,IAAI,IAAI,CAACX,OAAO,EAAE;QAChB,IAAI,CAACE,IAAI,GAAG,IAAI,CAACiB,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAACX,KAAK,CAAC,EAAE,IAAI,EAAE,IAAI,CAACE,eAAe,EAChD,IAAI,CAACI,eAAe,EAAE,IAAI,EAAE,IAAI,CAACF,cAAc,CAAC;;;IAIxD,IAAI,CAAChB,SAAS,GAAG,CAAC;MAACoB,OAAO,EAAE,CAAC;MAAEI,IAAI,EAAE;QAAC,CAAC,CAAC,CAAC,GAAGF;MAAY;IAAC,CAAC,CAAC;IAC3D,IAAI,CAACG,KAAK,GAAG,IAAI;EACnB;EAESC,kBAAkBA,CAAChD,UAAyB;IACnDA,UAAU,GAAGd,kBAAkB,CAACc,UAAU,CAAC;IAC3C,MAAMiD,WAAW,GAAGjD,UAAU,CAACkD,KAAK,EAAE;IACtCD,WAAW,CAACA,WAAW,CAAC9C,MAAM,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC+B,KAAK;IAChD,OAAOe,WAAW;EACpB;EAES5C,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC;MACA,MAAMR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACzC,MAAM6C,mBAAmB,GACrBpE,0BAA0B,CAAC,IAAI,CAAC0C,UAAU,CAAC2B,YAAY,EAAE,CAAC;MAC9D,IAAI1C,MAAc;MAElB,IAAIyC,mBAAmB,IAAI,IAAI,EAAE;QAC/BzC,MAAM,GAAGtC,CAAC,CAACiF,GAAG,CACVtD,KAAK,EAAE,IAAI,CAAC4B,MAAM,CAAC2B,IAAI,EAAE,EAAEH,mBAAmB,EAC9C,IAAI,CAACvB,IAAI,GAAG,IAAI,CAACA,IAAI,CAAC0B,IAAI,EAAE,GAAG,IAAI,CAAC;OACzC,MAAM;QACL5C,MAAM,GAAGtC,CAAC,CAACiF,GAAG,CAACtD,KAAK,EAAE,IAAI,CAAC4B,MAAM,CAAC2B,IAAI,EAAE,CAAC;QACzC,IAAI,IAAI,CAAC1B,IAAI,IAAI,IAAI,EAAE;UACrBlB,MAAM,GAAGtC,CAAC,CAACmF,OAAO,CAAC7C,MAAM,EAAE,IAAI,CAACkB,IAAI,CAAC0B,IAAI,EAAE,CAAC;;QAE9C,IAAI,IAAI,CAAC7B,UAAU,IAAI,IAAI,EAAE;UAC3Bf,MAAM,GAAG,IAAI,CAACe,UAAU,CAAC+B,KAAK,CAAC9C,MAAM,CAAC;;;MAI1C,OAAOA,MAAM;IACf,CAAC,CAAC;EACJ;EAESG,SAASA,CAAA;IAChB,MAAMC,MAAM,GAA6B;MACvCoB,KAAK,EAAE,IAAI,CAACA,KAAK;MACjBT,UAAU,EAAEtD,mBAAmB,CAAC,IAAI,CAACsD,UAAU,CAAC;MAChDC,OAAO,EAAE,IAAI,CAACA,OAAO;MACrBS,iBAAiB,EAAExD,oBAAoB,CAAC,IAAI,CAACwD,iBAAiB,CAAC;MAC/DC,eAAe,EAAEzD,oBAAoB,CAAC,IAAI,CAACyD,eAAe,CAAC;MAC3DG,iBAAiB,EAAE1D,oBAAoB,CAAC,IAAI,CAAC0D,iBAAiB,CAAC;MAC/DC,eAAe,EAAE3D,oBAAoB,CAAC,IAAI,CAAC2D,eAAe,CAAC;MAC3DC,mBAAmB,EAAE5D,oBAAoB,CAAC,IAAI,CAAC4D,mBAAmB,CAAC;MACnEJ,gBAAgB,EAAE/D,mBAAmB,CAAC,IAAI,CAAC+D,gBAAgB,CAAC;MAC5DC,cAAc,EAAEhE,mBAAmB,CAAC,IAAI,CAACgE,cAAc;KACxD;IACD,MAAMvB,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAvHA;AACOU,KAAA,CAAAL,SAAS,GAAG,OAAO;SAFfK,KAAK;AA0HlB1D,aAAa,CAACsD,aAAa,CAACI,KAAK,CAAC;AAOlC,MAAaiC,OAAQ,SAAQjF,KAAK;EAKhCa,YAAYC,IAAuB;IACjCA,IAAI,GAAGA,IAAI,IAAI,EAAE;IACjB,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACgC,SAAS,GAAG,CAAC;MAACoB,OAAO,EAAE;IAAC,CAAC,CAAC;IAC/B,IAAI,CAACgB,UAAU,GAAGpE,IAAI,CAACoE,UAAU;EACnC;EAESV,kBAAkBA,CAAChD,UAAyB;IACnDA,UAAU,GAAGd,kBAAkB,CAACc,UAAU,CAAC;IAC3C,KAAK,MAAM2D,GAAG,IAAI3D,UAAU,CAACkD,KAAK,CAAC,CAAC,CAAC,EAAE;MACrC,IAAIS,GAAG,IAAI,IAAI,EAAE;QACf,MAAM,IAAIlF,UAAU,CAChB,2DAA2D,GAC3D,QAAQuB,UAAU,CAACkD,KAAK,CAAC,CAAC,CAAC,kCAAkC,GAC7D,6DAA6D,GAC7D,sBAAsB,CAAC;;;IAG/B,OAAO,CAAClD,UAAU,CAAC,CAAC,CAAC,EAAEhB,SAAS,CAACgB,UAAU,EAAE,CAAC,CAAC,CAAC;EAClD;EAESK,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MAEnC,IAAIR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACvC,IAAI,IAAI,CAACoD,UAAU,KAAK,eAAe,IAAI3D,KAAK,CAAC6D,IAAI,GAAG,CAAC,EAAE;QACzD,MAAMC,WAAW,GAAa,CAAC,CAAC,CAAC;QACjC,KAAK,IAAI3D,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,KAAK,CAAC6D,IAAI,EAAE,EAAE1D,CAAC,EAAE;UACnC2D,WAAW,CAACzD,IAAI,CAACF,CAAC,CAAC;;QAErB2D,WAAW,CAACzD,IAAI,CAAC,CAAC,CAAC;QACnBL,KAAK,GAAG/B,SAAS,CAAC+B,KAAK,EAAE8D,WAAW,CAAC;;MAGvC,OAAOzF,CAAC,CAAC0F,YAAY,CAAC/D,KAAK,CAAC;IAC9B,CAAC,CAAC;EACJ;EAESc,SAASA,CAAA;IAChB,MAAMC,MAAM,GAA6B,EAAE;IAC3C,IAAI,IAAI,CAAC4C,UAAU,IAAI,IAAI,EAAE;MAC3B5C,MAAM,CAAC,YAAY,CAAC,GAAG,IAAI,CAAC4C,UAAU;;IAExC,MAAM3C,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAjDA;AACO2C,OAAA,CAAAtC,SAAS,GAAG,SAAS;SAJjBsC,OAAO;AAsDpB3F,aAAa,CAACsD,aAAa,CAACqC,OAAO,CAAC;AASpC,MAAaM,UAAW,SAAQvF,KAAK;EAKnCa,YAAYC,IAAyB;IACnC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACO,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAC4B,UAAU,GAAGvD,aAAa,CAACoB,IAAI,CAACmC,UAAU,CAAC;EAClD;EAESpB,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAMR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACzC,OAAO,IAAI,CAACmB,UAAU,CAAC+B,KAAK,CAACzD,KAAK,CAAC;IACrC,CAAC,CAAC;EACJ;EAESc,SAASA,CAAA;IAChB,MAAMC,MAAM,GAAG;MAACW,UAAU,EAAEtD,mBAAmB,CAAC,IAAI,CAACsD,UAAU;IAAC,CAAC;IACjE,MAAMV,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAvBA;AACOiD,UAAA,CAAA5C,SAAS,GAAG,YAAY;SAFpB4C,UAAU;AA0BvBjG,aAAa,CAACsD,aAAa,CAAC2C,UAAU,CAAC;AAcvC,MAAaC,YAAa,SAAQxF,KAAK;EAKrCa,YAAYC,IAA2B;IACrC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAAC2E,CAAC,GAAG3E,IAAI,CAAC2E,CAAC;IACf,IAAI,CAAC3C,SAAS,GAAG,CAAC;MAACC,IAAI,EAAE;IAAC,CAAC,CAAC;EAC9B;EAESyB,kBAAkBA,CAAChD,UAAiB;IAC3C,OAAO,CAACA,UAAU,CAAC,CAAC,CAAC,EAAE,IAAI,CAACiE,CAAC,EAAEjE,UAAU,CAAC,CAAC,CAAC,CAAC;EAC/C;EAESK,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACfuC,MAAM,GAAGnB,mBAAmB,CAACmB,MAAM,CAAC;MACpC,OAAOlC,CAAC,CAAC8F,MAAM,CAAC5D,MAAM,EAAE,IAAI,CAAC2D,CAAC,CAAC;IACjC,CAAC,CAAC;EACJ;EAESpD,SAASA,CAAA;IAChB,MAAMC,MAAM,GAAG;MACbmD,CAAC,EAAE,IAAI,CAACA;KACT;IACD,MAAMlD,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AA5BA;AACOkD,YAAA,CAAA7C,SAAS,GAAG,cAAc;SAFtB6C,YAAY;AA+BzBlG,aAAa,CAACsD,aAAa,CAAC4C,YAAY,CAAC;AAEzC,MAAaG,OAAQ,SAAQ3F,KAAK;EAKhCa,YAAYC,IAAsB;IAChC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAAC8E,WAAW,GAAG9E,IAAI,CAAC8E,WAAW;IAEnC;IACA,KAAK,IAAIlE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACkE,WAAW,CAACjE,MAAM,EAAE,EAAED,CAAC,EAAE;MAChD,IAAI,IAAI,CAACmE,SAAS,CAAC,IAAI,CAACD,WAAW,CAAClE,CAAC,CAAC,CAAC,EAAE;QACvC,IAAI,CAACkE,WAAW,CAAClE,CAAC,CAAC,GAAG,IAAI;;;EAGhC;EAEQmE,SAASA,CAACV,GAAW;IAC3B,OAAOA,GAAG,GAAG,CAAC,IAAIA,GAAG,IAAI,IAAI;EAC/B;EAEA;;;;;;;;;;;;;;EAcQW,mBAAmBA,CAACtE,UAAiB,EAAEiD,WAAkB;IAC/D,MAAMsB,QAAQ,GAAG,4CAA4C;IAC7D,MAAMC,UAAU,GAAGvB,WAAW,CAACC,KAAK,EAAE;IACtC,IAAIuB,KAAK,GAAG,CAAC;IACb,IAAIC,OAAO,GAAG,IAAI;IAClB,KAAK,IAAIxE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGsE,UAAU,CAACrE,MAAM,EAAE,EAAED,CAAC,EAAE;MAC1C,MAAMyD,GAAG,GAAGa,UAAU,CAACtE,CAAC,CAAC;MACzB,IAAI,IAAI,CAACmE,SAAS,CAACV,GAAG,CAAC,EAAE;QACvB,IAAIe,OAAO,KAAK,IAAI,EAAE;UACpBA,OAAO,GAAGxE,CAAC;SACZ,MAAM;UACL,MAAM,IAAIzB,UAAU,CAAC,0CAA0C,CAAC;;OAEnE,MAAM;QACLgG,KAAK,IAAId,GAAG;;;IAIhB,MAAMgB,YAAY,GAAG3F,SAAS,CAACgB,UAAU,CAAC;IAC1C,IAAI0E,OAAO,KAAK,IAAI,EAAE;MACpB,IAAID,KAAK,KAAK,CAAC,IAAIE,YAAY,GAAGF,KAAK,KAAK,CAAC,EAAE;QAC7C,MAAM,IAAIhG,UAAU,CAAC8F,QAAQ,CAAC;;MAEhCC,UAAU,CAACE,OAAO,CAAC,GAAGC,YAAY,GAAGF,KAAK;KAC3C,MAAM,IAAIE,YAAY,KAAKF,KAAK,EAAE;MACjC,MAAM,IAAIhG,UAAU,CAAC8F,QAAQ,CAAC;;IAGhC,OAAOC,UAAU;EACnB;EAESxB,kBAAkBA,CAAChD,UAAiB;IAC3C,IAAI4E,cAAc,GAAG,KAAK;IAC1B,KAAK,IAAI1E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,UAAU,CAACG,MAAM,EAAE,EAAED,CAAC,EAAE;MAC1C,IAAI,IAAI,CAACmE,SAAS,CAACrE,UAAU,CAACE,CAAC,CAAC,CAAC,EAAE;QACjC0E,cAAc,GAAG,IAAI;QACrB;;;IAIJ,IAAIA,cAAc,EAAE;MAClB,OAAO5E,UAAU,CAACkD,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC2B,MAAM,CAAC,IAAI,CAACT,WAAW,CAAC;KACvD,MAAM;MACL,OAAOpE,UAAU,CAACkD,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC2B,MAAM,CAChC,IAAI,CAACP,mBAAmB,CAACtE,UAAU,CAACkD,KAAK,CAAC,CAAC,CAAC,EAAE,IAAI,CAACkB,WAAW,CAAC,CAAC;;EAExE;EAES/D,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAMR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACzC,MAAMN,UAAU,GAAGD,KAAK,CAACE,KAAK;MAC9B,MAAMgD,WAAW,GAAGjD,UAAU,CAACkD,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC2B,MAAM,CAC7C,IAAI,CAACP,mBAAmB,CAACtE,UAAU,CAACkD,KAAK,CAAC,CAAC,CAAC,EAAE,IAAI,CAACkB,WAAW,CAAC,CAAC;MACpE,OAAOvG,OAAO,CAACkC,KAAK,EAAEkD,WAAW,CAAC;IACpC,CAAC,CAAC;EACJ;EAESpC,SAASA,CAAA;IAChB,MAAMC,MAAM,GAAG;MACbsD,WAAW,EAAE,IAAI,CAACA;KACnB;IACD,MAAMrD,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AApGA;AACOqD,OAAA,CAAAhD,SAAS,GAAG,SAAS;SAFjBgD,OAAO;AAuGpBrG,aAAa,CAACsD,aAAa,CAAC+C,OAAO,CAAC;AAYpC,MAAaW,OAAQ,SAAQtG,KAAK;EAMhCa,YAAYC,IAAsB;IAChC,KAAK,CAACA,IAAI,CAAC;IACX,IAAIA,IAAI,CAACyF,IAAI,IAAI,IAAI,EAAE;MACrB,MAAM,IAAIC,KAAK,CACX,gEAAgE,GAChE,mBAAmB,CAAC;;IAE1B,IAAI,CAACC,KAAK,CAACC,OAAO,CAAC5F,IAAI,CAACyF,IAAI,CAAC,EAAE;MAC7B,MAAM,IAAIC,KAAK,CACX,mEAAmE,GACnE,GAAG1F,IAAI,CAACyF,IAAI,WAAW,CAAC;;IAG9B;IACA,MAAMI,qBAAqB,GAAGlG,KAAK,CAAC,CAAC,EAAEK,IAAI,CAACyF,IAAI,CAAC5E,MAAM,GAAG,CAAC,CAAC;IAC5D,IAAI,CAAClC,IAAI,CAACmH,WAAW,CAAC9F,IAAI,CAACyF,IAAI,CAAC7B,KAAK,EAAE,CAACmC,IAAI,EAAE,EAAEF,qBAAqB,CAAC,EAAE;MACtE,MAAM,IAAIH,KAAK,CACX,8BAA8B,GAAGM,IAAI,CAACC,SAAS,CAACjG,IAAI,CAACyF,IAAI,CAAC,GAC1D,4DAA4D,CAAC;;IAGnE,IAAI,CAACA,IAAI,GAAGzF,IAAI,CAACyF,IAAI;IACrB,IAAI,CAACS,kBAAkB,GAAG,CAAC,CAAC,CAAC,CAACX,MAAM,CAAC,IAAI,CAACE,IAAI,CAAC;IAC/C,IAAI,CAACzD,SAAS,GAAG,CAAC,IAAI/C,SAAS,CAAC;MAACgD,IAAI,EAAE,IAAI,CAACwD,IAAI,CAAC5E,MAAM,GAAG;IAAC,CAAC,CAAC,CAAC;EAChE;EAES6C,kBAAkBA,CAAChD,UAAyB;IACnDA,UAAU,GAAGd,kBAAkB,CAACc,UAAU,CAAC;IAC3C,MAAMiD,WAAW,GAAGjD,UAAU,CAACkD,KAAK,EAAE;IACtC,IAAI,CAAC6B,IAAI,CAACU,OAAO,CAAC,CAAC9B,GAAW,EAAEzD,CAAS,KAAI;MAC3C+C,WAAW,CAAC/C,CAAC,GAAG,CAAC,CAAC,GAAIF,UAAoB,CAAC2D,GAAG,CAAC;IACjD,CAAC,CAAC;IACF,OAAOV,WAAW;EACpB;EAES5C,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOvC,SAAS,CAACmB,mBAAmB,CAACmB,MAAM,CAAC,EAAE,IAAI,CAACkF,kBAAkB,CAAC;EACxE;EAES3E,SAASA,CAAA;IAChB,MAAMC,MAAM,GAAG;MACbiE,IAAI,EAAE,IAAI,CAACA;KACZ;IACD,MAAMhE,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAnDA;AACOgE,OAAA,CAAA3D,SAAS,GAAG,SAAS;SAFjB2D,OAAO;AAsDpBhH,aAAa,CAACsD,aAAa,CAAC0D,OAAO,CAAC;AASpC,MAAaY,OAAQ,SAAQlH,KAAK;EAKhCa,YAAYC,IAAkB;IAC5B,KAAK,CAACA,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI,CAAC;IAC/B,IAAI,CAACO,eAAe,GAAG,IAAI;IAC3B,IAAIP,IAAI,IAAI,IAAI,EAAE;MAChB,IAAI,CAACqG,SAAS,GAAGrG,IAAI,CAACqG,SAAS,IAAI,IAAI,GAAG,CAAC,GAAGrG,IAAI,CAACqG,SAAS;KAC7D,MAAM;MACL,IAAI,CAACA,SAAS,GAAG,CAAC;;EAEtB;EAES3C,kBAAkBA,CAAChD,UAAyB;IACnD,OAAOA,UAAU;EACnB;EAESa,SAASA,CAAA;IAChB,MAAME,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpC,MAAMC,MAAM,GAAG;MAAC6E,SAAS,EAAE,IAAI,CAACA;IAAS,CAAC;IAC1C3E,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;EAES8E,WAAWA,CAACtF,MAAuB,EAAEuF,IAAsB;IAElE,MAAM9F,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;IACzC,MAAMwF,IAAI,GAAG,CAAC,CAAC;IACf,OAAOrI,GAAG,CAACG,QAAQ,CAACmC,KAAK,EAAE,IAAI,CAAC4F,SAAS,CAAC,EAAEG,IAAI,CAAC;EACnD;EAESzF,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAOxC,IAAI,CAAC,MAAK;MACf,IAAI,CAACyC,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAMR,KAAK,GAAGZ,mBAAmB,CAACmB,MAAM,CAAC;MACzC,MAAMwF,IAAI,GAAG,CAAC,CAAC;MACf,MAAMC,QAAQ,GAAG,IAAI;MACrB,MAAMC,WAAW,GAAGvI,GAAG,CAACG,QAAQ,CAACmC,KAAK,EAAE,IAAI,CAAC4F,SAAS,CAAC,EAAEG,IAAI,EAAEC,QAAQ,CAAC;MACxE,MAAMrF,MAAM,GAAG/C,GAAG,CAACoC,KAAK,EAAErC,IAAI,CAACsI,WAAW,EAAEjG,KAAK,CAACkG,KAAK,CAAC,CAAC;MACzD,OAAOvF,MAAM;IACf,CAAC,CAAC;EACJ;;AA1CA;AACOgF,OAAA,CAAAvE,SAAS,GAAG,SAAS;SAFjBuE,OAAO;AA6CpB5H,aAAa,CAACsD,aAAa,CAACsE,OAAO,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}