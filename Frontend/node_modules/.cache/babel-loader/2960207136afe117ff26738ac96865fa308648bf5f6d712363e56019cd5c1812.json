{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { FusedBatchNorm, util } from '@tensorflow/tfjs-core';\nimport { assertNotComplex } from '../cpu_util';\nexport function batchNorm(args) {\n  const {\n    inputs,\n    backend,\n    attrs\n  } = args;\n  const {\n    x,\n    scale,\n    offset,\n    mean,\n    variance\n  } = inputs;\n  util.assert(mean.shape.length === variance.shape.length, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert(offset == null || mean.shape.length === offset.shape.length, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert(scale == null || mean.shape.length === scale.shape.length, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n  let {\n    varianceEpsilon\n  } = attrs;\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const xVals = backend.data.get(x.dataId).values;\n  const mVals = backend.data.get(mean.dataId).values;\n  const varVals = backend.data.get(variance.dataId).values;\n  const sVals = scale ? backend.data.get(scale.dataId).values : new Float32Array([1]);\n  const offVals = offset ? backend.data.get(offset.dataId).values : new Float32Array([0]);\n  const outVals = new Float32Array(xVals.length);\n  const offValsLength = offVals.length;\n  const sValsLength = sVals.length;\n  const varValsLength = varVals.length;\n  const mValsLength = mVals.length;\n  let offi = 0;\n  let mi = 0;\n  let si = 0;\n  let vi = 0;\n  for (let i = 0; i < xVals.length; ++i) {\n    outVals[i] = offVals[offi++] + (xVals[i] - mVals[mi++]) * sVals[si++] / Math.sqrt(varVals[vi++] + varianceEpsilon);\n    if (offi >= offValsLength) {\n      offi = 0;\n    }\n    if (mi >= mValsLength) {\n      mi = 0;\n    }\n    if (si >= sValsLength) {\n      si = 0;\n    }\n    if (vi >= varValsLength) {\n      vi = 0;\n    }\n  }\n  return backend.makeTensorInfo(x.shape, x.dtype, outVals);\n}\nexport const batchNormConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'cpu',\n  kernelFunc: batchNorm\n};","map":{"version":3,"names":["FusedBatchNorm","util","assertNotComplex","batchNorm","args","inputs","backend","attrs","x","scale","offset","mean","variance","assert","shape","length","varianceEpsilon","xVals","data","get","dataId","values","mVals","varVals","sVals","Float32Array","offVals","outVals","offValsLength","sValsLength","varValsLength","mValsLength","offi","mi","si","vi","i","Math","sqrt","makeTensorInfo","dtype","batchNormConfig","kernelName","backendName","kernelFunc"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-backend-cpu/src/kernels/BatchNorm.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs, KernelConfig, KernelFunc, TensorInfo, TypedArray, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {assertNotComplex} from '../cpu_util';\n\nexport function batchNorm(args: {\n  inputs: FusedBatchNormInputs,\n  backend: MathBackendCPU,\n  attrs: FusedBatchNormAttrs\n}): TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {x, scale, offset, mean, variance} = inputs;\n\n  util.assert(\n      mean.shape.length === variance.shape.length,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      offset == null || mean.shape.length === offset.shape.length,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      scale == null || mean.shape.length === scale.shape.length,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  assertNotComplex([x, mean, variance, scale, offset], 'batchNorm');\n\n  let {varianceEpsilon} = attrs;\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n\n  const xVals = backend.data.get(x.dataId).values as TypedArray;\n  const mVals = backend.data.get(mean.dataId).values as TypedArray;\n  const varVals = backend.data.get(variance.dataId).values as TypedArray;\n  const sVals = scale ? backend.data.get(scale.dataId).values as TypedArray :\n                        new Float32Array([1]);\n  const offVals = offset ?\n      backend.data.get(offset.dataId).values as TypedArray :\n      new Float32Array([0]);\n  const outVals = new Float32Array(xVals.length);\n\n  const offValsLength = offVals.length;\n  const sValsLength = sVals.length;\n  const varValsLength = varVals.length;\n  const mValsLength = mVals.length;\n\n  let offi = 0;\n  let mi = 0;\n  let si = 0;\n  let vi = 0;\n  for (let i = 0; i < xVals.length; ++i) {\n    outVals[i] = offVals[offi++] +\n        (xVals[i] - mVals[mi++]) * sVals[si++] /\n            Math.sqrt(varVals[vi++] + varianceEpsilon);\n    if (offi >= offValsLength) {\n      offi = 0;\n    }\n    if (mi >= mValsLength) {\n      mi = 0;\n    }\n    if (si >= sValsLength) {\n      si = 0;\n    }\n    if (vi >= varValsLength) {\n      vi = 0;\n    }\n  }\n  return backend.makeTensorInfo(x.shape, x.dtype, outVals);\n}\n\nexport const batchNormConfig: KernelConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: 'cpu',\n  kernelFunc: batchNorm as unknown as KernelFunc,\n};\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,cAAc,EAA+FC,IAAI,QAAO,uBAAuB;AAGvJ,SAAQC,gBAAgB,QAAO,aAAa;AAE5C,OAAM,SAAUC,SAASA,CAACC,IAIzB;EACC,MAAM;IAACC,MAAM;IAAEC,OAAO;IAAEC;EAAK,CAAC,GAAGH,IAAI;EACrC,MAAM;IAACI,CAAC;IAAEC,KAAK;IAAEC,MAAM;IAAEC,IAAI;IAAEC;EAAQ,CAAC,GAAGP,MAAM;EAEjDJ,IAAI,CAACY,MAAM,CACPF,IAAI,CAACG,KAAK,CAACC,MAAM,KAAKH,QAAQ,CAACE,KAAK,CAACC,MAAM,EAC3C,MAAM,kEAAkE,GACpE,cAAc,CAAC;EACvBd,IAAI,CAACY,MAAM,CACPH,MAAM,IAAI,IAAI,IAAIC,IAAI,CAACG,KAAK,CAACC,MAAM,KAAKL,MAAM,CAACI,KAAK,CAACC,MAAM,EAC3D,MAAM,gEAAgE,GAClE,cAAc,CAAC;EACvBd,IAAI,CAACY,MAAM,CACPJ,KAAK,IAAI,IAAI,IAAIE,IAAI,CAACG,KAAK,CAACC,MAAM,KAAKN,KAAK,CAACK,KAAK,CAACC,MAAM,EACzD,MAAM,+DAA+D,GACjE,cAAc,CAAC;EAEvBb,gBAAgB,CAAC,CAACM,CAAC,EAAEG,IAAI,EAAEC,QAAQ,EAAEH,KAAK,EAAEC,MAAM,CAAC,EAAE,WAAW,CAAC;EAEjE,IAAI;IAACM;EAAe,CAAC,GAAGT,KAAK;EAC7B,IAAIS,eAAe,IAAI,IAAI,EAAE;IAC3BA,eAAe,GAAG,KAAK;;EAGzB,MAAMC,KAAK,GAAGX,OAAO,CAACY,IAAI,CAACC,GAAG,CAACX,CAAC,CAACY,MAAM,CAAC,CAACC,MAAoB;EAC7D,MAAMC,KAAK,GAAGhB,OAAO,CAACY,IAAI,CAACC,GAAG,CAACR,IAAI,CAACS,MAAM,CAAC,CAACC,MAAoB;EAChE,MAAME,OAAO,GAAGjB,OAAO,CAACY,IAAI,CAACC,GAAG,CAACP,QAAQ,CAACQ,MAAM,CAAC,CAACC,MAAoB;EACtE,MAAMG,KAAK,GAAGf,KAAK,GAAGH,OAAO,CAACY,IAAI,CAACC,GAAG,CAACV,KAAK,CAACW,MAAM,CAAC,CAACC,MAAoB,GACnD,IAAII,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC;EAC3C,MAAMC,OAAO,GAAGhB,MAAM,GAClBJ,OAAO,CAACY,IAAI,CAACC,GAAG,CAACT,MAAM,CAACU,MAAM,CAAC,CAACC,MAAoB,GACpD,IAAII,YAAY,CAAC,CAAC,CAAC,CAAC,CAAC;EACzB,MAAME,OAAO,GAAG,IAAIF,YAAY,CAACR,KAAK,CAACF,MAAM,CAAC;EAE9C,MAAMa,aAAa,GAAGF,OAAO,CAACX,MAAM;EACpC,MAAMc,WAAW,GAAGL,KAAK,CAACT,MAAM;EAChC,MAAMe,aAAa,GAAGP,OAAO,CAACR,MAAM;EACpC,MAAMgB,WAAW,GAAGT,KAAK,CAACP,MAAM;EAEhC,IAAIiB,IAAI,GAAG,CAAC;EACZ,IAAIC,EAAE,GAAG,CAAC;EACV,IAAIC,EAAE,GAAG,CAAC;EACV,IAAIC,EAAE,GAAG,CAAC;EACV,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGnB,KAAK,CAACF,MAAM,EAAE,EAAEqB,CAAC,EAAE;IACrCT,OAAO,CAACS,CAAC,CAAC,GAAGV,OAAO,CAACM,IAAI,EAAE,CAAC,GACxB,CAACf,KAAK,CAACmB,CAAC,CAAC,GAAGd,KAAK,CAACW,EAAE,EAAE,CAAC,IAAIT,KAAK,CAACU,EAAE,EAAE,CAAC,GAClCG,IAAI,CAACC,IAAI,CAACf,OAAO,CAACY,EAAE,EAAE,CAAC,GAAGnB,eAAe,CAAC;IAClD,IAAIgB,IAAI,IAAIJ,aAAa,EAAE;MACzBI,IAAI,GAAG,CAAC;;IAEV,IAAIC,EAAE,IAAIF,WAAW,EAAE;MACrBE,EAAE,GAAG,CAAC;;IAER,IAAIC,EAAE,IAAIL,WAAW,EAAE;MACrBK,EAAE,GAAG,CAAC;;IAER,IAAIC,EAAE,IAAIL,aAAa,EAAE;MACvBK,EAAE,GAAG,CAAC;;;EAGV,OAAO7B,OAAO,CAACiC,cAAc,CAAC/B,CAAC,CAACM,KAAK,EAAEN,CAAC,CAACgC,KAAK,EAAEb,OAAO,CAAC;AAC1D;AAEA,OAAO,MAAMc,eAAe,GAAiB;EAC3CC,UAAU,EAAE1C,cAAc;EAC1B2C,WAAW,EAAE,KAAK;EAClBC,UAAU,EAAEzC;CACb","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}