{"ast":null,"code":"/**\n * @license\n * Copyright 2022 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util } from '../base';\nimport { Prod } from '../kernel_names';\nimport { cumprod } from '../ops/cumprod';\nimport { mul } from '../ops/mul';\nimport { reshape } from '../ops/reshape';\nimport { transpose } from '../ops/transpose';\n// Gradient for product operation on a single axis.\nfunction prodGradFn_(x, dy, axis) {\n  // The gradient tensor (dy) has a set of axes removed, so we create re-shaped\n  // versions (of size 1) for the removed axis; this supports broadcasting over\n  // those dimensions.\n  const expandedYShape = x.shape.slice();\n  expandedYShape[axis] = 1;\n  // The actual gradient computation.\n  const expandedDy = reshape(dy, expandedYShape);\n  const xCumProd = cumprod(x, axis, true, false);\n  const xCumRevProd = cumprod(x, axis, true, true);\n  const dx = mul(xCumProd, xCumRevProd);\n  return mul(expandedDy, dx);\n}\n// Support gradients when the product is done on many axes at once.\n// This done py pushing all the axes on which the product is applied into a\n// single axis.\nfunction prodsGradFn_(x, dy, axis) {\n  // Move all axes for doing prod over to the end of the tensor.\n  const xRank = x.shape.length;\n  const finalProdAxis = xRank - axis.length;\n  const xPermutation = backend_util.getAxesPermutation(axis, xRank);\n  let permutedX = x;\n  if (xPermutation != null) {\n    permutedX = transpose(x, xPermutation);\n  }\n  // Reshape all the prod dimensions into a single one, and do compute prod\n  // gradients on that.\n  const newShape = permutedX.shape.slice();\n  const removedShape = newShape.splice(xRank - axis.length, axis.length);\n  const endPartShape = removedShape.reduce((p, c) => p * c, 1);\n  newShape.push(endPartShape);\n  const reshapedPermutedX = permutedX.reshape(newShape);\n  let prodGrad = prodGradFn_(reshapedPermutedX, dy, finalProdAxis);\n  // Undo the re-shaping now we have the dx vector, and permute back to\n  // original axes order.\n  prodGrad = prodGrad.reshape(permutedX.shape);\n  if (xPermutation != null) {\n    const undoPermutation = backend_util.getUndoAxesPermutation(xPermutation);\n    prodGrad = transpose(prodGrad, undoPermutation);\n  }\n  return prodGrad;\n}\n// Running example:\n// [\n//   [\n//     [3.0, 4.0],\n//     [5.0, 6.0],\n//     [7.0, 8.0]\n//   ],\n//   [\n//     [3.0, 5.0],\n//     [0.0, 6.0],\n//     [5.0, 6.0]\n//   ]\n// ]\n//\nexport const prodGradConfig = {\n  kernelName: Prod,\n  inputsToSave: ['x'],\n  gradFunc: (dy, saved, attrs) => {\n    const [x] = saved;\n    const {\n      axis\n    } = attrs;\n    let axisArr = [];\n    if (axis === undefined || axis === null) {\n      axisArr = x.shape.map((_, i) => i);\n    } else if (typeof axis === 'number') {\n      axisArr = [axis];\n    } else {\n      axisArr = axis;\n    }\n    return {\n      x: () => prodsGradFn_(x, dy, axisArr)\n    };\n  }\n};","map":{"version":3,"names":["backend_util","Prod","cumprod","mul","reshape","transpose","prodGradFn_","x","dy","axis","expandedYShape","shape","slice","expandedDy","xCumProd","xCumRevProd","dx","prodsGradFn_","xRank","length","finalProdAxis","xPermutation","getAxesPermutation","permutedX","newShape","removedShape","splice","endPartShape","reduce","p","c","push","reshapedPermutedX","prodGrad","undoPermutation","getUndoAxesPermutation","prodGradConfig","kernelName","inputsToSave","gradFunc","saved","attrs","axisArr","undefined","map","_","i"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-core/src/gradients/Prod_grad.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2022 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util} from '../base';\nimport {Prod, ProdAttrs} from '../kernel_names';\nimport {GradConfig, NamedAttrMap} from '../kernel_registry';\nimport {cumprod} from '../ops/cumprod';\nimport {mul} from '../ops/mul';\nimport {reshape} from '../ops/reshape';\nimport {transpose} from '../ops/transpose';\nimport {Tensor} from '../tensor';\n\n// Gradient for product operation on a single axis.\nfunction prodGradFn_(x: Tensor, dy: Tensor, axis: number): Tensor {\n  // The gradient tensor (dy) has a set of axes removed, so we create re-shaped\n  // versions (of size 1) for the removed axis; this supports broadcasting over\n  // those dimensions.\n  const expandedYShape = x.shape.slice();\n  expandedYShape[axis] = 1;\n\n  // The actual gradient computation.\n  const expandedDy = reshape(dy, expandedYShape);\n  const xCumProd = cumprod(x, axis, true, false);\n  const xCumRevProd = cumprod(x, axis, true, true);\n  const dx = mul(xCumProd, xCumRevProd);\n  return mul(expandedDy, dx);\n}\n\n// Support gradients when the product is done on many axes at once.\n// This done py pushing all the axes on which the product is applied into a\n// single axis.\nfunction prodsGradFn_(x: Tensor, dy: Tensor, axis: number[]): Tensor {\n  // Move all axes for doing prod over to the end of the tensor.\n  const xRank = x.shape.length;\n  const finalProdAxis = xRank - axis.length;\n  const xPermutation = backend_util.getAxesPermutation(axis, xRank);\n  let permutedX = x;\n  if (xPermutation != null) {\n    permutedX = transpose(x, xPermutation);\n  }\n\n  // Reshape all the prod dimensions into a single one, and do compute prod\n  // gradients on that.\n  const newShape = permutedX.shape.slice();\n  const removedShape = newShape.splice(xRank - axis.length, axis.length);\n  const endPartShape = removedShape.reduce((p, c) => p * c, 1);\n  newShape.push(endPartShape);\n  const reshapedPermutedX = permutedX.reshape(newShape);\n  let prodGrad = prodGradFn_(reshapedPermutedX, dy, finalProdAxis);\n\n  // Undo the re-shaping now we have the dx vector, and permute back to\n  // original axes order.\n  prodGrad = prodGrad.reshape(permutedX.shape);\n  if (xPermutation != null) {\n    const undoPermutation = backend_util.getUndoAxesPermutation(xPermutation);\n    prodGrad = transpose(prodGrad, undoPermutation);\n  }\n  return prodGrad;\n}\n\n// Running example:\n// [\n//   [\n//     [3.0, 4.0],\n//     [5.0, 6.0],\n//     [7.0, 8.0]\n//   ],\n//   [\n//     [3.0, 5.0],\n//     [0.0, 6.0],\n//     [5.0, 6.0]\n//   ]\n// ]\n//\nexport const prodGradConfig: GradConfig = {\n  kernelName: Prod,\n  inputsToSave: ['x'],\n  gradFunc: (dy: Tensor|Tensor[], saved: Tensor[], attrs: NamedAttrMap) => {\n    const [x] = saved;\n    const {axis} = (attrs as {}) as ProdAttrs;\n    let axisArr = [] as number[];\n    if (axis === undefined || axis === null) {\n      axisArr = x.shape.map((_, i) => i);\n    } else if (typeof axis === 'number') {\n      axisArr = [axis];\n    } else {\n      axisArr = axis;\n    }\n    return {x: () => prodsGradFn_(x, dy as Tensor, axisArr)};\n  }\n};\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,YAAY,QAAO,SAAS;AACpC,SAAQC,IAAI,QAAkB,iBAAiB;AAE/C,SAAQC,OAAO,QAAO,gBAAgB;AACtC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,OAAO,QAAO,gBAAgB;AACtC,SAAQC,SAAS,QAAO,kBAAkB;AAG1C;AACA,SAASC,WAAWA,CAACC,CAAS,EAAEC,EAAU,EAAEC,IAAY;EACtD;EACA;EACA;EACA,MAAMC,cAAc,GAAGH,CAAC,CAACI,KAAK,CAACC,KAAK,EAAE;EACtCF,cAAc,CAACD,IAAI,CAAC,GAAG,CAAC;EAExB;EACA,MAAMI,UAAU,GAAGT,OAAO,CAACI,EAAE,EAAEE,cAAc,CAAC;EAC9C,MAAMI,QAAQ,GAAGZ,OAAO,CAACK,CAAC,EAAEE,IAAI,EAAE,IAAI,EAAE,KAAK,CAAC;EAC9C,MAAMM,WAAW,GAAGb,OAAO,CAACK,CAAC,EAAEE,IAAI,EAAE,IAAI,EAAE,IAAI,CAAC;EAChD,MAAMO,EAAE,GAAGb,GAAG,CAACW,QAAQ,EAAEC,WAAW,CAAC;EACrC,OAAOZ,GAAG,CAACU,UAAU,EAAEG,EAAE,CAAC;AAC5B;AAEA;AACA;AACA;AACA,SAASC,YAAYA,CAACV,CAAS,EAAEC,EAAU,EAAEC,IAAc;EACzD;EACA,MAAMS,KAAK,GAAGX,CAAC,CAACI,KAAK,CAACQ,MAAM;EAC5B,MAAMC,aAAa,GAAGF,KAAK,GAAGT,IAAI,CAACU,MAAM;EACzC,MAAME,YAAY,GAAGrB,YAAY,CAACsB,kBAAkB,CAACb,IAAI,EAAES,KAAK,CAAC;EACjE,IAAIK,SAAS,GAAGhB,CAAC;EACjB,IAAIc,YAAY,IAAI,IAAI,EAAE;IACxBE,SAAS,GAAGlB,SAAS,CAACE,CAAC,EAAEc,YAAY,CAAC;;EAGxC;EACA;EACA,MAAMG,QAAQ,GAAGD,SAAS,CAACZ,KAAK,CAACC,KAAK,EAAE;EACxC,MAAMa,YAAY,GAAGD,QAAQ,CAACE,MAAM,CAACR,KAAK,GAAGT,IAAI,CAACU,MAAM,EAAEV,IAAI,CAACU,MAAM,CAAC;EACtE,MAAMQ,YAAY,GAAGF,YAAY,CAACG,MAAM,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKD,CAAC,GAAGC,CAAC,EAAE,CAAC,CAAC;EAC5DN,QAAQ,CAACO,IAAI,CAACJ,YAAY,CAAC;EAC3B,MAAMK,iBAAiB,GAAGT,SAAS,CAACnB,OAAO,CAACoB,QAAQ,CAAC;EACrD,IAAIS,QAAQ,GAAG3B,WAAW,CAAC0B,iBAAiB,EAAExB,EAAE,EAAEY,aAAa,CAAC;EAEhE;EACA;EACAa,QAAQ,GAAGA,QAAQ,CAAC7B,OAAO,CAACmB,SAAS,CAACZ,KAAK,CAAC;EAC5C,IAAIU,YAAY,IAAI,IAAI,EAAE;IACxB,MAAMa,eAAe,GAAGlC,YAAY,CAACmC,sBAAsB,CAACd,YAAY,CAAC;IACzEY,QAAQ,GAAG5B,SAAS,CAAC4B,QAAQ,EAAEC,eAAe,CAAC;;EAEjD,OAAOD,QAAQ;AACjB;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAMG,cAAc,GAAe;EACxCC,UAAU,EAAEpC,IAAI;EAChBqC,YAAY,EAAE,CAAC,GAAG,CAAC;EACnBC,QAAQ,EAAEA,CAAC/B,EAAmB,EAAEgC,KAAe,EAAEC,KAAmB,KAAI;IACtE,MAAM,CAAClC,CAAC,CAAC,GAAGiC,KAAK;IACjB,MAAM;MAAC/B;IAAI,CAAC,GAAIgC,KAAyB;IACzC,IAAIC,OAAO,GAAG,EAAc;IAC5B,IAAIjC,IAAI,KAAKkC,SAAS,IAAIlC,IAAI,KAAK,IAAI,EAAE;MACvCiC,OAAO,GAAGnC,CAAC,CAACI,KAAK,CAACiC,GAAG,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAAC;KACnC,MAAM,IAAI,OAAOrC,IAAI,KAAK,QAAQ,EAAE;MACnCiC,OAAO,GAAG,CAACjC,IAAI,CAAC;KACjB,MAAM;MACLiC,OAAO,GAAGjC,IAAI;;IAEhB,OAAO;MAACF,CAAC,EAAEA,CAAA,KAAMU,YAAY,CAACV,CAAC,EAAEC,EAAY,EAAEkC,OAAO;IAAC,CAAC;EAC1D;CACD","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}