{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}) {\n  activation = activation || 'linear';\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    // TODO: Transpose bias and preluActivationWeights properly for NCHW\n    // format before computation.\n    util.assert(dataFormat === 'NHWC', () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` + `only NHWC is currently supported for the case of gradient depth ` + `is 0 and the activation is not linear.`);\n    let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n  let x4D = $x;\n  let reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` + `${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` + `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n  util.assert($filter.shape[2] === inputChannels, () => `Error in conv2d: depth of input (${inputChannels}) must match ` + `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' + `Got strides ${strides} and dilations '${dilations}'`);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  let $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n    // scalar.\n    //\n    // 3-D or 4-D bias is not disabled for NHWC format, because they are\n    // currently being used in some cases. For examplem in our code base,\n    // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n    if (dataFormat === 'NHWC') {\n      broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    } else {\n      util.assert($bias.shape.length <= 1, () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` + `bias for NCHW format but got the bias of ` + `rank-${$bias.shape.length}.`);\n      util.assert($bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels || $bias.shape[0] === 1, () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` + `compatible with the number of output channels ` + `(${convInfo.outChannels})`);\n    }\n  }\n  let $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n    // tensor.\n    const alphaShape = preluActivationWeights.shape;\n    util.assert(alphaShape.length <= 1 || alphaShape.length === 3, () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` + `3-D Tensor PReLU activation weights but got a tensor of ` + `rank-${alphaShape.length}.`);\n    if (alphaShape.length === 1) {\n      // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n      // weights tensor should be aligned with the output channels of conv2d\n      // result.\n      util.assert(alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels, () => `Error in fused conv2d: PReLU activation weights ` + `(${alphaShape}) is not compatible with the number of output ` + `channels (${convInfo.outChannels}).`);\n    } else if (alphaShape.length === 3) {\n      // Whether the data format is NCHW or NHWC, the PReLU activation weights\n      // tensor should has the compatible shape with the result of conv2d.\n      try {\n        broadcast_util.assertAndGetBroadcastShape(alphaShape, convInfo.outShape);\n      } catch (e) {\n        const errMsg = `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` + `is not compatible with the output shape of the conv2d ` + `(${convInfo.outShape}).`;\n        throw Error(errMsg);\n      }\n    }\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n  const grad = (dy, saved) => {\n    util.assert(dataFormat === 'NHWC', () => `Error in gradient of fused conv2D: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const [$filter, x4D, y, $bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' + `dilation rates greater than 1 ` + `are not yet supported in gradients. Got dilations '${dilations}'`);\n    const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der = [xDer, filterDer];\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n    return der;\n  };\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      let res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\nexport const conv2d = /* @__PURE__ */op({\n  fusedConv2d_\n});","map":{"version":3,"names":["ENGINE","customGrad","FusedConv2D","makeTypesMatch","convertToTensor","util","add","broadcast_util","conv2d","unfusedConv2d","conv2DBackpropFilter","conv2DBackpropInput","conv_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","op","reshape","fusedConv2d_","x","filter","strides","pad","dataFormat","dilations","dimRoundingMode","bias","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","assert","result","$x","$filter","x4D","reshapedTo4D","rank","shape","checkPadOnDimRoundingMode","inputChannels","eitherStridesOrDilationsAreOne","convInfo","computeConv2DInfo","$bias","assertAndGetBroadcastShape","outShape","length","outChannels","$preluActivationWeights","alphaShape","e","errMsg","Error","grad","dy","saved","y","dyActivation","tupleValuesAreOne","xDer","filterDer","der","biasDer","push","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-core/src/ops/fused/conv2d.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedConv2D, FusedConv2DAttrs, FusedConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {conv2d as unfusedConv2d} from '../conv2d';\nimport {conv2DBackpropFilter} from '../conv2d_backprop_filter';\nimport {conv2DBackpropInput} from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number|conv_util.ExplicitPadding,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    // TODO: Transpose bias and preluActivationWeights properly for NCHW\n    // format before computation.\n    util.assert(\n        dataFormat === 'NHWC',\n        () => `Error in fused conv2d: got dataFormat of ${dataFormat} but ` +\n            `only NHWC is currently supported for the case of gradient depth ` +\n            `is 0 and the activation is not linear.`);\n\n    let result = unfusedConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d', 'float32');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n          `${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n          `${$filter.rank}.`);\n  conv_util.checkPadOnDimRoundingMode('fused conv2d', pad, dimRoundingMode);\n  const inputChannels = dataFormat === 'NHWC' ? x4D.shape[3] : x4D.shape[1];\n  util.assert(\n      $filter.shape[2] === inputChannels,\n      () => `Error in conv2d: depth of input (${inputChannels}) must match ` +\n          `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n          `Got strides ${strides} and dilations '${dilations}'`);\n\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    // According to TensorFlow, the bias is supposed be a 1-D tensor or a\n    // scalar.\n    //\n    // 3-D or 4-D bias is not disabled for NHWC format, because they are\n    // currently being used in some cases. For examplem in our code base,\n    // https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/fused_conv2d_test.ts#L1972.\n    if (dataFormat === 'NHWC') {\n      broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    } else {\n      util.assert(\n          $bias.shape.length <= 1,\n          () => `Error in fused conv2d: only supports scalar or 1-D Tensor ` +\n              `bias for NCHW format but got the bias of ` +\n              `rank-${$bias.shape.length}.`);\n\n      util.assert(\n          $bias.shape.length === 0 || $bias.shape[0] === convInfo.outChannels ||\n              $bias.shape[0] === 1,\n          () => `Error in fused conv2d: bias shape (${$bias.shape}) is not ` +\n              `compatible with the number of output channels ` +\n              `(${convInfo.outChannels})`);\n    }\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    // PReLU's activation weights could be a scalar, a 1-D tensor or a 3-D\n    // tensor.\n    const alphaShape = preluActivationWeights.shape;\n    util.assert(\n        alphaShape.length <= 1 || alphaShape.length === 3,\n        () => `Error in fused conv2d: only supports scalar, 1-D Tensor or ` +\n            `3-D Tensor PReLU activation weights but got a tensor of ` +\n            `rank-${alphaShape.length}.`);\n\n    if (alphaShape.length === 1) {\n      // Whether the data format is NCHW or NHWC, the 1-D PReLU activation\n      // weights tensor should be aligned with the output channels of conv2d\n      // result.\n      util.assert(\n          alphaShape[0] === 1 || alphaShape[0] === convInfo.outChannels,\n          () => `Error in fused conv2d: PReLU activation weights ` +\n              `(${alphaShape}) is not compatible with the number of output ` +\n              `channels (${convInfo.outChannels}).`);\n    } else if (alphaShape.length === 3) {\n      // Whether the data format is NCHW or NHWC, the PReLU activation weights\n      // tensor should has the compatible shape with the result of conv2d.\n      try {\n        broadcast_util.assertAndGetBroadcastShape(\n            alphaShape, convInfo.outShape);\n      } catch (e) {\n        const errMsg =\n            `Error in fused conv2d: PReLU activation weights (${alphaShape}) ` +\n            `is not compatible with the output shape of the conv2d ` +\n            `(${convInfo.outShape}).`;\n        throw Error(errMsg);\n      }\n    }\n\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    util.assert(\n        dataFormat === 'NHWC',\n        () => `Error in gradient of fused conv2D: got dataFormat of ${\n            dataFormat} but only NHWC is currently supported.`);\n\n    const [$filter, x4D, y, $bias] =\n        saved as [Tensor4D, Tensor4D, Tensor4D, Tensor];\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n\n    const xDer =\n        conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer =\n        conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der: Tensor[] = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n    return der;\n  };\n\n  const inputs: FusedConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n\n  const attrs: FusedConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D =\n              // tslint:disable-next-line: no-unnecessary-type-assertion\n              ENGINE.runKernel(\n                  FusedConv2D, inputs as unknown as NamedTensorMap,\n                  attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const conv2d = /* @__PURE__ */ op({fusedConv2d_});\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,cAAc;AACnC,SAAQC,UAAU,QAAO,iBAAiB;AAC1C,SAAQC,WAAW,QAA4C,oBAAoB;AAInF,SAAQC,cAAc,QAAO,mBAAmB;AAChD,SAAQC,eAAe,QAAO,uBAAuB;AAErD,OAAO,KAAKC,IAAI,MAAM,YAAY;AAClC,SAAQC,GAAG,QAAO,QAAQ;AAC1B,OAAO,KAAKC,cAAc,MAAM,mBAAmB;AACnD,SAAQC,MAAM,IAAIC,aAAa,QAAO,WAAW;AACjD,SAAQC,oBAAoB,QAAO,2BAA2B;AAC9D,SAAQC,mBAAmB,QAAO,0BAA0B;AAC5D,OAAO,KAAKC,SAAS,MAAM,cAAc;AAEzC,SAAQC,eAAe,EAAEC,oBAAoB,EAAEC,oBAAoB,EAAEC,UAAU,QAAO,eAAe;AACrG,SAAQC,EAAE,QAAO,cAAc;AAC/B,SAAQC,OAAO,QAAO,YAAY;AAElC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyDA,SAASC,YAAYA,CAA8B;EACjDC,CAAC;EACDC,MAAM;EACNC,OAAO;EACPC,GAAG;EACHC,UAAU,GAAG,MAAM;EACnBC,SAAS,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;EAClBC,eAAe;EACfC,IAAI;EACJC,UAAU,GAAG,QAAQ;EACrBC,sBAAsB;EACtBC;AAAc,CAaf;EACCF,UAAU,GAAGA,UAAU,IAAI,QAAQ;EAEnC,IAAIZ,UAAU,CAAChB,MAAM,CAAC+B,KAAK,CAACC,aAAa,EAAEJ,UAAU,CAAC,KAAK,KAAK,EAAE;IAChE;IACA;IACAvB,IAAI,CAAC4B,MAAM,CACPT,UAAU,KAAK,MAAM,EACrB,MAAM,4CAA4CA,UAAU,OAAO,GAC/D,kEAAkE,GAClE,wCAAwC,CAAC;IAEjD,IAAIU,MAAM,GAAGzB,aAAa,CACtBW,CAAC,EAAEC,MAAM,EAAEC,OAAO,EAAEC,GAAG,EAAEC,UAAU,EAAEC,SAAS,EAAEC,eAAe,CAAC;IACpE,IAAIC,IAAI,IAAI,IAAI,EAAE;MAChBO,MAAM,GAAG5B,GAAG,CAAC4B,MAAM,EAAEP,IAAI,CAAC;;IAG5B,OAAOd,eAAe,CACXqB,MAAM,EAAEN,UAAU,EAAEC,sBAAsB,EAAEC,cAAc,CAAM;;EAG7E,MAAMK,EAAE,GAAG/B,eAAe,CAACgB,CAAC,EAAE,GAAG,EAAE,QAAQ,EAAE,SAAS,CAAC;EACvD,MAAMgB,OAAO,GAAGhC,eAAe,CAACiB,MAAM,EAAE,QAAQ,EAAE,QAAQ,EAAE,SAAS,CAAC;EAEtE,IAAIgB,GAAG,GAAGF,EAAc;EACxB,IAAIG,YAAY,GAAG,KAAK;EAExB,IAAIH,EAAE,CAACI,IAAI,KAAK,CAAC,EAAE;IACjBD,YAAY,GAAG,IAAI;IACnBD,GAAG,GAAGnB,OAAO,CAACiB,EAAE,EAAE,CAAC,CAAC,EAAEA,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,EAAEL,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,EAAEL,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;;EAE/DnC,IAAI,CAAC4B,MAAM,CACPI,GAAG,CAACE,IAAI,KAAK,CAAC,EACd,MAAM,4DAA4D,GAC9D,GAAGF,GAAG,CAACE,IAAI,GAAG,CAAC;EACvBlC,IAAI,CAAC4B,MAAM,CACPG,OAAO,CAACG,IAAI,KAAK,CAAC,EAClB,MAAM,6DAA6D,GAC/D,GAAGH,OAAO,CAACG,IAAI,GAAG,CAAC;EAC3B3B,SAAS,CAAC6B,yBAAyB,CAAC,cAAc,EAAElB,GAAG,EAAEG,eAAe,CAAC;EACzE,MAAMgB,aAAa,GAAGlB,UAAU,KAAK,MAAM,GAAGa,GAAG,CAACG,KAAK,CAAC,CAAC,CAAC,GAAGH,GAAG,CAACG,KAAK,CAAC,CAAC,CAAC;EACzEnC,IAAI,CAAC4B,MAAM,CACPG,OAAO,CAACI,KAAK,CAAC,CAAC,CAAC,KAAKE,aAAa,EAClC,MAAM,oCAAoCA,aAAa,eAAe,GAClE,0BAA0BN,OAAO,CAACI,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC;EACtDnC,IAAI,CAAC4B,MAAM,CACPrB,SAAS,CAAC+B,8BAA8B,CAACrB,OAAO,EAAEG,SAAS,CAAC,EAC5D,MAAM,0DAA0D,GAC5D,eAAeH,OAAO,mBAAmBG,SAAS,GAAG,CAAC;EAE9D,MAAMmB,QAAQ,GAAGhC,SAAS,CAACiC,iBAAiB,CACxCR,GAAG,CAACG,KAAK,EAAEJ,OAAO,CAACI,KAAK,EAAElB,OAAO,EAAEG,SAAS,EAAEF,GAAG,EAAEG,eAAe,CAAC;EAEvE,IAAIoB,KAAa;EACjB,IAAInB,IAAI,IAAI,IAAI,EAAE;IAChBmB,KAAK,GAAG1C,eAAe,CAACuB,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC;IACrD,CAACmB,KAAK,CAAC,GAAG3C,cAAc,CAAC2C,KAAK,EAAEX,EAAE,CAAC;IAEnC;IACA;IACA;IACA;IACA;IACA;IACA,IAAIX,UAAU,KAAK,MAAM,EAAE;MACzBjB,cAAc,CAACwC,0BAA0B,CAACH,QAAQ,CAACI,QAAQ,EAAEF,KAAK,CAACN,KAAK,CAAC;KAC1E,MAAM;MACLnC,IAAI,CAAC4B,MAAM,CACPa,KAAK,CAACN,KAAK,CAACS,MAAM,IAAI,CAAC,EACvB,MAAM,4DAA4D,GAC9D,2CAA2C,GAC3C,QAAQH,KAAK,CAACN,KAAK,CAACS,MAAM,GAAG,CAAC;MAEtC5C,IAAI,CAAC4B,MAAM,CACPa,KAAK,CAACN,KAAK,CAACS,MAAM,KAAK,CAAC,IAAIH,KAAK,CAACN,KAAK,CAAC,CAAC,CAAC,KAAKI,QAAQ,CAACM,WAAW,IAC/DJ,KAAK,CAACN,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,EACxB,MAAM,sCAAsCM,KAAK,CAACN,KAAK,WAAW,GAC9D,gDAAgD,GAChD,IAAII,QAAQ,CAACM,WAAW,GAAG,CAAC;;;EAIxC,IAAIC,uBAA+B;EACnC,IAAItB,sBAAsB,IAAI,IAAI,EAAE;IAClC;IACA;IACA,MAAMuB,UAAU,GAAGvB,sBAAsB,CAACW,KAAK;IAC/CnC,IAAI,CAAC4B,MAAM,CACPmB,UAAU,CAACH,MAAM,IAAI,CAAC,IAAIG,UAAU,CAACH,MAAM,KAAK,CAAC,EACjD,MAAM,6DAA6D,GAC/D,0DAA0D,GAC1D,QAAQG,UAAU,CAACH,MAAM,GAAG,CAAC;IAErC,IAAIG,UAAU,CAACH,MAAM,KAAK,CAAC,EAAE;MAC3B;MACA;MACA;MACA5C,IAAI,CAAC4B,MAAM,CACPmB,UAAU,CAAC,CAAC,CAAC,KAAK,CAAC,IAAIA,UAAU,CAAC,CAAC,CAAC,KAAKR,QAAQ,CAACM,WAAW,EAC7D,MAAM,kDAAkD,GACpD,IAAIE,UAAU,gDAAgD,GAC9D,aAAaR,QAAQ,CAACM,WAAW,IAAI,CAAC;KAC/C,MAAM,IAAIE,UAAU,CAACH,MAAM,KAAK,CAAC,EAAE;MAClC;MACA;MACA,IAAI;QACF1C,cAAc,CAACwC,0BAA0B,CACrCK,UAAU,EAAER,QAAQ,CAACI,QAAQ,CAAC;OACnC,CAAC,OAAOK,CAAC,EAAE;QACV,MAAMC,MAAM,GACR,oDAAoDF,UAAU,IAAI,GAClE,wDAAwD,GACxD,IAAIR,QAAQ,CAACI,QAAQ,IAAI;QAC7B,MAAMO,KAAK,CAACD,MAAM,CAAC;;;IAIvBH,uBAAuB,GAAG/C,eAAe,CACrCyB,sBAAsB,EAAE,eAAe,EAAE,cAAc,CAAC;;EAG9D,MAAM2B,IAAI,GAAGA,CAACC,EAAY,EAAEC,KAAe,KAAI;IAC7CrD,IAAI,CAAC4B,MAAM,CACPT,UAAU,KAAK,MAAM,EACrB,MAAM,wDACFA,UAAU,wCAAwC,CAAC;IAE3D,MAAM,CAACY,OAAO,EAAEC,GAAG,EAAEsB,CAAC,EAAEb,KAAK,CAAC,GAC1BY,KAA+C;IAEnD,MAAME,YAAY,GAAG7C,oBAAoB,CAAC0C,EAAE,EAAEE,CAAC,EAAE/B,UAAU,CAAa;IAExEvB,IAAI,CAAC4B,MAAM,CACPrB,SAAS,CAACiD,iBAAiB,CAACpC,SAAS,CAAC,EACtC,MAAM,qCAAqC,GACvC,gCAAgC,GAChC,sDAAsDA,SAAS,GAAG,CAAC;IAE3E,MAAMqC,IAAI,GACNnD,mBAAmB,CAAC0B,GAAG,CAACG,KAAK,EAAEoB,YAAY,EAAExB,OAAO,EAAEd,OAAO,EAAEC,GAAG,CAAC;IACvE,MAAMwC,SAAS,GACXrD,oBAAoB,CAAC2B,GAAG,EAAEuB,YAAY,EAAExB,OAAO,CAACI,KAAK,EAAElB,OAAO,EAAEC,GAAG,CAAC;IACxE,MAAMyC,GAAG,GAAa,CAACF,IAAI,EAAEC,SAAS,CAAC;IAEvC,IAAIjB,KAAK,IAAI,IAAI,EAAE;MACjB,MAAMmB,OAAO,GAAGnD,oBAAoB,CAACgC,KAAK,EAAEc,YAAY,CAAC;MACzDI,GAAG,CAACE,IAAI,CAACD,OAAO,CAAC;;IAEnB,OAAOD,GAAG;EACZ,CAAC;EAED,MAAMG,MAAM,GAAsB;IAChC/C,CAAC,EAAEiB,GAAG;IACNhB,MAAM,EAAEe,OAAO;IACfT,IAAI,EAAEmB,KAAK;IACXjB,sBAAsB,EAAEsB;GACzB;EAED,MAAMiB,KAAK,GAAqB;IAC9B9C,OAAO;IACPC,GAAG;IACHC,UAAU;IACVC,SAAS;IACTC,eAAe;IACfE,UAAU;IACVE;GACD;EAED;EACA;EACA,IAAIH,IAAI,IAAI,IAAI,EAAE;IAChB,MAAM0C,QAAQ,GACVpE,UAAU,CAAC,CAACoC,GAAa,EAAEhB,MAAgB,EAAEiD,IAAkB,KAAI;MACjE,IAAIC,GAAG;MACH;MACAvE,MAAM,CAACwE,SAAS,CACZtE,WAAW,EAAEiE,MAAmC,EAChDC,KAAgC,CAAC;MAEzCE,IAAI,CAAC,CAACjD,MAAM,EAAEgB,GAAG,EAAEkC,GAAG,CAAC,CAAC;MAExB,IAAIjC,YAAY,EAAE;QAChB;QACAiC,GAAG,GAAGrD,OAAO,CAACqD,GAAG,EAAE,CAACA,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,EAAE+B,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,EAAE+B,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;;MAGd,OAAO;QAACiC,KAAK,EAAEF,GAAG;QAAEG,QAAQ,EAAElB;MAAI,CAAC;IACrC,CAAC,CAAC;IACN,OAAOa,QAAQ,CAAChC,GAAG,EAAED,OAAO,CAAM;GACnC,MAAM;IACL,MAAMuC,gBAAgB,GAAG1E,UAAU,CAC/B,CAACoC,GAAa,EAAEhB,MAAgB,EAAEM,IAAY,EAAE2C,IAAkB,KAAI;MACpE,IAAIC,GAAG,GAAsBvE,MAAM,CAACwE,SAAS,CACzCtE,WAAW,EAAEiE,MAAmC,EAChDC,KAAgC,CAAC;MAErCE,IAAI,CAAC,CAACjD,MAAM,EAAEgB,GAAG,EAAEkC,GAAG,EAAE5C,IAAI,CAAC,CAAC;MAE9B,IAAIW,YAAY,EAAE;QAChB;QACAiC,GAAG,GAAGrD,OAAO,CAACqD,GAAG,EAAE,CAACA,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,EAAE+B,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,EAAE+B,GAAG,CAAC/B,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;;MAGd,OAAO;QAACiC,KAAK,EAAEF,GAAG;QAAEG,QAAQ,EAAElB;MAAI,CAAC;IACrC,CAAC,CAAC;IAEN,OAAOmB,gBAAgB,CAACtC,GAAG,EAAED,OAAO,EAAEU,KAAK,CAAM;;AAErD;AACA,OAAO,MAAMtC,MAAM,GAAG,eAAgBS,EAAE,CAAC;EAACE;AAAY,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}