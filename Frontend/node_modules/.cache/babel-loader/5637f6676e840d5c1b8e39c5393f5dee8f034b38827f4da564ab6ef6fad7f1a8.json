{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { elu } from '../kernels/Elu';\nimport { identity } from '../kernels/Identity';\nimport { leakyRelu } from '../kernels/LeakyRelu';\nimport { prelu } from '../kernels/Prelu';\nimport { relu } from '../kernels/Relu';\nimport { relu6 } from '../kernels/Relu6';\nimport { sigmoid } from '../kernels/Sigmoid';\nexport function applyActivation(backend, x, activation, preluActivationWeights, leakyreluAlpha) {\n  if (activation === 'linear') {\n    return identity({\n      inputs: {\n        x\n      },\n      backend\n    });\n  } else if (activation === 'relu') {\n    return relu({\n      inputs: {\n        x\n      },\n      backend\n    });\n  } else if (activation === 'elu') {\n    return elu({\n      inputs: {\n        x\n      },\n      backend\n    });\n  } else if (activation === 'relu6') {\n    return relu6({\n      inputs: {\n        x\n      },\n      backend\n    });\n  } else if (activation === 'prelu') {\n    return prelu({\n      inputs: {\n        x,\n        alpha: preluActivationWeights\n      },\n      backend\n    });\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu({\n      inputs: {\n        x\n      },\n      backend,\n      attrs: {\n        alpha: leakyreluAlpha\n      }\n    });\n  } else if (activation === 'sigmoid') {\n    return sigmoid({\n      inputs: {\n        x\n      },\n      backend\n    });\n  }\n  throw new Error(`Activation ${activation} has not been implemented for the CPU backend.`);\n}","map":{"version":3,"names":["elu","identity","leakyRelu","prelu","relu","relu6","sigmoid","applyActivation","backend","x","activation","preluActivationWeights","leakyreluAlpha","inputs","alpha","attrs","Error"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-backend-cpu/src/utils/fused_utils.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, backend_util, TensorInfo} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\nimport {elu} from '../kernels/Elu';\nimport {identity} from '../kernels/Identity';\nimport {leakyRelu} from '../kernels/LeakyRelu';\nimport {prelu} from '../kernels/Prelu';\nimport {relu} from '../kernels/Relu';\nimport {relu6} from '../kernels/Relu6';\nimport {sigmoid} from '../kernels/Sigmoid';\n\nexport function applyActivation(\n    backend: MathBackendCPU, x: TensorInfo, activation: backend_util.Activation,\n    preluActivationWeights?: TensorInfo, leakyreluAlpha?: number): TensorInfo {\n  if (activation === 'linear') {\n    return identity({inputs: {x}, backend});\n  } else if (activation === 'relu') {\n    return relu({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'elu') {\n    return elu({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'relu6') {\n    return relu6({inputs: {x}, backend}) as TensorInfo;\n  } else if (activation === 'prelu') {\n    return prelu({inputs: {x, alpha: preluActivationWeights}, backend});\n  } else if (activation === 'leakyrelu') {\n    return leakyRelu({inputs: {x}, backend, attrs: {alpha: leakyreluAlpha}});\n  } else if (activation === 'sigmoid') {\n    return sigmoid({inputs: {x}, backend}) as TensorInfo;\n  }\n  throw new Error(\n      `Activation ${activation} has not been implemented for the CPU backend.`);\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAoBA,SAAQA,GAAG,QAAO,gBAAgB;AAClC,SAAQC,QAAQ,QAAO,qBAAqB;AAC5C,SAAQC,SAAS,QAAO,sBAAsB;AAC9C,SAAQC,KAAK,QAAO,kBAAkB;AACtC,SAAQC,IAAI,QAAO,iBAAiB;AACpC,SAAQC,KAAK,QAAO,kBAAkB;AACtC,SAAQC,OAAO,QAAO,oBAAoB;AAE1C,OAAM,SAAUC,eAAeA,CAC3BC,OAAuB,EAAEC,CAAa,EAAEC,UAAmC,EAC3EC,sBAAmC,EAAEC,cAAuB;EAC9D,IAAIF,UAAU,KAAK,QAAQ,EAAE;IAC3B,OAAOT,QAAQ,CAAC;MAACY,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED;IAAO,CAAC,CAAC;GACxC,MAAM,IAAIE,UAAU,KAAK,MAAM,EAAE;IAChC,OAAON,IAAI,CAAC;MAACS,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED;IAAO,CAAC,CAAe;GAClD,MAAM,IAAIE,UAAU,KAAK,KAAK,EAAE;IAC/B,OAAOV,GAAG,CAAC;MAACa,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED;IAAO,CAAC,CAAe;GACjD,MAAM,IAAIE,UAAU,KAAK,OAAO,EAAE;IACjC,OAAOL,KAAK,CAAC;MAACQ,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED;IAAO,CAAC,CAAe;GACnD,MAAM,IAAIE,UAAU,KAAK,OAAO,EAAE;IACjC,OAAOP,KAAK,CAAC;MAACU,MAAM,EAAE;QAACJ,CAAC;QAAEK,KAAK,EAAEH;MAAsB,CAAC;MAAEH;IAAO,CAAC,CAAC;GACpE,MAAM,IAAIE,UAAU,KAAK,WAAW,EAAE;IACrC,OAAOR,SAAS,CAAC;MAACW,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED,OAAO;MAAEO,KAAK,EAAE;QAACD,KAAK,EAAEF;MAAc;IAAC,CAAC,CAAC;GACzE,MAAM,IAAIF,UAAU,KAAK,SAAS,EAAE;IACnC,OAAOJ,OAAO,CAAC;MAACO,MAAM,EAAE;QAACJ;MAAC,CAAC;MAAED;IAAO,CAAC,CAAe;;EAEtD,MAAM,IAAIQ,KAAK,CACX,cAAcN,UAAU,gDAAgD,CAAC;AAC/E","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}