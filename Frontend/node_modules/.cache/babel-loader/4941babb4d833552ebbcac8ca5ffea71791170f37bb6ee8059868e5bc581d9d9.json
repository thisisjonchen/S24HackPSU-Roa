{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { FusedBatchNorm } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport * as util from '../util';\nimport { xAs4D } from './batchnorm_util';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be a `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction batchNorm_(x, mean, variance, offset, scale, varianceEpsilon) {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale;\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n  let $offset;\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n  util.assert($mean.rank === $variance.rank, () => 'Batch normalization gradient requires mean and variance to have ' + 'equal ranks.');\n  util.assert($offset == null || $mean.rank === $offset.rank, () => 'Batch normalization gradient requires mean and offset to have ' + 'equal ranks.');\n  util.assert($scale == null || $mean.rank === $scale.rank, () => 'Batch normalization gradient requires mean and scale to have ' + 'equal ranks.');\n  const x4D = xAs4D($x);\n  const inputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n  const attrs = {\n    varianceEpsilon\n  };\n  // tslint:disable-next-line: no-unnecessary-type-assertion\n  const res = ENGINE.runKernel(FusedBatchNorm, inputs, attrs);\n  return reshape(res, $x.shape);\n}\nexport const batchNorm = /* @__PURE__ */op({\n  batchNorm_\n});","map":{"version":3,"names":["ENGINE","FusedBatchNorm","convertToTensor","util","xAs4D","op","reshape","batchNorm_","x","mean","variance","offset","scale","varianceEpsilon","$x","$mean","$variance","$scale","$offset","assert","rank","x4D","inputs","attrs","res","runKernel","shape","batchNorm"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-core/src/ops/batchnorm.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {FusedBatchNorm, FusedBatchNormAttrs, FusedBatchNormInputs} from '../kernel_names';\nimport {NamedAttrMap} from '../kernel_registry';\nimport {Tensor, Tensor1D, Tensor4D} from '../tensor';\nimport {NamedTensorMap} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {Rank, TensorLike} from '../types';\nimport * as util from '../util';\n\nimport {xAs4D} from './batchnorm_util';\nimport {op} from './operation';\nimport {reshape} from './reshape';\n\n/**\n * Batch normalization.\n *\n * As described in\n * [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).\n *\n * Mean, variance, scale, and offset can be of two shapes:\n *   - The same shape as the input.\n *   - In the common case, the depth dimension is the last dimension of x, so\n *     the values would be a `tf.Tensor1D` of shape [depth].\n *\n * Also available are stricter rank-specific methods with the same signature\n * as this method that assert that parameters passed are of given rank\n *   - `tf.batchNorm2d`\n *   - `tf.batchNorm3d`\n *   - `tf.batchNorm4d`\n *\n * @param x The input Tensor.\n * @param mean A mean Tensor.\n * @param variance A variance Tensor.\n * @param offset An offset Tensor.\n * @param scale A scale Tensor.\n * @param varianceEpsilon A small float number to avoid dividing by 0.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction batchNorm_<R extends Rank>(\n    x: Tensor<R>|TensorLike, mean: Tensor<R>|Tensor1D|TensorLike,\n    variance: Tensor<R>|Tensor1D|TensorLike,\n    offset?: Tensor<R>|Tensor1D|TensorLike,\n    scale?: Tensor<R>|Tensor1D|TensorLike,\n    varianceEpsilon?: number): Tensor<R> {\n  if (varianceEpsilon == null) {\n    varianceEpsilon = 0.001;\n  }\n  const $x = convertToTensor(x, 'x', 'batchNorm');\n  const $mean = convertToTensor(mean, 'mean', 'batchNorm');\n  const $variance = convertToTensor(variance, 'variance', 'batchNorm');\n  let $scale: Tensor<R>|Tensor1D;\n  if (scale != null) {\n    $scale = convertToTensor(scale, 'scale', 'batchNorm');\n  }\n  let $offset: Tensor<R>|Tensor1D;\n  if (offset != null) {\n    $offset = convertToTensor(offset, 'offset', 'batchNorm');\n  }\n\n  util.assert(\n      $mean.rank === $variance.rank,\n      () => 'Batch normalization gradient requires mean and variance to have ' +\n          'equal ranks.');\n  util.assert(\n      $offset == null || $mean.rank === $offset.rank,\n      () => 'Batch normalization gradient requires mean and offset to have ' +\n          'equal ranks.');\n  util.assert(\n      $scale == null || $mean.rank === $scale.rank,\n      () => 'Batch normalization gradient requires mean and scale to have ' +\n          'equal ranks.');\n\n  const x4D: Tensor4D = xAs4D($x);\n\n  const inputs: FusedBatchNormInputs = {\n    x: x4D,\n    scale: $scale,\n    offset: $offset,\n    mean: $mean,\n    variance: $variance\n  };\n\n  const attrs: FusedBatchNormAttrs = {varianceEpsilon};\n\n  // tslint:disable-next-line: no-unnecessary-type-assertion\n  const res = ENGINE.runKernel(\n                  FusedBatchNorm, inputs as unknown as NamedTensorMap,\n                  attrs as unknown as NamedAttrMap) as Tensor<R>;\n\n  return reshape(res, $x.shape);\n}\n\nexport const batchNorm = /* @__PURE__ */ op({batchNorm_});\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,cAAc,QAAkD,iBAAiB;AAIzF,SAAQC,eAAe,QAAO,oBAAoB;AAElD,OAAO,KAAKC,IAAI,MAAM,SAAS;AAE/B,SAAQC,KAAK,QAAO,kBAAkB;AACtC,SAAQC,EAAE,QAAO,aAAa;AAC9B,SAAQC,OAAO,QAAO,WAAW;AAEjC;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA,SAASC,UAAUA,CACfC,CAAuB,EAAEC,IAAmC,EAC5DC,QAAuC,EACvCC,MAAsC,EACtCC,KAAqC,EACrCC,eAAwB;EAC1B,IAAIA,eAAe,IAAI,IAAI,EAAE;IAC3BA,eAAe,GAAG,KAAK;;EAEzB,MAAMC,EAAE,GAAGZ,eAAe,CAACM,CAAC,EAAE,GAAG,EAAE,WAAW,CAAC;EAC/C,MAAMO,KAAK,GAAGb,eAAe,CAACO,IAAI,EAAE,MAAM,EAAE,WAAW,CAAC;EACxD,MAAMO,SAAS,GAAGd,eAAe,CAACQ,QAAQ,EAAE,UAAU,EAAE,WAAW,CAAC;EACpE,IAAIO,MAA0B;EAC9B,IAAIL,KAAK,IAAI,IAAI,EAAE;IACjBK,MAAM,GAAGf,eAAe,CAACU,KAAK,EAAE,OAAO,EAAE,WAAW,CAAC;;EAEvD,IAAIM,OAA2B;EAC/B,IAAIP,MAAM,IAAI,IAAI,EAAE;IAClBO,OAAO,GAAGhB,eAAe,CAACS,MAAM,EAAE,QAAQ,EAAE,WAAW,CAAC;;EAG1DR,IAAI,CAACgB,MAAM,CACPJ,KAAK,CAACK,IAAI,KAAKJ,SAAS,CAACI,IAAI,EAC7B,MAAM,kEAAkE,GACpE,cAAc,CAAC;EACvBjB,IAAI,CAACgB,MAAM,CACPD,OAAO,IAAI,IAAI,IAAIH,KAAK,CAACK,IAAI,KAAKF,OAAO,CAACE,IAAI,EAC9C,MAAM,gEAAgE,GAClE,cAAc,CAAC;EACvBjB,IAAI,CAACgB,MAAM,CACPF,MAAM,IAAI,IAAI,IAAIF,KAAK,CAACK,IAAI,KAAKH,MAAM,CAACG,IAAI,EAC5C,MAAM,+DAA+D,GACjE,cAAc,CAAC;EAEvB,MAAMC,GAAG,GAAajB,KAAK,CAACU,EAAE,CAAC;EAE/B,MAAMQ,MAAM,GAAyB;IACnCd,CAAC,EAAEa,GAAG;IACNT,KAAK,EAAEK,MAAM;IACbN,MAAM,EAAEO,OAAO;IACfT,IAAI,EAAEM,KAAK;IACXL,QAAQ,EAAEM;GACX;EAED,MAAMO,KAAK,GAAwB;IAACV;EAAe,CAAC;EAEpD;EACA,MAAMW,GAAG,GAAGxB,MAAM,CAACyB,SAAS,CACZxB,cAAc,EAAEqB,MAAmC,EACnDC,KAAgC,CAAc;EAE9D,OAAOjB,OAAO,CAACkB,GAAG,EAAEV,EAAE,CAACY,KAAK,CAAC;AAC/B;AAEA,OAAO,MAAMC,SAAS,GAAG,eAAgBtB,EAAE,CAAC;EAACE;AAAU,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}