{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { mul } from './mul';\nimport { neg } from './neg';\nimport { op } from './operation';\nimport { sigmoid } from './sigmoid';\nimport { softplus } from './softplus';\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction logSigmoid_(x) {\n  const $x = convertToTensor(x, 'x', 'logSigmoid');\n  // Use a custom gradient to maintain previous implementation.\n  // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n  // directly\n  const customOp = customGrad(x => {\n    // TODO(yassogba) we can remove the chained softplus call here only\n    // after backends have modualrized softplus at which point we can call\n    // engine runKernel(..., Sotfplus, ...) directly.\n    const value = neg(softplus(neg(x)));\n    const gradFunc = dy => {\n      const derX = mul(dy, sigmoid(neg(x)));\n      return derX;\n    };\n    return {\n      value,\n      gradFunc\n    };\n  });\n  return customOp($x);\n}\nexport const logSigmoid = /* @__PURE__ */op({\n  logSigmoid_\n});","map":{"version":3,"names":["customGrad","convertToTensor","mul","neg","op","sigmoid","softplus","logSigmoid_","x","$x","customOp","value","gradFunc","dy","derX","logSigmoid"],"sources":["/Users/jonchen/Documents/HackPSU/tfjs-core/src/ops/log_sigmoid.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {customGrad} from '../gradients';\nimport {Tensor} from '../tensor';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {mul} from './mul';\nimport {neg} from './neg';\nimport {op} from './operation';\nimport {sigmoid} from './sigmoid';\nimport {softplus} from './softplus';\n\n/**\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\n *\n * ```js\n * const x = tf.tensor1d([0, 1, -1, .7]);\n *\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\n * ```\n * @param x The input tensor.\n *\n * @doc {heading: 'Operations', subheading: 'Basic math'}\n */\nfunction logSigmoid_<T extends Tensor>(x: T|TensorLike): T {\n  const $x = convertToTensor(x, 'x', 'logSigmoid');\n\n  // Use a custom gradient to maintain previous implementation.\n  // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n  // directly\n  const customOp = customGrad((x: Tensor) => {\n    // TODO(yassogba) we can remove the chained softplus call here only\n    // after backends have modualrized softplus at which point we can call\n    // engine runKernel(..., Sotfplus, ...) directly.\n    const value = neg(softplus(neg(x)));\n\n    const gradFunc = (dy: T) => {\n      const derX = mul(dy, sigmoid(neg(x)));\n      return derX;\n    };\n    return {value, gradFunc};\n  });\n\n  return customOp($x) as T;\n}\nexport const logSigmoid = /* @__PURE__ */ op({logSigmoid_});\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,UAAU,QAAO,cAAc;AAEvC,SAAQC,eAAe,QAAO,oBAAoB;AAGlD,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,EAAE,QAAO,aAAa;AAC9B,SAAQC,OAAO,QAAO,WAAW;AACjC,SAAQC,QAAQ,QAAO,YAAY;AAEnC;;;;;;;;;;;;;AAaA,SAASC,WAAWA,CAAmBC,CAAe;EACpD,MAAMC,EAAE,GAAGR,eAAe,CAACO,CAAC,EAAE,GAAG,EAAE,YAAY,CAAC;EAEhD;EACA;EACA;EACA,MAAME,QAAQ,GAAGV,UAAU,CAAEQ,CAAS,IAAI;IACxC;IACA;IACA;IACA,MAAMG,KAAK,GAAGR,GAAG,CAACG,QAAQ,CAACH,GAAG,CAACK,CAAC,CAAC,CAAC,CAAC;IAEnC,MAAMI,QAAQ,GAAIC,EAAK,IAAI;MACzB,MAAMC,IAAI,GAAGZ,GAAG,CAACW,EAAE,EAAER,OAAO,CAACF,GAAG,CAACK,CAAC,CAAC,CAAC,CAAC;MACrC,OAAOM,IAAI;IACb,CAAC;IACD,OAAO;MAACH,KAAK;MAAEC;IAAQ,CAAC;EAC1B,CAAC,CAAC;EAEF,OAAOF,QAAQ,CAACD,EAAE,CAAM;AAC1B;AACA,OAAO,MAAMM,UAAU,GAAG,eAAgBX,EAAE,CAAC;EAACG;AAAW,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}